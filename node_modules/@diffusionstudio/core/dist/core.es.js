
/*!
 * @diffusionstudio/core v3.8.3
 * (c) 2025 Diffusion Studio Inc.
 * 
 * Licensed under the Diffusion Studio Non-Commercial License.
 * You may NOT use this software for any commercial purposes.
 * 
 * For commercial licensing, visit:
 * https://diffusion.studio
 */

let na = class Hs {
  data;
  format;
  numberOfChannels;
  numberOfFrames;
  sampleRate;
  timestamp;
  duration;
  constructor(e) {
    this.data = e.data, this.format = e.format, this.numberOfChannels = e.numberOfChannels, this.numberOfFrames = e.numberOfFrames, this.sampleRate = e.sampleRate, this.timestamp = e.timestamp || 0, this.duration = this.numberOfFrames / this.sampleRate * 1e6;
  }
  copyTo(e, i) {
    if (!i?.format || i.format !== "s16")
      throw new Error("Only s16 format is supported for copyTo");
    for (let s = 0; s < this.numberOfFrames; s++)
      for (let r = 0; r < this.numberOfChannels; r++) {
        const a = r * this.numberOfFrames + s, n = s * this.numberOfChannels + r, o = Math.max(-1, Math.min(1, this.data[a]));
        e[n] = Math.round(o * 32767);
      }
  }
  clone() {
    return new Hs({
      data: new Float32Array(this.data),
      format: this.format,
      numberOfChannels: this.numberOfChannels,
      numberOfFrames: this.numberOfFrames,
      sampleRate: this.sampleRate,
      timestamp: this.timestamp
    });
  }
  close() {
    this.data = new Float32Array(0);
  }
};
"AudioData" in window || Object.assign(window, { AudioData: na });
const Yl = 0.2, Rt = 30, oa = 1;
class lt extends Error {
  message;
  code;
  constructor({ message: e = "", code: i = "" }, ...s) {
    super(e, ...s), console.error(e), this.code = i, this.message = e;
  }
}
class Ee extends lt {
}
class ee extends lt {
}
class at extends lt {
}
class Jl extends lt {
}
class Bt extends lt {
}
function ca(t, e = Rt) {
  if (e < 1) throw new ee({
    code: "invalidArgument",
    message: "FPS must be greater or equal to 1"
  });
  return Math.round(t * e);
}
function Zl(t, e = Rt) {
  if (e < 1) throw new ee({
    code: "invalidArgument",
    message: "FPS must be greater or equal to 1"
  });
  return Math.round(t / e * 1e3) / 1e3;
}
function jt(t, e = Rt) {
  if (e < 1) throw new ee({
    code: "invalidArgument",
    message: "FPS must be greater or equal to 1"
  });
  return Math.round(t / e * 1e3);
}
function ed(t) {
  return Math.floor(t * 255).toString(16).padStart(2, "0").toUpperCase();
}
function td(t, e) {
  return t.reduce(
    (i, s) => {
      const r = s[e];
      return i[r] || (i[r] = []), i[r].push(s), i;
    },
    // @ts-ignore
    {}
  );
}
function Ct(t, e) {
  return [t.slice(0, e), t.slice(e)].filter((i) => i.length > 0);
}
function vt(t, e) {
  return e ? Math.floor(Math.random() * (e - t + 1) + t) : t;
}
async function id(t) {
  t <= 0 || await new Promise((e) => setTimeout(e, t));
}
function B(t, e) {
  if (!t)
    throw new Error(e || `Assertion failed: ${String(t)}`);
}
function sd(t, e = 300) {
  let i;
  return (...s) => {
    clearTimeout(i), i = setTimeout(() => {
      t.apply(t, s);
    }, e);
  };
}
function la(t, e, i) {
  i < 0 && (i = 0);
  const s = t[e];
  t.splice(e, 1), t.splice(i, 0, s);
}
function rd() {
  return typeof crypto < "u" ? crypto.randomUUID().split("-").at(0) : Math.random().toString(36).substring(2, 15) + Math.random().toString(36).substring(2, 15);
}
function ad(t) {
  return t.charAt(0).toUpperCase() + t.slice(1);
}
function da(t, e, i) {
  return Math.max(e, Math.min(i, t));
}
function nd(t) {
  throw new Error("This should not run!");
}
function ha(t) {
  if (t.numberOfChannels === 1)
    return t.getChannelData(0);
  const e = [];
  for (let n = 0; n < t.numberOfChannels; n++)
    e.push(t.getChannelData(n));
  const i = Math.max(...e.map((n) => n.length)), s = new Float32Array(i * t.numberOfChannels);
  let r = 0, a = 0;
  for (; a < i; )
    e.forEach((n) => {
      s[r++] = n[a] !== void 0 ? n[a] : 0;
    }), a++;
  return s;
}
function wt(t, e, i) {
  for (let s = 0; s < i.length; s++)
    t.setUint8(e + s, i.charCodeAt(s));
}
function ua(t, e, i) {
  for (let s = 0; s < e.length; s++, i += 2) {
    const r = Math.max(-1, Math.min(1, e[s]));
    t.setInt16(i, r < 0 ? r * 32768 : r * 32767, !0);
  }
  return t;
}
function fa(t, e, i) {
  const a = e * 2, n = 8, o = 36, c = t.length * 2, l = o + c, d = new ArrayBuffer(n + l), h = new DataView(d);
  return wt(h, 0, "RIFF"), h.setUint32(4, l, !0), wt(h, 8, "WAVE"), wt(h, 12, "fmt "), h.setUint32(16, 16, !0), h.setUint16(20, 1, !0), h.setUint16(22, e, !0), h.setUint32(24, i, !0), h.setUint32(28, i * a, !0), h.setUint16(32, a, !0), h.setUint16(34, 16, !0), wt(h, 36, "data"), h.setUint32(40, c, !0), ua(h, t, n + o);
}
function od(t, e = "audio/wav") {
  const i = ha(t), s = fa(i, t.numberOfChannels, t.sampleRate);
  return new Blob([s], { type: e });
}
function cd(t) {
  const e = new Float32Array(t.length * t.numberOfChannels);
  let i = 0;
  for (let s = 0; s < t.numberOfChannels; s++) {
    const r = t.getChannelData(s);
    e.set(r, i), i += r.length;
  }
  return e;
}
function ld(t) {
  const e = t.numberOfChannels, i = t.length, s = new Int16Array(i * e);
  for (let r = 0; r < i; r++)
    for (let a = 0; a < e; a++) {
      let n = t.getChannelData(a)[r] * 32767;
      n > 32767 && (n = 32767), n < -32767 && (n = -32767), s[r * e + a] = n;
    }
  return s;
}
async function dd(t, e = 22050, i = Math.sqrt(2)) {
  const s = await t.arrayBuffer(), r = new OfflineAudioContext({ sampleRate: e, length: 1 }), a = await r.decodeAudioData(s), n = r.createBuffer(1, a.length, e);
  if (a.numberOfChannels >= 2) {
    const o = a.getChannelData(0), c = a.getChannelData(1), l = n.getChannelData(0);
    for (let d = 0; d < a.length; ++d)
      l[d] = i * (o[d] + c[d]) / 2;
    return n;
  }
  return a;
}
function hd(t, e = 44100, i = 2) {
  if (t.sampleRate == e && t.numberOfChannels == i)
    return t;
  const s = Math.floor(t.duration * e), a = new OfflineAudioContext(i, 1, e).createBuffer(i, s, e);
  for (let n = 0; n < t.numberOfChannels; n++) {
    const o = t.getChannelData(n), c = a.getChannelData(n), l = t.sampleRate / e;
    for (let d = 0; d < c.length; d++) {
      const h = d * l, u = Math.floor(h), m = Math.ceil(h);
      if (m >= o.length)
        c[d] = o[u];
      else {
        const p = h - u;
        c[d] = o[u] * (1 - p) + o[m] * p;
      }
    }
  }
  return a;
}
async function $s(t, e) {
  const i = document.createElement("a");
  if (document.head.appendChild(i), i.target = "_blank", e ? i.download = e : t instanceof File ? i.download = t.name : t instanceof Blob ? i.download = "untitled" : typeof t == "string" && (i.download = t.split("/").pop() ?? "untitled"), typeof t == "string" && t.startsWith("data:image/svg+xml;base64,")) {
    const s = t.split(",")[1], r = atob(s), a = new Array(r.length);
    for (let c = 0; c < r.length; c++)
      a[c] = r.charCodeAt(c);
    const n = new Uint8Array(a), o = new Blob([n], { type: "image/svg+xml" });
    i.href = URL.createObjectURL(o), i.download = e?.split(".")[0] + ".svg";
  } else typeof t == "string" ? i.href = t : t instanceof Blob ? i.href = URL.createObjectURL(t) : i.href = URL.createObjectURL(await t.getFile());
  i.click(), i.remove();
}
async function ud(t, e = !0) {
  return new Promise((i) => {
    const s = document.createElement("input");
    s.type = "file", s.accept = t, s.multiple = e, s.onchange = (r) => {
      const a = Array.from(r.target?.files ?? []);
      i(a);
    }, s.click();
  });
}
function O(t) {
  if (!t) return new E(0);
  if (typeof t == "number")
    return E.fromFrames(t);
  if (t instanceof E)
    return t;
  const e = t.split(":");
  if (e.length === 3) {
    const [i, s, r] = e.map(Number);
    if (isNaN(i) || isNaN(s) || isNaN(r))
      throw new Error(`Invalid time format: ${t}`);
    return new E(0, r, s, i);
  }
  if (e.length === 2) {
    const [i, s] = e.map(Number);
    if (isNaN(i) || isNaN(s))
      throw new Error(`Invalid time format: ${t}`);
    return new E(0, s, i);
  }
  if (typeof t == "string") {
    const i = parseFloat(t);
    if (isNaN(i))
      throw new Error(`Invalid time format: ${t}`);
    if (t.endsWith("ms"))
      return new E(i);
    if (t.endsWith("s"))
      return new E(0, i);
    if (t.endsWith("f"))
      return E.fromFrames(i);
    if (t.endsWith("min"))
      return new E(0, 0, i);
    throw new Error(`Invalid time format: ${t}`);
  }
  throw new Error(`Invalid time format: ${t}`);
}
async function fd(t) {
  const { fps: e, height: i, width: s, bitrate: r } = t, a = [
    "avc1.640033",
    "avc1.4d4033",
    "avc1.424033",
    "avc1.640029",
    "avc1.4d4029",
    "avc1.424029"
  ], n = ["prefer-hardware", "prefer-software"], o = [];
  for (const l of a)
    for (const d of n)
      o.push({
        codec: l,
        hardwareAcceleration: d,
        width: s,
        height: i,
        bitrate: r,
        framerate: e
      });
  const c = [];
  if ("VideoEncoder" in window) {
    for (const l of o) {
      const d = await VideoEncoder.isConfigSupported(l);
      d.supported && c.push(d.config ?? l);
    }
    return c.sort(ma)[0].codec;
  }
}
function ma(t, e) {
  const i = t.hardwareAcceleration ?? "", s = e.hardwareAcceleration ?? "";
  return i < s ? -1 : i > s ? 1 : 0;
}
const Ot = {
  fromJSON: (t) => new Date(t)
};
class E {
  /**
   * Time state in **milliseconds**
   */
  time;
  /**
   * Create a new timestamp from various time units
   * @param millis - Time in milliseconds
   * @param seconds - Time in seconds
   * @param minutes - Time in minutes
   * @param hours - Time in hours
   */
  constructor(e = 0, i = 0, s = 0, r = 0) {
    this.time = Math.round(e + i * 1e3 + s * 6e4 + r * 36e5);
  }
  /**
   * Base unit of the timestamp
   */
  get millis() {
    return this.time;
  }
  set millis(e) {
    this.time = Math.round(e);
  }
  /**
   * Defines the time in frames at the
   * current frame rate
   */
  get frames() {
    return ca(this.millis / 1e3);
  }
  set frames(e) {
    this.millis = jt(e);
  }
  /**
   * Convert the timestamp to seconds
   */
  get seconds() {
    return this.millis / 1e3;
  }
  set seconds(e) {
    this.millis = e * 1e3;
  }
  /**
   * Equivalent to millis += x
   */
  addMillis(e) {
    return this.millis = this.millis + e, this;
  }
  /**
   * Equivalent to frames += x
   */
  addFrames(e) {
    const i = jt(e);
    return this.millis = this.millis + i, this;
  }
  /**
   * add two timestamps the timestamp being added will adapt
   * its fps to the current fps
   * @returns A new Timestamp instance with the added frames
   */
  add(e) {
    return new E(e.millis + this.millis);
  }
  /**
   * subtract two timestamps timestamp being subtracted
   * will adapt its fps to the current fps
   * @returns A new Timestamp instance with the added frames
   */
  subtract(e) {
    return new E(this.millis - e.millis);
  }
  /**
   * Create a new timestamp from frames
   */
  static fromFrames(e, i) {
    const s = new E();
    return s.millis = jt(e, i), s;
  }
  /**
   * get a copy of the object
   */
  copy() {
    return new E(this.millis);
  }
  toJSON() {
    return this.millis;
  }
  fromJSON(e) {
    return B(typeof e == "number"), this.millis = e, this;
  }
  static fromJSON(e) {
    return B(typeof e == "number"), new E(e);
  }
}
function Ki(t) {
  return `${t.hours.toString().padStart(2, "0")}:${t.minutes.toString().padStart(2, "0")}:${t.seconds.toString().padStart(2, "0")},` + t.milliseconds.toString().padStart(3, "0");
}
function Gi(t) {
  const e = new Date(1970, 0, 1);
  return e.setSeconds(t), e.setMilliseconds(Math.round(t % 1 * 1e3)), {
    hours: e.getHours(),
    minutes: e.getMinutes(),
    seconds: e.getSeconds(),
    milliseconds: e.getMilliseconds()
  };
}
class Le {
  words = [];
  constructor(e) {
    e && (this.words = e);
  }
  get duration() {
    return this.stop.subtract(this.start);
  }
  get text() {
    return this.words.map(({ text: e }) => e).join(" ");
  }
  get start() {
    return this.words.at(0)?.start ?? new E();
  }
  get stop() {
    return this.words.at(-1)?.stop ?? new E();
  }
}
var ai = /* @__PURE__ */ ((t) => (t.en = "en", t.de = "de", t))(ai || {});
class Tt {
  /**
   * A unique identifier for the word
   */
  id = crypto.randomUUID();
  /**
   * Defines the text to be displayed
   */
  text;
  /**
   * Defines the time stamp at
   * which the text is spoken
   */
  start;
  /**
   * Defines the time stamp at
   * which the text was spoken
   */
  stop;
  /**
   * Defines the confidence of
   * the predicition
   */
  confidence;
  /**
   * Create a new Word object
   * @param text The string contents of the word
   * @param start Start in **milliseconds**
   * @param stop Stop in **milliseconds**
   * @param confidence Predicition confidence
   */
  constructor(e, i, s, r) {
    this.text = e, this.start = new E(i), this.stop = new E(s), this.confidence = r;
  }
  /**
   * Defines the time between start
   * and stop returned as a timestamp
   */
  get duration() {
    return this.stop.subtract(this.start);
  }
}
class se {
  id = crypto.randomUUID();
  language = ai.en;
  groups = [];
  get text() {
    return this.groups.map(({ text: e }) => e).join(" ");
  }
  get words() {
    return this.groups.flatMap(({ words: e }) => e);
  }
  constructor(e = [], i = ai.en) {
    this.groups = e, this.language = i;
  }
  /**
   * Iterate over all words in groups
   */
  *iter({ count: e, duration: i, length: s }) {
    for (const r of this.groups) {
      let a;
      for (const [n, o] of r.words.entries())
        a && (e && a.words.length >= vt(...e) ? (yield a, a = void 0) : i && a?.duration.seconds >= vt(...i) ? (yield a, a = void 0) : s && a.text.length >= vt(...s) && (yield a, a = void 0)), a ? a.words.push(o) : a = new Le([o]), n == r.words.length - 1 && (yield a);
    }
  }
  /**
   * This method will optimize the transcipt for display
   */
  optimize() {
    const e = this.groups.flatMap((i) => i.words);
    for (let i = 0; i < e.length - 1; i++) {
      const s = e[i], r = e[i + 1];
      r.start.millis - s.stop.millis < 0 ? r.start.millis = s.stop.millis + 1 : s.stop.millis = r.start.millis - 1;
    }
    return this;
  }
  /**
   * Convert the transcript into a SRT compatible
   * string and downloadable blob
   */
  toSRT(e = {}) {
    let i = 1, s = "";
    for (const r of this.iter(e)) {
      const a = Gi(r.start.seconds), n = Gi(r.stop.seconds);
      s += `${i}
` + Ki(a) + " --> " + Ki(n) + `
${r.text}

`, i += 1;
    }
    return {
      text: s,
      blob: new Blob([s], { type: "text/plain;charset=utf8" })
    };
  }
  toJSON() {
    return this.groups.map(
      (e) => e.words.map((i) => ({
        token: i.text,
        start: i.start.millis,
        stop: i.stop.millis
      }))
    );
  }
  /**
   * Create a new Transcript containing the
   * first `{count}` words
   * @param count Defines the number of words required
   * @param startAtZero Defines if the first word should start at 0 milliseconds
   * @returns A new Transcript instance
   */
  slice(e, i = !0) {
    let s = 0;
    const r = [];
    for (const a of this.groups)
      for (const n of a.words)
        if (r.length == 0 && i && (s = n.start.millis), r.push(new Tt(n.text, n.start.millis - s, n.stop.millis - s)), r.length == e)
          return new se([new Le(r)]);
    return new se([new Le(r)]);
  }
  /**
   * Create a deep copy of the transcript
   * @returns A new Transcript instance
   */
  copy() {
    return se.fromJSON(this.toJSON());
  }
  static fromJSON(e) {
    const i = new se();
    for (const s of e) {
      const r = new Le();
      for (const a of s)
        r.words.push(new Tt(a.token, a.start, a.stop));
      i.groups.push(r);
    }
    return i;
  }
  /**
   * Create a Transcript from an input medium of the form:
   * `{ token: string; start: number; stop: number; }[][]`
   * @param input The input medium, can be a URL, Blob, or an array of captions
   * @returns A Transcript with processed captions
   */
  static async from(e) {
    if (Array.isArray(e))
      return se.fromJSON(e);
    if (e instanceof FileSystemFileHandle) {
      const s = await e.getFile();
      return se.fromJSON(JSON.parse(await s.text()));
    }
    if (e instanceof Blob)
      return se.fromJSON(JSON.parse(await e.text()));
    const i = await fetch(e);
    if (!i.ok)
      throw new Ee({
        code: "unexpectedIOError",
        message: "An unexpected error occurred while fetching the file"
      });
    return se.fromJSON(await i.json());
  }
  fromJSON(e) {
    B(typeof e == "object"), B(Array.isArray(e)), this.groups = [];
    for (const i of e) {
      const s = new Le();
      for (const r of i)
        s.words.push(new Tt(r.token, r.start, r.stop));
      this.groups.push(s);
    }
    return this;
  }
}
class Te {
  toJSON(e) {
    const i = {}, s = this.constructor.__serializableProperties || [], r = (a) => {
      if (a == null || a instanceof Blob || a instanceof FileSystemFileHandle)
        return a;
      if (Array.isArray(a))
        return a.map((n) => r(n)).filter((n) => n !== void 0);
      if (typeof a == "object" && "toJSON" in a)
        return a.toJSON();
      if (typeof a == "object") {
        const n = {};
        for (const o in a) {
          const c = r(a[o]);
          c !== void 0 && (n[o] = c);
        }
        return n;
      }
      return a;
    };
    return s.forEach(({ propertyKey: a, mapTo: n }) => {
      if (e?.includes(a))
        return;
      const o = this[a], c = n ?? a, l = r(o);
      l !== void 0 && (i[c] = l);
    }), i;
  }
  fromJSON(e) {
    return (this.constructor.__serializableProperties || []).forEach(({ propertyKey: s, deserializer: r, mapTo: a }) => {
      const n = a ?? s, o = r.fromJSON?.bind(r) ?? ((c) => c);
      e.hasOwnProperty(n) && (this[s] = o(e[n]));
    }), this;
  }
  static fromJSON(e) {
    return new this().fromJSON(e);
  }
}
function v(t = {}, e) {
  return function(i, s) {
    const r = i.constructor;
    r.__serializableProperties || (r.__serializableProperties = []), r.__serializableProperties = [
      ...r.__serializableProperties.filter((a) => a.propertyKey !== s),
      {
        propertyKey: s,
        deserializer: t,
        mapTo: e
      }
    ];
  };
}
function dt(t) {
  return class extends t {
    _handlers = {};
    on(i, s) {
      if (typeof s != "function")
        throw new Error("The callback of an event listener needs to be a function.");
      const r = crypto.randomUUID();
      return this._handlers[i] ? this._handlers[i][r] = s : this._handlers[i] = { [r]: s }, r;
    }
    off(i, ...s) {
      if (i) {
        if (i === "*") {
          this._handlers = {};
          return;
        }
        for (const r of Object.values(this._handlers))
          i in r && delete r[i];
        for (const r of s)
          this.off(r);
      }
    }
    emit(i, s) {
      const r = new CustomEvent(i, {
        detail: s
      });
      Object.defineProperty(r, "currentTarget", { writable: !1, value: this });
      for (const a in this._handlers[i] ?? {})
        this._handlers[i]?.[a](r);
      for (const a in this._handlers["*"] ?? {})
        this._handlers["*"]?.[a](r);
    }
    bubble(i) {
      return this.on("*", (s) => {
        i.emit(s.type, s.detail);
      });
    }
    resolve(i) {
      return (s, r) => {
        this.on("error", r), this.on(i, s);
      };
    }
  };
}
class qt extends dt(Te) {
  _key;
  _value;
  _store;
  loaded = !1;
  constructor(e, i, s) {
    super(), this._store = e, this._key = i, this.initValue(s);
  }
  get key() {
    return this._key;
  }
  get value() {
    return this._value;
  }
  set value(e) {
    this._value = e, this._store.set(this._key, e), this.emit("update", void 0);
  }
  async initValue(e) {
    e instanceof Promise ? this._value = await e : this._value = e, this.loaded = !0, this.emit("update", void 0);
  }
}
class pd {
  storageEngine;
  namespace;
  constructor(e, i = localStorage) {
    this.storageEngine = i, this.namespace = e;
  }
  define(e, i, s) {
    const r = this.get(e);
    return r === null ? (this.set(e, i), new qt(this, e, i)) : s && r != null ? new qt(this, e, s(r)) : new qt(this, e, r);
  }
  set(e, i) {
    this.storageEngine.setItem(
      this.getStorageId(e),
      JSON.stringify({
        value: i
      })
    );
  }
  get(e) {
    const i = this.storageEngine.getItem(this.getStorageId(e));
    return i ? JSON.parse(i).value : null;
  }
  remove(e) {
    this.storageEngine.removeItem(this.getStorageId(e));
  }
  getStorageId(e) {
    return this.namespace ? `${this.namespace}.${e}` : e;
  }
}
function js() {
  return class extends dt(class {
  }) {
  };
}
function gd(t, e) {
  return e == "lower" ? t.toLocaleLowerCase() : e == "upper" ? t.toUpperCase() : t;
}
function $e(t = "#000000", e = 100) {
  return `${t}${Math.round(e / 100 * 255).toString(16)}`;
}
function _(t, e) {
  return typeof t == "number" ? t : Number.parseFloat(t.replace("%", "")) * e / 100;
}
function pa(t) {
  const e = t.startsWith("#") ? t.slice(1) : t, i = parseInt(e, 16), s = i >> 16 & 255, r = i >> 8 & 255, a = i & 255;
  return { r: s, g: r, b: a };
}
function ga(t, e, i) {
  return `#${((1 << 24) + (Math.round(t) << 16) + (Math.round(e) << 8) + Math.round(i)).toString(16).slice(1)}`;
}
function Ii(t, e) {
  switch (e) {
    case "ease-in":
      return t * t;
    case "ease-out":
      return t * (2 - t);
    case "ease-in-out":
      return t < 0.5 ? 2 * t * t : -1 + (4 - 2 * t) * t;
    case "ease-out-in":
      if (t < 0.5) {
        const i = t * 2;
        return i * (2 - i) / 2;
      } else {
        const i = (t - 0.5) * 2;
        return i * i / 2 + 0.5;
      }
    default:
      return t;
  }
}
function nt(t, e, i) {
  return t + (e - t) * i;
}
function Yi(t) {
  const e = pa(t);
  return wa(e.r, e.g, e.b);
}
function wa(t, e, i) {
  t /= 255, e /= 255, i /= 255;
  const s = Math.max(t, e, i), r = Math.min(t, e, i);
  let a = 0, n = 0;
  const o = (s + r) / 2;
  if (s !== r) {
    const c = s - r;
    switch (n = o > 0.5 ? c / (2 - s - r) : c / (s + r), s) {
      case t:
        a = (e - i) / c + (e < i ? 6 : 0);
        break;
      case e:
        a = (i - t) / c + 2;
        break;
      case i:
        a = (t - e) / c + 4;
        break;
    }
    a /= 6;
  }
  return { h: Math.round(a * 360), s: Math.round(n * 100), l: Math.round(o * 100) };
}
function ya(t, e, i) {
  e /= 100, i /= 100, t = (t + 360) % 360;
  function s(l, d, h) {
    return h < 0 && (h += 1), h > 1 && (h -= 1), h < 1 / 6 ? l + (d - l) * 6 * h : h < 1 / 2 ? d : h < 2 / 3 ? l + (d - l) * (2 / 3 - h) * 6 : l;
  }
  const r = i < 0.5 ? i * (1 + e) : i + e - i * e, a = 2 * i - r, n = s(a, r, t / 360 + 1 / 3), o = s(a, r, t / 360), c = s(a, r, t / 360 - 1 / 3);
  return ga(
    Math.round(n * 255),
    Math.round(o * 255),
    Math.round(c * 255)
  );
}
function ba(t, e) {
  const { frames: i, extrapolate: s = "clamp", easing: r } = t;
  if (e.millis <= O(i[0].time).millis)
    return i[0].value;
  if (e.millis >= O(i[i.length - 1].time).millis)
    return i[i.length - 1].value;
  let a, n;
  for (let m = 0; m < i.length - 1; m++)
    if (e.millis >= O(i[m].time).millis && e.millis <= O(i[m + 1].time).millis) {
      a = i[m], n = i[m + 1];
      break;
    }
  if (!a || !n)
    throw new Error("Unexpected error in keyframe interpolation");
  const o = (e.millis - O(a.time).millis) / (O(n.time).millis - O(a.time).millis), c = Ii(o, a.easing ?? r), l = Yi(a.value), d = Yi(n.value);
  let h = l.h, u = d.h;
  return Math.abs(u - h) > 180 && (h < u ? h += 360 : u += 360), ya(
    nt(h, u, c),
    nt(l.s, d.s, c),
    nt(l.l, d.l, c)
  );
}
function qs(t, e) {
  const { frames: i, extrapolate: s = "clamp", easing: r } = t;
  if (e.millis <= O(i[0].time).millis)
    return i[0].value;
  if (e.millis >= O(i[i.length - 1].time).millis)
    return i[i.length - 1].value;
  let a, n;
  for (let l = 0; l < i.length - 1; l++)
    if (e.millis >= O(i[l].time).millis && e.millis <= O(i[l + 1].time).millis) {
      a = i[l], n = i[l + 1];
      break;
    }
  if (!a || !n)
    throw new Error("Unexpected error in keyframe interpolation");
  const o = (e.millis - O(a.time).millis) / (O(n.time).millis - O(a.time).millis), c = Ii(o, a.easing ?? r);
  return typeof a.value == "number" && typeof n.value == "number" ? nt(a.value, n.value, c) : `${nt(Ji(a.value), Ji(n.value), c)}%`;
}
function Ji(t) {
  return typeof t == "number" ? t : Number(t.replace("%", ""));
}
function ka(t, e) {
  const { frames: i, extrapolate: s = "clamp" } = t;
  if (e.millis <= O(i[0].time).millis)
    return i[0].value;
  if (e.millis >= O(i[i.length - 1].time).millis)
    return i[i.length - 1].value;
  let r, a;
  for (let h = 0; h < i.length - 1; h++)
    if (e.millis >= O(i[h].time).millis && e.millis <= O(i[h + 1].time).millis) {
      r = i[h], a = i[h + 1];
      break;
    }
  if (!r || !a)
    throw new Error("Unexpected error in keyframe interpolation");
  const n = (e.millis - O(r.time).millis) / (O(a.time).millis - O(r.time).millis), o = Ii(n, r.easing), c = a.value, l = c.length, d = Math.floor(o * l);
  return c.slice(0, d);
}
var Sa = Object.defineProperty, ht = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && Sa(e, i, r), r;
};
class we extends Te {
  /**
   * Unique identifier of the mask
   */
  id = crypto.randomUUID();
  type = "base";
  width;
  height;
  fillRule;
  animations = [];
  clip;
  renderer;
  constructor({ width: e = 100, height: i = 100 } = {}) {
    super(), this.width = e, this.height = i;
  }
  connect(e) {
    return this.clip = e, this;
  }
  draw(e) {
    return this.renderer = e, new Path2D();
  }
  animate(e) {
    for (const i of this.animations) {
      const s = i?.frames[0].value;
      (typeof s == "number" || typeof s == "string" && s.match(/^[0-9]+(\.[0-9]+)?%$/)) && (this[i.key] = qs(i, e.subtract(this.start)));
    }
    return this;
  }
  get start() {
    return this.clip?.start ?? new E();
  }
  get stop() {
    return this.clip?.stop ?? new E();
  }
  get size() {
    return {
      width: _(this.width, this.renderer?.width ?? 0),
      height: _(this.height, this.renderer?.height ?? 0)
    };
  }
  get bounds() {
    const { width: e, height: i } = this.size;
    return [
      { x: 0, y: 0 },
      { x: e, y: 0 },
      { x: e, y: i },
      { x: 0, y: i }
    ];
  }
  detach() {
    return this.clip && (this.clip.mask = void 0, this.clip = void 0), this;
  }
}
ht([
  v()
], we.prototype, "type");
ht([
  v()
], we.prototype, "width");
ht([
  v()
], we.prototype, "height");
ht([
  v()
], we.prototype, "fillRule");
ht([
  v()
], we.prototype, "animations");
var va = Object.defineProperty, Mt = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && va(e, i, r), r;
};
class ut extends we {
  type = "rectangle";
  x;
  y;
  radius;
  constructor({ x: e = 0, y: i = 0, radius: s = 0, animations: r, ...a } = {}) {
    super(a), this.x = e, this.y = i, this.radius = s, this.animations = r ?? [];
  }
  draw(e) {
    const i = super.draw(e);
    if (this.radius) {
      const s = _(this.width, e.width), r = _(this.height, e.height), a = _(this.radius, Math.min(s, r));
      i.roundRect(
        _(this.x, e.width) * e.resolution,
        _(this.y, e.height) * e.resolution,
        s * e.resolution,
        r * e.resolution,
        a * e.resolution
      );
    } else
      i.rect(
        _(this.x, e.width) * e.resolution,
        _(this.y, e.height) * e.resolution,
        _(this.width, e.width) * e.resolution,
        _(this.height, e.height) * e.resolution
      );
    return i;
  }
  get bounds() {
    return super.bounds.map((e) => ({
      x: e.x + _(this.x, this.renderer?.width ?? 0),
      y: e.y + _(this.y, this.renderer?.height ?? 0)
    }));
  }
}
Mt([
  v()
], ut.prototype, "type");
Mt([
  v()
], ut.prototype, "x");
Mt([
  v()
], ut.prototype, "y");
Mt([
  v()
], ut.prototype, "radius");
class Ta {
  static fromJSON(e) {
    switch (B(typeof e == "object"), B(e != null), B("type" in e), e.type) {
      case "rectangle":
        return ut.fromJSON(e);
      case "circle":
        return At.fromJSON(e);
      default:
        return we.fromJSON(e);
    }
  }
}
var xa = Object.defineProperty, Ei = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && xa(e, i, r), r;
};
class At extends we {
  type = "circle";
  cx;
  cy;
  constructor({ cx: e, cy: i, x: s = 0, y: r = 0, radius: a, animations: n, ...o } = {}) {
    super(o), this.cx = e ?? s, this.cy = i ?? r, a && (this.radius = a), this.animations = n ?? [];
  }
  set x(e) {
    this.cx = e;
  }
  set y(e) {
    this.cy = e;
  }
  get x() {
    return this.cx;
  }
  get y() {
    return this.cy;
  }
  get radius() {
    const e = this.size;
    return Math.min(e.width, e.height) * 0.5;
  }
  set radius(e) {
    typeof e == "number" ? (this.height = e * 2, this.width = e * 2) : (this.height = `${Number(e.replace("%", "")) * 2}%`, this.width = `${Number(e.replace("%", "")) * 2}%`);
  }
  draw(e) {
    const i = super.draw(e);
    return i.ellipse(
      _(this.cx, e.width) * e.resolution,
      _(this.cy, e.height) * e.resolution,
      _(this.width, e.width) * e.resolution * 0.5,
      _(this.height, e.height) * e.resolution * 0.5,
      0,
      0,
      2 * Math.PI
    ), i;
  }
  get bounds() {
    const { width: e, height: i } = this.size, { cx: s, cy: r } = this, a = _(s, this.renderer?.width ?? 0) - e * 0.5, n = _(r, this.renderer?.height ?? 0) - i * 0.5;
    return [
      { x: a, y: n },
      { x: a + e, y: n },
      { x: a + e, y: n + i },
      { x: a, y: n + i }
    ];
  }
}
Ei([
  v()
], At.prototype, "type");
Ei([
  v()
], At.prototype, "cx");
Ei([
  v()
], At.prototype, "cy");
var Ca = Object.defineProperty, _a = Object.getOwnPropertyDescriptor, zt = (t, e, i, s) => {
  for (var r = s > 1 ? void 0 : s ? _a(e, i) : e, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = (s ? n(e, i, r) : n(r)) || r);
  return s && r && Ca(e, i, r), r;
};
class Xe extends Te {
  _background = "#000000";
  /**
  * The canvas element
  */
  canvas = document.createElement("canvas");
  /**
   * The main 2d context of the canvas
   */
  context = this.canvas.getContext("2d");
  resolution = 1;
  width = 0;
  height = 0;
  constructor({
    width: e = 1920,
    height: i = 1080,
    background: s = "#000000",
    resolution: r = 1
  } = {}) {
    super(), this.resolution = r, this.background = s, this.resize(e, i);
  }
  get background() {
    return this._background;
  }
  set background(e) {
    this.background != e && (this.canvas.style.background = e, this._background = e);
  }
  /**
   * Resize the canvas
   */
  resize(e = this.width, i = this.height, s = this.resolution) {
    const r = Math.round(e / 2) * 2, a = Math.round(i / 2) * 2;
    return (r !== this.width || a !== this.height || s !== this.resolution) && (this.width = r, this.height = a, this.resolution = s, this.canvas.width = Math.round(r * this.resolution / 2) * 2, this.canvas.height = Math.round(a * this.resolution / 2) * 2, this.context.imageSmoothingEnabled = !1), this;
  }
  clear(e) {
    let i = 0, s = 0, r = this.width * this.resolution, a = this.height * this.resolution;
    return this.context.fillStyle = this._background, e && (i = _(e.x ?? 0, this.width) * this.resolution, s = _(e.y ?? 0, this.height) * this.resolution, r = _(e.width, this.width) * this.resolution, a = _(e.height, this.height) * this.resolution), this.context.clearRect(i, s, r, a), this.context.fillRect(i, s, r, a), this;
  }
  rect(e) {
    if (this.context.beginPath(), e.radius) {
      const i = _(e.width, this.width), s = _(e.height, this.height), r = Math.max(_(e.radius, Math.min(i, s)), 0);
      this.context.roundRect(
        _(e.x ?? 0, this.width) * this.resolution | 0,
        _(e.y ?? 0, this.height) * this.resolution | 0,
        i * this.resolution | 0,
        s * this.resolution | 0,
        r * this.resolution | 0
      );
    } else
      this.context.rect(
        _(e.x ?? 0, this.width) * this.resolution | 0,
        _(e.y ?? 0, this.height) * this.resolution | 0,
        _(e.width, this.width) * this.resolution | 0,
        _(e.height, this.height) * this.resolution | 0
      );
    return this.context.closePath(), this;
  }
  circle(e) {
    return this.context.beginPath(), "height" in e ? this.context.ellipse(
      _(e.cx, this.width) * this.resolution | 0,
      _(e.cy, this.height) * this.resolution | 0,
      _(e.width, this.width) * this.resolution | 0,
      _(e.height, this.height) * this.resolution | 0,
      0,
      0,
      Math.PI * 2
    ) : this.context.arc(
      _(e.cx, this.width) * this.resolution | 0,
      _(e.cy, this.height) * this.resolution | 0,
      _(e.radius, Math.min(this.width, this.height) * 0.5) * this.resolution | 0,
      0,
      Math.PI * 2
    ), this.context.closePath(), this;
  }
  image(e, i) {
    const { x: s = 0, y: r = 0, width: a, height: n, rotation: o } = i, c = _(s, this.width) * this.resolution | 0, l = _(r, this.height) * this.resolution | 0, d = _(a, this.width) * this.resolution | 0, h = _(n, this.height) * this.resolution | 0;
    if (o !== void 0 && o !== 0) {
      this.context.save(), this.context.translate(c + d / 2, l + h / 2);
      const u = o * Math.PI / 180;
      this.context.rotate(u);
      const m = Math.abs(o % 180) === 90;
      this.context.drawImage(
        e,
        (m ? -h / 2 : -d / 2) | 0,
        (m ? -d / 2 : -h / 2) | 0,
        (m ? h : d) | 0,
        (m ? d : h) | 0
      ), this.context.restore();
    } else
      this.context.drawImage(e, c, l, d, h);
    return this;
  }
  clip(e, i) {
    return e instanceof we ? this.context.clip(e.draw(this), e.fillRule) : e && this.context.clip(e, i), this;
  }
  opacity(e) {
    return this.context.globalAlpha *= e / 100, this;
  }
  transform(e) {
    return e ? (e.translate && this.context.translate(
      _(e.translate.x, this.width) * this.resolution | 0,
      _(e.translate.y, this.height) * this.resolution | 0
    ), e.rotate && this.context.rotate(e.rotate * Math.PI / 180), e.scale && this.context.scale(
      _(e.scale.x, this.width),
      _(e.scale.y, this.height)
    ), this) : (this.context.setTransform(1, 0, 0, 1, 0, 0), this);
  }
  blendMode(e) {
    return e && (this.context.globalCompositeOperation = e), this;
  }
  save() {
    return this.context.save(), this;
  }
  restore() {
    return this.context.restore(), this;
  }
  filter(e) {
    return e ? (this.context.filter = e, this) : this;
  }
  fill(e, i = !1) {
    if (!e)
      return this.context.fillStyle = "transparent", this;
    if (typeof e.color == "string")
      this.context.fillStyle = $e(e.color, e.opacity);
    else if ("type" in e.color) {
      const s = this.createGradient(e.color);
      this.context.fillStyle = s;
    } else if ("image" in e.color) {
      const s = this.context.createPattern(e.color.image, e.color.repetition);
      this.context.fillStyle = s ?? "";
    }
    return i && this.context.fill(), this;
  }
  shadow(e) {
    return e ? (this.context.fillStyle = this.context.shadowColor = $e(e.color, e.opacity), this.context.shadowOffsetX = (e.offsetX ?? 0) * this.resolution * this.textScale, this.context.shadowOffsetY = (e.offsetY ?? 0) * this.resolution * this.textScale, this.context.shadowBlur = (e.blur ?? 24) * this.resolution * this.textScale, this) : (this.context.shadowColor = "transparent", this.context.shadowBlur = 0, this.context.shadowOffsetX = 0, this.context.shadowOffsetY = 0, this);
  }
  stroke(e, i = !1) {
    return e ? (this.context.strokeStyle = $e(e.color, e.opacity), this.context.lineWidth = (e.width ?? 1) * this.resolution * this.textScale, e.lineCap && (this.context.lineCap = e.lineCap), e.lineJoin && (this.context.lineJoin = e.lineJoin), e.miterLimit && (this.context.miterLimit = e.miterLimit), i && this.context.stroke(), this) : (this.context.strokeStyle = "transparent", this.context.lineWidth = 0, this.context.lineCap = "butt", this.context.lineJoin = "miter", this.context.miterLimit = 10, this);
  }
  /**
   * Add the renderer to the dom
   */
  mount(e) {
    e.appendChild(this.canvas);
  }
  /**
   * Remove the renderer from the dom
   */
  unmount() {
    this.canvas.parentElement?.removeChild(this.canvas);
  }
  createGradient(e) {
    let i;
    return e.type === "linear" ? i = this.context.createLinearGradient(0, 0, this.canvas.width, 0) : i = this.context.createRadialGradient(
      this.canvas.width / 2,
      this.canvas.height / 2,
      0,
      this.canvas.width / 2,
      this.canvas.height / 2,
      this.canvas.width / 2
    ), e.stops.forEach((s) => {
      i.addColorStop(s.offset, s.color);
    }), i;
  }
  /**
   * Draw a watermark on the canvas
   */
  watermark(e) {
    !e || !e.length || (this.context.save(), this.context.font = `${46 * this.resolution}px Verdana`, this.context.fillStyle = "#ffffff", this.context.textAlign = "center", this.context.textBaseline = "middle", this.context.strokeStyle = "#000000", this.context.lineWidth = 8 * this.resolution, this.context.strokeText(e, this.width * this.resolution * 0.5, this.height * this.resolution * 0.9), this.context.fillText(e, this.width * this.resolution * 0.5, this.height * this.resolution * 0.9), this.context.restore());
  }
  /**
   * The scale of the text
   * @deprecated
   */
  textScale = 4;
  /**
   * @deprecated use Text node instead
   */
  text(e) {
    return e.font && (this.context.font = this.createFontString(
      e.font.style,
      e.font.weight,
      e.font.size * this.resolution,
      e.font.family
    )), e.color && (this.context.fillStyle = e.color), e.alignment && (this.context.textAlign = e.alignment), e.baseline && (this.context.textBaseline = e.baseline), this;
  }
  /**
   * @deprecated use Text node instead
   */
  strokeText(e, i) {
    return this.context.strokeText(
      e,
      _(i.x, this.width) * this.resolution,
      _(i.y, this.height) * this.resolution
    ), this;
  }
  /**
   * @deprecated use Text node instead
   */
  fillText(e, i) {
    return this.context.fillText(
      e,
      _(i.x, this.width) * this.resolution,
      _(i.y, this.height) * this.resolution
    ), this;
  }
  /**
   * @deprecated use Text node instead
   */
  measureText(e, i) {
    return this.context.font = this.createFontString(
      i.style,
      i.weight,
      i.size,
      i.family
    ), this.context.measureText(e);
  }
  /**
   * @deprecated use Text node instead
   */
  createFontString(e, i, s, r) {
    return `${e ?? "normal"} ${i ?? "400"} ${(s ?? 16) * this.textScale}px ${r ?? "Arial"}`.trim();
  }
}
zt([
  v()
], Xe.prototype, "resolution", 2);
zt([
  v()
], Xe.prototype, "width", 2);
zt([
  v()
], Xe.prototype, "height", 2);
zt([
  v()
], Xe.prototype, "background", 1);
class Qs {
  context;
  /**
   * Offset in **seconds** relative to the hardware time when the playback started
   */
  hardwareOffset = 0;
  /**
   * Offset in **seconds** relative to 0 when the playback started
   */
  playbackOffset = 0;
  /**
   * Defines the fps used for rendering.
   */
  playbackFps = Rt;
  /**
   * The fps used when the ticker is inactive (not playing)
   */
  inactiveFps = oa;
  /**
   * Defines the current state of the ticker
   */
  playing = !1;
  /**
   * Defines if the ticker is active
   */
  stopped = !0;
  /**
   * User defined fixed duration
   * 
   * @deprecated Use markers.stop instead
   */
  duration;
  /**
   * The last time the timer was updated
   */
  lastFrameTime = 0;
  /**
   * Creates a new ticker
   * @param callback - The function to call when the ticker is updated
   */
  constructor(e = {}) {
    this.callback = e.callback, this.context = e.context ?? new AudioContext(), this.playbackFps = e.fps ?? 30;
  }
  /**
   * The current time of the hardware in seconds
   */
  get hardwareTime() {
    return this.context.currentTime;
  }
  /**
   * The current time of the playback in **seconds** relative to 0
   */
  get playbackTime() {
    return this.playing ? this.hardwareTime - this.hardwareOffset + this.playbackOffset : this.playbackOffset;
  }
  get playbackTimestamp() {
    return new E(0, this.playbackTime);
  }
  /**
   * The current frame that the playback is set to
   */
  get playbackFrame() {
    return Math.floor(this.playbackTime * this.playbackFps);
  }
  /**
   * Starts the animation loop
   */
  start() {
    this.stopped && (this.stopped = !1, this.timer());
  }
  /**
   * Stops the animation loop
   */
  stop() {
    this.stopped = !0;
  }
  /**
   * Starts the frame incrementation
   */
  async play() {
    this.playing || this.stopped || (this.resumeAudioContext(), this.hardwareOffset = this.hardwareTime, this.playing = !0);
  }
  /**
   * Pauses the frame incrementation
   */
  async pause() {
    this.playing && (this.playbackOffset = this.playbackTime, this.playing = !1);
  }
  /**
   * The animation loop
   */
  async timer(e = 0) {
    if (this.stopped) return;
    const s = 1e3 / (this.playing ? this.playbackFps : this.inactiveFps);
    e - this.lastFrameTime >= s && (this.lastFrameTime = e, await this.callback?.()), requestAnimationFrame(this.timer.bind(this));
  }
  async resumeAudioContext() {
    this.context.state === "suspended" && await this.context.resume();
  }
}
const Ia = {
  fromJSON: (t) => {
    B(typeof t == "object"), B(t != null);
    const e = {};
    for (const [i, s] of Object.entries(t))
      B(typeof s == "number"), e[i] = new E(s);
    return e;
  }
};
function Ea(t) {
  const e = new E();
  for (const i of t)
    i.disabled || i.stop.frames > e.frames && (e.frames = i.stop.frames);
  return e;
}
var Pa = Object.defineProperty, Fe = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && Pa(e, i, r), r;
};
class le extends Te {
  id = crypto.randomUUID();
  data = {};
  type = "base";
  mimeType;
  input;
  name;
  createdAt = /* @__PURE__ */ new Date();
  constructor(e) {
    super(), this.input = e.input, this.mimeType = e.mimeType, e.name ? this.name = e.name : typeof this.input == "string" ? this.name = this.input.split("/").at(-1) ?? "" : this.input instanceof File ? this.name = this.input.name : this.input instanceof FileSystemFileHandle ? this.name = this.input.name : this.name = "UNTITLED_BLOB";
  }
  async init(e = {}) {
  }
  /**
   * Get the source as an array buffer
   */
  async arrayBuffer() {
    return typeof this.input == "string" ? await (await fetch(this.input)).arrayBuffer() : this.input instanceof Blob ? await this.input.arrayBuffer() : await (await this.input.getFile()).arrayBuffer();
  }
  /**
   * Downloads the file
   */
  async download() {
    $s(this.input, this.name);
  }
  /**
   * Get a visulization of the source
   * as an html element
   * @deprecated
   */
  async thumbnail() {
    return document.createElement("div");
  }
  /**
   * Create a checkpoint of the source. May include Blob or FileSystemFileHandle.
   * @param middleware A function to modify the checkpoint data
   * @returns A serialized representation of the source
   */
  async createCheckpoint() {
    return this.toJSON();
  }
}
Fe([
  v()
], le.prototype, "id");
Fe([
  v()
], le.prototype, "data");
Fe([
  v()
], le.prototype, "type");
Fe([
  v()
], le.prototype, "mimeType");
Fe([
  v()
], le.prototype, "input");
Fe([
  v()
], le.prototype, "name");
Fe([
  v(Ot)
], le.prototype, "createdAt");
function Pi(t) {
  class e extends t {
    /**
     * The height of the source
     */
    height = 1080;
    /**
     * The width of the source
     */
    width = 1920;
    /**
     * The aspect ratio of the source
     */
    get aspectRatio() {
      return this.width / this.height;
    }
  }
  return e;
}
async function Fa(t) {
  if (t instanceof FileSystemFileHandle)
    return (await t.getFile()).type;
  if (t instanceof Blob)
    return t.type;
  if (t.startsWith("<html>"))
    return "text/html";
  let e;
  try {
    e = await fetch(t, { method: "HEAD" });
  } catch {
    console.info("Using AbortController fallback");
    const r = new AbortController();
    e = await fetch(t, { signal: r.signal }), r.abort();
  }
  if (!e.ok)
    throw new Error(`HTTP error! Status: ${e.status}`);
  const i = e.headers.get("Content-Type");
  if (!i)
    throw new Ee({
      message: "No content type found",
      code: "no_content_type_found"
    });
  return i;
}
function f(t) {
  if (!t)
    throw new Error("Assertion failed.");
}
var Fi = (t) => {
  const e = (t % 360 + 360) % 360;
  if (e === 0 || e === 90 || e === 180 || e === 270)
    return e;
  throw new Error(`Invalid rotation ${t}.`);
}, H = (t) => t && t[t.length - 1], Qe = (t) => t >= 0 && t < 2 ** 32, Se = class Xs {
  constructor(e) {
    this.bytes = e, this.pos = 0;
  }
  seekToByte(e) {
    this.pos = 8 * e;
  }
  readBit() {
    const e = Math.floor(this.pos / 8), i = this.bytes[e] ?? 0, s = 7 - (this.pos & 7), r = (i & 1 << s) >> s;
    return this.pos++, r;
  }
  readBits(e) {
    let i = 0;
    for (let s = 0; s < e; s++)
      i <<= 1, i |= this.readBit();
    return i;
  }
  readAlignedByte() {
    if (this.pos % 8 !== 0)
      throw new Error("Bitstream is not byte-aligned.");
    const e = this.pos / 8, i = this.bytes[e] ?? 0;
    return this.pos += 8, i;
  }
  skipBits(e) {
    this.pos += e;
  }
  getBitsLeft() {
    return this.bytes.length * 8 - this.pos;
  }
  clone() {
    const e = new Xs(this.bytes);
    return e.pos = this.pos, e;
  }
}, I = (t) => {
  let e = 0;
  for (; t.readBit() === 0 && e < 32; )
    e++;
  if (e >= 32)
    throw new Error("Invalid exponential-Golomb code.");
  return (1 << e) - 1 + t.readBits(e);
}, ot = (t) => {
  const e = I(t);
  return e & 1 ? e + 1 >> 1 : -(e >> 1);
}, Ra = (t, e, i, s) => {
  for (let r = e; r < i; r++) {
    const a = Math.floor(r / 8);
    let n = t[a];
    const o = 7 - (r & 7);
    n &= ~(1 << o), n |= (s & 1 << i - r - 1) >> i - r - 1 << o, t[a] = n;
  }
}, te = (t) => t instanceof ArrayBuffer ? new Uint8Array(t) : new Uint8Array(t.buffer, t.byteOffset, t.byteLength), Re = (t) => t instanceof ArrayBuffer ? new DataView(t) : new DataView(t.buffer, t.byteOffset, t.byteLength), he = new TextEncoder(), Ri = (t) => Object.fromEntries(Object.entries(t).map(([e, i]) => [i, e])), Ke = {
  bt709: 1,
  // ITU-R BT.709
  bt470bg: 5,
  // ITU-R BT.470BG
  smpte170m: 6,
  // ITU-R BT.601 525 - SMPTE 170M
  bt2020: 9,
  // ITU-R BT.202
  smpte432: 12
  // SMPTE EG 432-1
}, Ks = Ri(Ke), Ge = {
  bt709: 1,
  // ITU-R BT.709
  smpte170m: 6,
  // SMPTE 170M
  linear: 8,
  // Linear transfer characteristics
  "iec61966-2-1": 13,
  // IEC 61966-2-1
  pg: 16,
  // Rec. ITU-R BT.2100-2 perceptual quantization (PQ) system
  hlg: 18
  // Rec. ITU-R BT.2100-2 hybrid loggamma (HLG) system
}, Gs = Ri(Ge), Ye = {
  rgb: 0,
  // Identity
  bt709: 1,
  // ITU-R BT.709
  bt470bg: 5,
  // ITU-R BT.470BG
  smpte170m: 6,
  // SMPTE 170M
  "bt2020-ncl": 9
  // ITU-R BT.2020-2 (non-constant luminance)
}, Ys = Ri(Ye), Js = (t) => !!t && !!t.primaries && !!t.transfer && !!t.matrix && t.fullRange !== void 0, Ut = (t) => t instanceof ArrayBuffer || typeof SharedArrayBuffer < "u" && t instanceof SharedArrayBuffer || ArrayBuffer.isView(t) && !(t instanceof DataView), ft = class {
  constructor() {
    this.currentPromise = Promise.resolve();
  }
  async acquire() {
    let t;
    const e = new Promise((s) => {
      t = s;
    }), i = this.currentPromise;
    return this.currentPromise = e, await i, t;
  }
}, Zi = (t) => [...t].map((e) => e.toString(16).padStart(2, "0")).join(""), es = (t) => (t = t >> 1 & 1431655765 | (t & 1431655765) << 1, t = t >> 2 & 858993459 | (t & 858993459) << 2, t = t >> 4 & 252645135 | (t & 252645135) << 4, t = t >> 8 & 16711935 | (t & 16711935) << 8, t = t >> 16 & 65535 | (t & 65535) << 16, t >>> 0), X = (t, e, i) => {
  let s = 0, r = t.length - 1, a = -1;
  for (; s <= r; ) {
    const n = s + r >> 1, o = i(t[n]);
    o === e ? (a = n, r = n - 1) : o < e ? s = n + 1 : r = n - 1;
  }
  return a;
}, N = (t, e, i) => {
  let s = -1, r = 0, a = t.length - 1;
  for (; r <= a; ) {
    const n = r + (a - r + 1) / 2 | 0;
    i(t[n]) <= e ? (s = n, r = n + 1) : a = n - 1;
  }
  return s;
}, Z = () => {
  let t, e;
  return { promise: new Promise((s, r) => {
    t = s, e = r;
  }), resolve: t, reject: e };
}, Ba = (t, e) => {
  const i = t.indexOf(e);
  i !== -1 && t.splice(i, 1);
}, Zs = (t, e) => {
  for (let i = t.length - 1; i >= 0; i--)
    if (e(t[i]))
      return t[i];
}, er = (t, e) => {
  for (let i = t.length - 1; i >= 0; i--)
    if (e(t[i]))
      return i;
  return -1;
}, Oa = async function* (t) {
  Symbol.iterator in t ? yield* t[Symbol.iterator]() : yield* t[Symbol.asyncIterator]();
}, Ma = (t) => {
  if (!(Symbol.iterator in t) && !(Symbol.asyncIterator in t))
    throw new TypeError("Argument must be an iterable or async iterable.");
}, tr = (t, e, i) => {
  const s = t.getUint8(e), r = t.getUint8(e + 1), a = t.getUint8(e + 2);
  return i ? s | r << 8 | a << 16 : s << 16 | r << 8 | a;
}, Aa = (t, e, i) => tr(t, e, i) << 8 >> 8, ir = (t, e, i, s) => {
  i = i >>> 0, i = i & 16777215, s ? (t.setUint8(e, i & 255), t.setUint8(e + 1, i >>> 8 & 255), t.setUint8(e + 2, i >>> 16 & 255)) : (t.setUint8(e, i >>> 16 & 255), t.setUint8(e + 1, i >>> 8 & 255), t.setUint8(e + 2, i & 255));
}, za = (t, e, i, s) => {
  i = J(i, -8388608, 8388607), i < 0 && (i = i + 16777216 & 16777215), ir(t, e, i, s);
}, Ua = (t, e, i, s) => {
  t.setUint32(e + 0, i, !0), t.setInt32(e + 4, Math.floor(i / 2 ** 32), !0);
}, _t = (t, e) => ({
  async next() {
    const i = await t.next();
    return i.done ? { value: void 0, done: !0 } : { value: e(i.value), done: !1 };
  },
  return() {
    return t.return();
  },
  throw(i) {
    return t.throw(i);
  },
  [Symbol.asyncIterator]() {
    return this;
  }
}), J = (t, e, i) => Math.max(e, Math.min(i, t)), fe = "und", Bi = (t, e) => {
  const i = 10 ** e;
  return Math.round(t * i) / i;
}, ni = (t, e) => Math.round(t / e) * e, Da = (t) => {
  let e = 0;
  for (; t; )
    e++, t >>= 1;
  return e;
}, Na = /^[a-z]{3}$/, Oi = (t) => Na.test(t), be = 1e6 * (1 + Number.EPSILON), oi = (t, e) => {
  const i = { ...t };
  for (const s in e)
    typeof t[s] == "object" && t[s] !== null && typeof e[s] == "object" && e[s] !== null ? i[s] = oi(
      t[s],
      e[s]
    ) : i[s] = e[s];
  return i;
}, ts = async (t, e, i) => {
  let s = 0;
  for (; ; )
    try {
      return await fetch(t, e);
    } catch (r) {
      console.error("Retrying failed fetch. Error:", r), s++;
      const a = i(s);
      if (a === null)
        throw r;
      if (!Number.isFinite(a) || a < 0)
        throw new TypeError("Retry delay must be a non-negative finite number.");
      a > 0 && await new Promise((n) => setTimeout(n, 1e3 * a));
    }
}, Va = (t, e) => {
  const i = t < 0 ? -1 : 1;
  t = Math.abs(t);
  let s = 0, r = 1, a = 1, n = 0, o = t;
  for (; ; ) {
    const c = Math.floor(o), l = c * a + s, d = c * n + r;
    if (d > e)
      return {
        numerator: i * a,
        denominator: n
      };
    if (s = a, r = n, a = l, n = d, o = 1 / (o - c), !isFinite(o))
      break;
  }
  return {
    numerator: i * a,
    denominator: n
  };
}, Dt = class {
  constructor() {
    this.currentPromise = Promise.resolve();
  }
  call(t) {
    return this.currentPromise = this.currentPromise.then(t);
  }
}, La = class {
  /** Returns true iff the encoder can encode the given codec configuration. */
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  static supports(t, e) {
    return !1;
  }
}, sr = class {
  /** Returns true iff the encoder can encode the given codec configuration. */
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  static supports(t, e) {
    return !1;
  }
}, rr = [], ar = [], It = [], Et = [], Wa = (t) => {
  if (t.prototype instanceof La)
    It.push(t);
  else if (t.prototype instanceof sr)
    Et.push(t);
  else
    throw new TypeError("Encoder must be a CustomVideoEncoder or CustomAudioEncoder.");
}, re = [
  "avc",
  "hevc",
  "vp9",
  "av1",
  "vp8"
], K = [
  "pcm-s16",
  // We don't prefix 'le' so we're compatible with the WebCodecs-registered PCM codec strings
  "pcm-s16be",
  "pcm-s24",
  "pcm-s24be",
  "pcm-s32",
  "pcm-s32be",
  "pcm-f32",
  "pcm-f32be",
  "pcm-u8",
  "pcm-s8",
  "ulaw",
  "alaw"
], Mi = [
  "aac",
  "opus",
  "mp3",
  "vorbis",
  "flac"
], ne = [
  ...Mi,
  ...K
], ve = [
  "webvtt"
], is = [
  { maxMacroblocks: 99, maxBitrate: 64e3, level: 10 },
  // Level 1
  { maxMacroblocks: 396, maxBitrate: 192e3, level: 11 },
  // Level 1.1
  { maxMacroblocks: 396, maxBitrate: 384e3, level: 12 },
  // Level 1.2
  { maxMacroblocks: 396, maxBitrate: 768e3, level: 13 },
  // Level 1.3
  { maxMacroblocks: 396, maxBitrate: 2e6, level: 20 },
  // Level 2
  { maxMacroblocks: 792, maxBitrate: 4e6, level: 21 },
  // Level 2.1
  { maxMacroblocks: 1620, maxBitrate: 4e6, level: 22 },
  // Level 2.2
  { maxMacroblocks: 1620, maxBitrate: 1e7, level: 30 },
  // Level 3
  { maxMacroblocks: 3600, maxBitrate: 14e6, level: 31 },
  // Level 3.1
  { maxMacroblocks: 5120, maxBitrate: 2e7, level: 32 },
  // Level 3.2
  { maxMacroblocks: 8192, maxBitrate: 2e7, level: 40 },
  // Level 4
  { maxMacroblocks: 8192, maxBitrate: 5e7, level: 41 },
  // Level 4.1
  { maxMacroblocks: 8704, maxBitrate: 5e7, level: 42 },
  // Level 4.2
  { maxMacroblocks: 22080, maxBitrate: 135e6, level: 50 },
  // Level 5
  { maxMacroblocks: 36864, maxBitrate: 24e7, level: 51 },
  // Level 5.1
  { maxMacroblocks: 36864, maxBitrate: 24e7, level: 52 },
  // Level 5.2
  { maxMacroblocks: 139264, maxBitrate: 24e7, level: 60 },
  // Level 6
  { maxMacroblocks: 139264, maxBitrate: 48e7, level: 61 },
  // Level 6.1
  { maxMacroblocks: 139264, maxBitrate: 8e8, level: 62 }
  // Level 6.2
], ss = [
  { maxPictureSize: 36864, maxBitrate: 128e3, tier: "L", level: 30 },
  // Level 1 (Low Tier)
  { maxPictureSize: 122880, maxBitrate: 15e5, tier: "L", level: 60 },
  // Level 2 (Low Tier)
  { maxPictureSize: 245760, maxBitrate: 3e6, tier: "L", level: 63 },
  // Level 2.1 (Low Tier)
  { maxPictureSize: 552960, maxBitrate: 6e6, tier: "L", level: 90 },
  // Level 3 (Low Tier)
  { maxPictureSize: 983040, maxBitrate: 1e7, tier: "L", level: 93 },
  // Level 3.1 (Low Tier)
  { maxPictureSize: 2228224, maxBitrate: 12e6, tier: "L", level: 120 },
  // Level 4 (Low Tier)
  { maxPictureSize: 2228224, maxBitrate: 3e7, tier: "H", level: 120 },
  // Level 4 (High Tier)
  { maxPictureSize: 2228224, maxBitrate: 2e7, tier: "L", level: 123 },
  // Level 4.1 (Low Tier)
  { maxPictureSize: 2228224, maxBitrate: 5e7, tier: "H", level: 123 },
  // Level 4.1 (High Tier)
  { maxPictureSize: 8912896, maxBitrate: 25e6, tier: "L", level: 150 },
  // Level 5 (Low Tier)
  { maxPictureSize: 8912896, maxBitrate: 1e8, tier: "H", level: 150 },
  // Level 5 (High Tier)
  { maxPictureSize: 8912896, maxBitrate: 4e7, tier: "L", level: 153 },
  // Level 5.1 (Low Tier)
  { maxPictureSize: 8912896, maxBitrate: 16e7, tier: "H", level: 153 },
  // Level 5.1 (High Tier)
  { maxPictureSize: 8912896, maxBitrate: 6e7, tier: "L", level: 156 },
  // Level 5.2 (Low Tier)
  { maxPictureSize: 8912896, maxBitrate: 24e7, tier: "H", level: 156 },
  // Level 5.2 (High Tier)
  { maxPictureSize: 35651584, maxBitrate: 6e7, tier: "L", level: 180 },
  // Level 6 (Low Tier)
  { maxPictureSize: 35651584, maxBitrate: 24e7, tier: "H", level: 180 },
  // Level 6 (High Tier)
  { maxPictureSize: 35651584, maxBitrate: 12e7, tier: "L", level: 183 },
  // Level 6.1 (Low Tier)
  { maxPictureSize: 35651584, maxBitrate: 48e7, tier: "H", level: 183 },
  // Level 6.1 (High Tier)
  { maxPictureSize: 35651584, maxBitrate: 24e7, tier: "L", level: 186 },
  // Level 6.2 (Low Tier)
  { maxPictureSize: 35651584, maxBitrate: 8e8, tier: "H", level: 186 }
  // Level 6.2 (High Tier)
], ke = [
  { maxPictureSize: 36864, maxBitrate: 2e5, level: 10 },
  // Level 1
  { maxPictureSize: 73728, maxBitrate: 8e5, level: 11 },
  // Level 1.1
  { maxPictureSize: 122880, maxBitrate: 18e5, level: 20 },
  // Level 2
  { maxPictureSize: 245760, maxBitrate: 36e5, level: 21 },
  // Level 2.1
  { maxPictureSize: 552960, maxBitrate: 72e5, level: 30 },
  // Level 3
  { maxPictureSize: 983040, maxBitrate: 12e6, level: 31 },
  // Level 3.1
  { maxPictureSize: 2228224, maxBitrate: 18e6, level: 40 },
  // Level 4
  { maxPictureSize: 2228224, maxBitrate: 3e7, level: 41 },
  // Level 4.1
  { maxPictureSize: 8912896, maxBitrate: 6e7, level: 50 },
  // Level 5
  { maxPictureSize: 8912896, maxBitrate: 12e7, level: 51 },
  // Level 5.1
  { maxPictureSize: 8912896, maxBitrate: 18e7, level: 52 },
  // Level 5.2
  { maxPictureSize: 35651584, maxBitrate: 18e7, level: 60 },
  // Level 6
  { maxPictureSize: 35651584, maxBitrate: 24e7, level: 61 },
  // Level 6.1
  { maxPictureSize: 35651584, maxBitrate: 48e7, level: 62 }
  // Level 6.2
], rs = [
  { maxPictureSize: 147456, maxBitrate: 15e5, tier: "M", level: 0 },
  // Level 2.0 (Main Tier)
  { maxPictureSize: 278784, maxBitrate: 3e6, tier: "M", level: 1 },
  // Level 2.1 (Main Tier)
  { maxPictureSize: 665856, maxBitrate: 6e6, tier: "M", level: 4 },
  // Level 3.0 (Main Tier)
  { maxPictureSize: 1065024, maxBitrate: 1e7, tier: "M", level: 5 },
  // Level 3.1 (Main Tier)
  { maxPictureSize: 2359296, maxBitrate: 12e6, tier: "M", level: 8 },
  // Level 4.0 (Main Tier)
  { maxPictureSize: 2359296, maxBitrate: 3e7, tier: "H", level: 8 },
  // Level 4.0 (High Tier)
  { maxPictureSize: 2359296, maxBitrate: 2e7, tier: "M", level: 9 },
  // Level 4.1 (Main Tier)
  { maxPictureSize: 2359296, maxBitrate: 5e7, tier: "H", level: 9 },
  // Level 4.1 (High Tier)
  { maxPictureSize: 8912896, maxBitrate: 3e7, tier: "M", level: 12 },
  // Level 5.0 (Main Tier)
  { maxPictureSize: 8912896, maxBitrate: 1e8, tier: "H", level: 12 },
  // Level 5.0 (High Tier)
  { maxPictureSize: 8912896, maxBitrate: 4e7, tier: "M", level: 13 },
  // Level 5.1 (Main Tier)
  { maxPictureSize: 8912896, maxBitrate: 16e7, tier: "H", level: 13 },
  // Level 5.1 (High Tier)
  { maxPictureSize: 8912896, maxBitrate: 6e7, tier: "M", level: 14 },
  // Level 5.2 (Main Tier)
  { maxPictureSize: 8912896, maxBitrate: 24e7, tier: "H", level: 14 },
  // Level 5.2 (High Tier)
  { maxPictureSize: 35651584, maxBitrate: 6e7, tier: "M", level: 15 },
  // Level 5.3 (Main Tier)
  { maxPictureSize: 35651584, maxBitrate: 24e7, tier: "H", level: 15 },
  // Level 5.3 (High Tier)
  { maxPictureSize: 35651584, maxBitrate: 6e7, tier: "M", level: 16 },
  // Level 6.0 (Main Tier)
  { maxPictureSize: 35651584, maxBitrate: 24e7, tier: "H", level: 16 },
  // Level 6.0 (High Tier)
  { maxPictureSize: 35651584, maxBitrate: 1e8, tier: "M", level: 17 },
  // Level 6.1 (Main Tier)
  { maxPictureSize: 35651584, maxBitrate: 48e7, tier: "H", level: 17 },
  // Level 6.1 (High Tier)
  { maxPictureSize: 35651584, maxBitrate: 16e7, tier: "M", level: 18 },
  // Level 6.2 (Main Tier)
  { maxPictureSize: 35651584, maxBitrate: 8e8, tier: "H", level: 18 },
  // Level 6.2 (High Tier)
  { maxPictureSize: 35651584, maxBitrate: 16e7, tier: "M", level: 19 },
  // Level 6.3 (Main Tier)
  { maxPictureSize: 35651584, maxBitrate: 8e8, tier: "H", level: 19 }
  // Level 6.3 (High Tier)
], Ha = ".01.01.01.01.00", $a = ".0.110.01.01.01.0", ci = (t, e, i, s) => {
  if (t === "avc") {
    const a = Math.ceil(e / 16) * Math.ceil(i / 16), n = is.find(
      (h) => a <= h.maxMacroblocks && s <= h.maxBitrate
    ) ?? H(is), o = n ? n.level : 0, c = "64".padStart(2, "0"), l = "00", d = o.toString(16).padStart(2, "0");
    return `avc1.${c}${l}${d}`;
  } else if (t === "hevc") {
    const r = "", n = "6", o = e * i, c = ss.find(
      (d) => o <= d.maxPictureSize && s <= d.maxBitrate
    ) ?? H(ss);
    return `hev1.${r}1.${n}.${c.tier}${c.level}.B0`;
  } else {
    if (t === "vp8")
      return "vp8";
    if (t === "vp9") {
      const r = "00", a = e * i, n = ke.find(
        (c) => a <= c.maxPictureSize && s <= c.maxBitrate
      ) ?? H(ke);
      return `vp09.${r}.${n.level.toString().padStart(2, "0")}.08${Ha}`;
    } else if (t === "av1") {
      const a = e * i, n = rs.find(
        (l) => a <= l.maxPictureSize && s <= l.maxBitrate
      ) ?? H(rs);
      return `av01.0.${n.level.toString().padStart(2, "0")}${n.tier}.08${$a}`;
    }
  }
  throw new TypeError(`Unhandled codec '${t}'.`);
}, ja = (t) => {
  const e = t.split("."), i = Number(e[1]), s = Number(e[2]), r = Number(e[3]), a = e[4] ? Number(e[4]) : 1;
  return [
    1,
    1,
    i,
    2,
    1,
    s,
    3,
    1,
    r,
    4,
    1,
    a
  ];
}, nr = (t) => {
  const e = t.split("."), r = (1 << 7) + 1, a = Number(e[1]), n = e[2], o = Number(n.slice(0, -1)), c = (a << 5) + o, l = n.slice(-1) === "H" ? 1 : 0, h = Number(e[3]) === 8 ? 0 : 1, u = 0, m = e[4] ? Number(e[4]) : 0, p = e[5] ? Number(e[5][0]) : 1, g = e[5] ? Number(e[5][1]) : 1, w = e[5] ? Number(e[5][2]) : 0, b = (l << 7) + (h << 6) + (u << 5) + (m << 4) + (p << 3) + (g << 2) + w;
  return [r, c, b, 0];
}, or = (t) => {
  const { codec: e, codecDescription: i, colorSpace: s, avcCodecInfo: r, hevcCodecInfo: a, vp9CodecInfo: n, av1CodecInfo: o } = t;
  if (e === "avc") {
    if (r) {
      const c = new Uint8Array([
        r.avcProfileIndication,
        r.profileCompatibility,
        r.avcLevelIndication
      ]);
      return `avc1.${Zi(c)}`;
    }
    if (!i || i.byteLength < 4)
      throw new TypeError("AVC decoder description is not provided or is not at least 4 bytes long.");
    return `avc1.${Zi(i.subarray(1, 4))}`;
  } else if (e === "hevc") {
    let c, l, d, h, u, m;
    if (a)
      c = a.generalProfileSpace, l = a.generalProfileIdc, d = es(a.generalProfileCompatibilityFlags), h = a.generalTierFlag, u = a.generalLevelIdc, m = [...a.generalConstraintIndicatorFlags];
    else {
      if (!i || i.byteLength < 23)
        throw new TypeError("HEVC decoder description is not provided or is not at least 23 bytes long.");
      const g = Re(i), w = g.getUint8(1);
      c = w >> 6 & 3, l = w & 31, d = es(g.getUint32(2)), h = w >> 5 & 1, u = g.getUint8(12), m = [];
      for (let b = 0; b < 6; b++)
        m.push(g.getUint8(6 + b));
    }
    let p = "hev1.";
    for (p += ["", "A", "B", "C"][c] + l, p += ".", p += d.toString(16).toUpperCase(), p += ".", p += h === 0 ? "L" : "H", p += u; m.length > 0 && m[m.length - 1] === 0; )
      m.pop();
    return m.length > 0 && (p += ".", p += m.map((g) => g.toString(16).toUpperCase()).join(".")), p;
  } else {
    if (e === "vp8")
      return "vp8";
    if (e === "vp9") {
      if (!n) {
        const x = t.width * t.height;
        let S = H(ke).level;
        for (const k of ke)
          if (x <= k.maxPictureSize) {
            S = k.level;
            break;
          }
        return `vp09.00.${S.toString().padStart(2, "0")}.08`;
      }
      const c = n.profile.toString().padStart(2, "0"), l = n.level.toString().padStart(2, "0"), d = n.bitDepth.toString().padStart(2, "0"), h = n.chromaSubsampling.toString().padStart(2, "0"), u = n.colourPrimaries.toString().padStart(2, "0"), m = n.transferCharacteristics.toString().padStart(2, "0"), p = n.matrixCoefficients.toString().padStart(2, "0"), g = n.videoFullRangeFlag.toString().padStart(2, "0");
      let w = `vp09.${c}.${l}.${d}.${h}`;
      w += `.${u}.${m}.${p}.${g}`;
      const b = ".01.01.01.01.00";
      return w.endsWith(b) && (w = w.slice(0, -b.length)), w;
    } else if (e === "av1") {
      if (!o) {
        const k = t.width * t.height;
        let y = H(ke).level;
        for (const T of ke)
          if (k <= T.maxPictureSize) {
            y = T.level;
            break;
          }
        return `av01.0.${y.toString().padStart(2, "0")}M.08`;
      }
      const c = o.profile, l = o.level.toString().padStart(2, "0"), d = o.tier ? "H" : "M", h = o.bitDepth.toString().padStart(2, "0"), u = o.monochrome ? "1" : "0", m = 100 * o.chromaSubsamplingX + 10 * o.chromaSubsamplingY + 1 * (o.chromaSubsamplingX && o.chromaSubsamplingY ? o.chromaSamplePosition : 0), p = s?.primaries ? Ke[s.primaries] : 1, g = s?.transfer ? Ge[s.transfer] : 1, w = s?.matrix ? Ye[s.matrix] : 1, b = s?.fullRange ? 1 : 0;
      let x = `av01.${c}.${l}${d}.${h}`;
      x += `.${u}.${m.toString().padStart(3, "0")}`, x += `.${p.toString().padStart(2, "0")}`, x += `.${g.toString().padStart(2, "0")}`, x += `.${w.toString().padStart(2, "0")}`, x += `.${b}`;
      const S = ".0.110.01.01.01.0";
      return x.endsWith(S) && (x = x.slice(0, -S.length)), x;
    }
  }
  throw new TypeError(`Unhandled codec '${e}'.`);
}, li = (t, e, i) => {
  if (t === "aac")
    return e >= 2 && i <= 24e3 ? "mp4a.40.29" : i <= 24e3 ? "mp4a.40.5" : "mp4a.40.2";
  if (t === "mp3")
    return "mp3";
  if (t === "opus")
    return "opus";
  if (t === "vorbis")
    return "vorbis";
  if (t === "flac")
    return "flac";
  if (K.includes(t))
    return t;
  throw new TypeError(`Unhandled codec '${t}'.`);
}, cr = (t) => {
  const { codec: e, codecDescription: i, aacCodecInfo: s } = t;
  if (e === "aac") {
    if (!s)
      throw new TypeError("AAC codec info must be provided.");
    return s.isMpeg2 ? "mp4a.67" : `mp4a.40.${lr(i).objectType}`;
  } else {
    if (e === "mp3")
      return "mp3";
    if (e === "opus")
      return "opus";
    if (e === "vorbis")
      return "vorbis";
    if (e === "flac")
      return "flac";
    if (e && K.includes(e))
      return e;
  }
  throw new TypeError(`Unhandled codec '${e}'.`);
}, lr = (t) => {
  if (!t || t.byteLength < 2)
    throw new TypeError("AAC description must be at least 2 bytes long.");
  const e = new Se(t);
  let i = e.readBits(5);
  i === 31 && (i = 32 + e.readBits(6));
  const s = e.readBits(4);
  let r = null;
  if (s === 15)
    r = e.readBits(24);
  else {
    const o = [
      96e3,
      88200,
      64e3,
      48e3,
      44100,
      32e3,
      24e3,
      22050,
      16e3,
      12e3,
      11025,
      8e3,
      7350
    ];
    s < o.length && (r = o[s]);
  }
  const a = e.readBits(4);
  let n = null;
  return a >= 1 && a <= 7 && (n = {
    1: 1,
    2: 2,
    3: 3,
    4: 4,
    5: 5,
    6: 6,
    7: 8
  }[a]), {
    objectType: i,
    frequencyIndex: s,
    sampleRate: r,
    channelConfiguration: a,
    numberOfChannels: n
  };
}, Ai = 48e3, dr = /^pcm-([usf])(\d+)+(be)?$/, Be = (t) => {
  if (f(K.includes(t)), t === "ulaw")
    return { dataType: "ulaw", sampleSize: 1, littleEndian: !0, silentValue: 255 };
  if (t === "alaw")
    return { dataType: "alaw", sampleSize: 1, littleEndian: !0, silentValue: 213 };
  const e = dr.exec(t);
  f(e);
  let i;
  e[1] === "u" ? i = "unsigned" : e[1] === "s" ? i = "signed" : i = "float";
  const s = Number(e[2]) / 8, r = e[3] !== "be", a = t === "pcm-u8" ? 2 ** 7 : 0;
  return { dataType: i, sampleSize: s, littleEndian: r, silentValue: a };
}, hr = (t) => t.startsWith("avc1") || t.startsWith("avc3") ? "avc" : t.startsWith("hev1") || t.startsWith("hvc1") ? "hevc" : t === "vp8" ? "vp8" : t.startsWith("vp09") ? "vp9" : t.startsWith("av01") ? "av1" : t.startsWith("mp4a.40") || t === "mp4a.67" ? "aac" : t === "mp3" || t === "mp4a.69" || t === "mp4a.6B" || t === "mp4a.6b" ? "mp3" : t === "opus" ? "opus" : t === "vorbis" ? "vorbis" : t === "flac" ? "flac" : t === "ulaw" ? "ulaw" : t === "alaw" ? "alaw" : dr.test(t) ? t : t === "webvtt" ? "webvtt" : null, di = (t) => t === "avc" ? {
  avc: {
    format: "avc"
    // Ensure the format is not Annex B
  }
} : t === "hevc" ? {
  hevc: {
    format: "hevc"
    // Ensure the format is not Annex B
  }
} : {}, hi = (t) => t === "aac" ? {
  aac: {
    format: "aac"
    // Ensure the format is not ADTS
  }
} : t === "opus" ? {
  opus: {
    format: "opus"
  }
} : {}, Nt = class {
  /** @internal */
  constructor(t) {
    this._factor = t;
  }
  /** @internal */
  _toVideoBitrate(t, e, i) {
    const s = e * i, r = {
      avc: 1,
      // H.264/AVC (baseline)
      hevc: 0.6,
      // H.265/HEVC (~40% more efficient than AVC)
      vp9: 0.6,
      // Similar to HEVC
      av1: 0.4,
      // ~60% more efficient than AVC
      vp8: 1.2
      // Slightly less efficient than AVC
    }, c = ((l) => 2e6 * Math.pow(l / 2073600, 0.75))(s) * r[t] * this._factor;
    return Math.ceil(c / 1e3) * 1e3;
  }
  /** @internal */
  _toAudioBitrate(t) {
    if (K.includes(t) || t === "flac")
      return -1;
    const i = {
      aac: 128e3,
      // 128kbps base for AAC
      opus: 64e3,
      // 64kbps base for Opus
      mp3: 16e4,
      // 160kbps base for MP3
      vorbis: 64e3
      // 64kbps base for Vorbis
    }[t];
    if (!i)
      throw new Error(`Unhandled codec: ${t}`);
    let s = i * this._factor;
    return t === "aac" ? s = [96e3, 128e3, 16e4, 192e3].reduce(
      (a, n) => Math.abs(n - s) < Math.abs(a - s) ? n : a
    ) : t === "opus" || t === "vorbis" ? s = Math.max(6e3, s) : t === "mp3" && (s = [
      8e3,
      16e3,
      24e3,
      32e3,
      4e4,
      48e3,
      64e3,
      8e4,
      96e3,
      112e3,
      128e3,
      16e4,
      192e3,
      224e3,
      256e3,
      32e4
    ].reduce(
      (a, n) => Math.abs(n - s) < Math.abs(a - s) ? n : a
    )), Math.round(s / 1e3) * 1e3;
  }
}, qa = ["avc1", "avc3", "hev1", "hvc1", "vp8", "vp09", "av01"], Qa = /^(avc1|avc3)\.[0-9a-fA-F]{6}$/, Xa = /^(hev1|hvc1)\.(?:[ABC]?\d+)\.[0-9a-fA-F]{1,8}\.[LH]\d+(?:\.[0-9a-fA-F]{1,2}){0,6}$/, Ka = /^vp09(?:\.\d{2}){3}(?:(?:\.\d{2}){5})?$/, Ga = /^av01\.\d\.\d{2}[MH]\.\d{2}(?:\.\d\.\d{3}\.\d{2}\.\d{2}\.\d{2}\.\d)?$/, ur = (t) => {
  if (!t)
    throw new TypeError("Video chunk metadata must be provided.");
  if (typeof t != "object")
    throw new TypeError("Video chunk metadata must be an object.");
  if (!t.decoderConfig)
    throw new TypeError("Video chunk metadata must include a decoder configuration.");
  if (typeof t.decoderConfig != "object")
    throw new TypeError("Video chunk metadata decoder configuration must be an object.");
  if (typeof t.decoderConfig.codec != "string")
    throw new TypeError("Video chunk metadata decoder configuration must specify a codec string.");
  if (!qa.some((e) => t.decoderConfig.codec.startsWith(e)))
    throw new TypeError(
      "Video chunk metadata decoder configuration codec string must be a valid video codec string as specified in the WebCodecs Codec Registry."
    );
  if (!Number.isInteger(t.decoderConfig.codedWidth) || t.decoderConfig.codedWidth <= 0)
    throw new TypeError(
      "Video chunk metadata decoder configuration must specify a valid codedWidth (positive integer)."
    );
  if (!Number.isInteger(t.decoderConfig.codedHeight) || t.decoderConfig.codedHeight <= 0)
    throw new TypeError(
      "Video chunk metadata decoder configuration must specify a valid codedHeight (positive integer)."
    );
  if (t.decoderConfig.description !== void 0 && !Ut(t.decoderConfig.description))
    throw new TypeError(
      "Video chunk metadata decoder configuration description, when defined, must be an ArrayBuffer or an ArrayBuffer view."
    );
  if (t.decoderConfig.colorSpace !== void 0) {
    const { colorSpace: e } = t.decoderConfig;
    if (typeof e != "object")
      throw new TypeError(
        "Video chunk metadata decoder configuration colorSpace, when provided, must be an object."
      );
    const i = Object.keys(Ke);
    if (e.primaries != null && !i.includes(e.primaries))
      throw new TypeError(
        `Video chunk metadata decoder configuration colorSpace primaries, when defined, must be one of ${i.join(", ")}.`
      );
    const s = Object.keys(Ge);
    if (e.transfer != null && !s.includes(e.transfer))
      throw new TypeError(
        `Video chunk metadata decoder configuration colorSpace transfer, when defined, must be one of ${s.join(", ")}.`
      );
    const r = Object.keys(Ye);
    if (e.matrix != null && !r.includes(e.matrix))
      throw new TypeError(
        `Video chunk metadata decoder configuration colorSpace matrix, when defined, must be one of ${r.join(", ")}.`
      );
    if (e.fullRange != null && typeof e.fullRange != "boolean")
      throw new TypeError(
        "Video chunk metadata decoder configuration colorSpace fullRange, when defined, must be a boolean."
      );
  }
  if (t.decoderConfig.codec.startsWith("avc1") || t.decoderConfig.codec.startsWith("avc3")) {
    if (!Qa.test(t.decoderConfig.codec))
      throw new TypeError(
        "Video chunk metadata decoder configuration codec string for AVC must be a valid AVC codec string as specified in Section 3.4 of RFC 6381."
      );
  } else if (t.decoderConfig.codec.startsWith("hev1") || t.decoderConfig.codec.startsWith("hvc1")) {
    if (!Xa.test(t.decoderConfig.codec))
      throw new TypeError(
        "Video chunk metadata decoder configuration codec string for HEVC must be a valid HEVC codec string as specified in Section E.3 of ISO 14496-15."
      );
  } else if (t.decoderConfig.codec.startsWith("vp8")) {
    if (t.decoderConfig.codec !== "vp8")
      throw new TypeError('Video chunk metadata decoder configuration codec string for VP8 must be "vp8".');
  } else if (t.decoderConfig.codec.startsWith("vp09")) {
    if (!Ka.test(t.decoderConfig.codec))
      throw new TypeError(
        'Video chunk metadata decoder configuration codec string for VP9 must be a valid VP9 codec string as specified in Section "Codecs Parameter String" of https://www.webmproject.org/vp9/mp4/.'
      );
  } else if (t.decoderConfig.codec.startsWith("av01") && !Ga.test(t.decoderConfig.codec))
    throw new TypeError(
      'Video chunk metadata decoder configuration codec string for AV1 must be a valid AV1 codec string as specified in Section "Codecs Parameter String" of https://aomediacodec.github.io/av1-isobmff/.'
    );
}, Ya = ["mp4a", "mp3", "opus", "vorbis", "flac", "ulaw", "alaw", "pcm"], zi = (t) => {
  if (!t)
    throw new TypeError("Audio chunk metadata must be provided.");
  if (typeof t != "object")
    throw new TypeError("Audio chunk metadata must be an object.");
  if (!t.decoderConfig)
    throw new TypeError("Audio chunk metadata must include a decoder configuration.");
  if (typeof t.decoderConfig != "object")
    throw new TypeError("Audio chunk metadata decoder configuration must be an object.");
  if (typeof t.decoderConfig.codec != "string")
    throw new TypeError("Audio chunk metadata decoder configuration must specify a codec string.");
  if (!Ya.some((e) => t.decoderConfig.codec.startsWith(e)))
    throw new TypeError(
      "Audio chunk metadata decoder configuration codec string must be a valid audio codec string as specified in the WebCodecs Codec Registry."
    );
  if (!Number.isInteger(t.decoderConfig.sampleRate) || t.decoderConfig.sampleRate <= 0)
    throw new TypeError(
      "Audio chunk metadata decoder configuration must specify a valid sampleRate (positive integer)."
    );
  if (!Number.isInteger(t.decoderConfig.numberOfChannels) || t.decoderConfig.numberOfChannels <= 0)
    throw new TypeError(
      "Audio chunk metadata decoder configuration must specify a valid numberOfChannels (positive integer)."
    );
  if (t.decoderConfig.description !== void 0 && !Ut(t.decoderConfig.description))
    throw new TypeError(
      "Audio chunk metadata decoder configuration description, when defined, must be an ArrayBuffer or an ArrayBuffer view."
    );
  if (t.decoderConfig.codec.startsWith("mp4a") && t.decoderConfig.codec !== "mp4a.69" && t.decoderConfig.codec !== "mp4a.6B" && t.decoderConfig.codec !== "mp4a.6b") {
    if (!["mp4a.40.2", "mp4a.40.02", "mp4a.40.5", "mp4a.40.05", "mp4a.40.29", "mp4a.67"].includes(t.decoderConfig.codec))
      throw new TypeError(
        "Audio chunk metadata decoder configuration codec string for AAC must be a valid AAC codec string as specified in https://www.w3.org/TR/webcodecs-aac-codec-registration/."
      );
    if (!t.decoderConfig.description)
      throw new TypeError(
        "Audio chunk metadata decoder configuration for AAC must include a description, which is expected to be an AudioSpecificConfig as specified in ISO 14496-3."
      );
  } else if (t.decoderConfig.codec.startsWith("mp3") || t.decoderConfig.codec.startsWith("mp4a")) {
    if (t.decoderConfig.codec !== "mp3" && t.decoderConfig.codec !== "mp4a.69" && t.decoderConfig.codec !== "mp4a.6B" && t.decoderConfig.codec !== "mp4a.6b")
      throw new TypeError(
        'Audio chunk metadata decoder configuration codec string for MP3 must be "mp3", "mp4a.69" or "mp4a.6B".'
      );
  } else if (t.decoderConfig.codec.startsWith("opus")) {
    if (t.decoderConfig.codec !== "opus")
      throw new TypeError('Audio chunk metadata decoder configuration codec string for Opus must be "opus".');
    if (t.decoderConfig.description && t.decoderConfig.description.byteLength < 18)
      throw new TypeError(
        "Audio chunk metadata decoder configuration description, when specified, is expected to be an Identification Header as specified in Section 5.1 of RFC 7845."
      );
  } else if (t.decoderConfig.codec.startsWith("vorbis")) {
    if (t.decoderConfig.codec !== "vorbis")
      throw new TypeError('Audio chunk metadata decoder configuration codec string for Vorbis must be "vorbis".');
    if (!t.decoderConfig.description)
      throw new TypeError(
        "Audio chunk metadata decoder configuration for Vorbis must include a description, which is expected to adhere to the format described in https://www.w3.org/TR/webcodecs-vorbis-codec-registration/."
      );
  } else if (t.decoderConfig.codec.startsWith("flac")) {
    if (t.decoderConfig.codec !== "flac")
      throw new TypeError('Audio chunk metadata decoder configuration codec string for FLAC must be "flac".');
    if (!t.decoderConfig.description || t.decoderConfig.description.byteLength < 42)
      throw new TypeError(
        "Audio chunk metadata decoder configuration for FLAC must include a description, which is expected to adhere to the format described in https://www.w3.org/TR/webcodecs-flac-codec-registration/."
      );
  } else if ((t.decoderConfig.codec.startsWith("pcm") || t.decoderConfig.codec.startsWith("ulaw") || t.decoderConfig.codec.startsWith("alaw")) && !K.includes(t.decoderConfig.codec))
    throw new TypeError(
      `Audio chunk metadata decoder configuration codec string for PCM must be one of the supported PCM codecs (${K.join(", ")}).`
    );
}, fr = (t) => {
  if (!t)
    throw new TypeError("Subtitle metadata must be provided.");
  if (typeof t != "object")
    throw new TypeError("Subtitle metadata must be an object.");
  if (!t.config)
    throw new TypeError("Subtitle metadata must include a config object.");
  if (typeof t.config != "object")
    throw new TypeError("Subtitle metadata config must be an object.");
  if (typeof t.config.description != "string")
    throw new TypeError("Subtitle metadata config description must be a string.");
}, as = (t) => {
  if (re.includes(t))
    return mr(t);
  if (ne.includes(t))
    return pr(t);
  if (ve.includes(t))
    return Ja(t);
  throw new TypeError(`Unknown codec '${t}'.`);
}, mr = async (t, { width: e = 1280, height: i = 720, bitrate: s = 1e6 } = {}) => {
  if (!re.includes(t))
    return !1;
  if (!Number.isInteger(e) || e <= 0)
    throw new TypeError("width must be a positive integer.");
  if (!Number.isInteger(i) || i <= 0)
    throw new TypeError("height must be a positive integer.");
  if (!Number.isInteger(s) || s <= 0)
    throw new TypeError("bitrate must be a positive integer.");
  if (It.length > 0) {
    const a = {
      codec: ci(
        t,
        e,
        i,
        s
      ),
      width: e,
      height: i,
      bitrate: s,
      ...di(t)
    };
    if (It.some((n) => n.supports(t, a)))
      return !0;
  }
  return typeof VideoEncoder > "u" ? !1 : (await VideoEncoder.isConfigSupported({
    codec: ci(t, e, i, s),
    width: e,
    height: i,
    bitrate: s,
    ...di(t)
  })).supported === !0;
}, pr = async (t, { numberOfChannels: e = 2, sampleRate: i = 48e3, bitrate: s = 128e3 } = {}) => {
  if (!ne.includes(t))
    return !1;
  if (!Number.isInteger(e) || e <= 0)
    throw new TypeError("numberOfChannels must be a positive integer.");
  if (!Number.isInteger(i) || i <= 0)
    throw new TypeError("sampleRate must be a positive integer.");
  if (!Number.isInteger(s) || s <= 0)
    throw new TypeError("bitrate must be a positive integer.");
  if (Et.length > 0) {
    const a = {
      codec: li(
        t,
        e,
        i
      ),
      numberOfChannels: e,
      sampleRate: i,
      bitrate: s,
      ...hi(t)
    };
    if (Et.some((n) => n.supports(t, a)))
      return !0;
  }
  return K.includes(t) ? !0 : typeof AudioEncoder > "u" ? !1 : (await AudioEncoder.isConfigSupported({
    codec: li(t, e, i),
    numberOfChannels: e,
    sampleRate: i,
    bitrate: s,
    ...hi(t)
  })).supported === !0;
}, Ja = async (t) => !!ve.includes(t), Za = async (t = re, e) => {
  const i = await Promise.all(t.map((s) => mr(s, e)));
  return t.filter((s, r) => i[r]);
}, en = async (t = ne, e) => {
  const i = await Promise.all(t.map((s) => pr(s, e)));
  return t.filter((s, r) => i[r]);
}, Pt = /<(?:(\d{2}):)?(\d{2}):(\d{2}).(\d{3})>/g, tn = /(?:(\d{2}):)?(\d{2}):(\d{2}).(\d{3})/, sn = (t) => {
  const e = tn.exec(t);
  if (!e) throw new Error("Expected match.");
  return 60 * 60 * 1e3 * Number(e[1] || "0") + 60 * 1e3 * Number(e[2]) + 1e3 * Number(e[3]) + Number(e[4]);
}, gr = (t) => {
  const e = Math.floor(t / 36e5), i = Math.floor(t % (60 * 60 * 1e3) / (60 * 1e3)), s = Math.floor(t % (60 * 1e3) / 1e3), r = t % 1e3;
  return e.toString().padStart(2, "0") + ":" + i.toString().padStart(2, "0") + ":" + s.toString().padStart(2, "0") + "." + r.toString().padStart(3, "0");
}, Ui = (t) => {
  const e = [];
  let i = 0;
  for (; i < t.length; ) {
    let s = -1, r = 0;
    for (let a = i; a < t.length - 3; a++) {
      if (t[a] === 0 && t[a + 1] === 0 && t[a + 2] === 1) {
        s = a, r = 3;
        break;
      }
      if (a < t.length - 4 && t[a] === 0 && t[a + 1] === 0 && t[a + 2] === 0 && t[a + 3] === 1) {
        s = a, r = 4;
        break;
      }
    }
    if (s === -1)
      break;
    if (i > 0 && s > i) {
      const a = t.subarray(i, s);
      a.length > 0 && e.push(a);
    }
    i = s + r;
  }
  if (i < t.length) {
    const s = t.subarray(i);
    s.length > 0 && e.push(s);
  }
  return e;
}, ui = (t) => {
  const e = [], i = t.length;
  for (let s = 0; s < i; s++)
    s + 2 < i && t[s] === 0 && t[s + 1] === 0 && t[s + 2] === 3 ? (e.push(0, 0), s += 2) : e.push(t[s]);
  return new Uint8Array(e);
}, rn = (t) => {
  const i = Ui(t);
  if (i.length === 0)
    return null;
  let s = 0;
  for (const o of i)
    s += 4 + o.byteLength;
  const r = new Uint8Array(s), a = new DataView(r.buffer);
  let n = 0;
  for (const o of i) {
    const c = o.byteLength;
    a.setUint32(n, c, !1), n += 4, r.set(o, n), n += o.byteLength;
  }
  return r;
}, Qt = (t) => t[0] & 31, wr = (t) => {
  try {
    const e = Ui(t), i = e.filter((u) => Qt(u) === 7), s = e.filter((u) => Qt(u) === 8), r = e.filter((u) => Qt(u) === 13);
    if (i.length === 0 || s.length === 0)
      return null;
    const a = i[0], n = new Se(ui(a));
    if (n.skipBits(1), n.skipBits(2), n.readBits(5) !== 7)
      return console.error("Invalid SPS NAL unit type"), null;
    const c = n.readAlignedByte(), l = n.readAlignedByte(), d = n.readAlignedByte(), h = {
      configurationVersion: 1,
      avcProfileIndication: c,
      profileCompatibility: l,
      avcLevelIndication: d,
      lengthSizeMinusOne: 3,
      // Typically 4 bytes for length field
      sequenceParameterSets: i,
      pictureParameterSets: s,
      chromaFormat: null,
      bitDepthLumaMinus8: null,
      bitDepthChromaMinus8: null,
      sequenceParameterSetExt: null
    };
    if (c === 100 || c === 110 || c === 122 || c === 144) {
      I(n);
      const u = I(n);
      u === 3 && n.skipBits(1);
      const m = I(n), p = I(n);
      h.chromaFormat = u, h.bitDepthLumaMinus8 = m, h.bitDepthChromaMinus8 = p, h.sequenceParameterSetExt = r;
    }
    return h;
  } catch (e) {
    return console.error("Error building AVC Decoder Configuration Record:", e), null;
  }
}, an = (t) => {
  const e = [];
  e.push(t.configurationVersion), e.push(t.avcProfileIndication), e.push(t.profileCompatibility), e.push(t.avcLevelIndication), e.push(252 | t.lengthSizeMinusOne & 3), e.push(224 | t.sequenceParameterSets.length & 31);
  for (const i of t.sequenceParameterSets) {
    const s = i.byteLength;
    e.push(s >> 8), e.push(s & 255);
    for (let r = 0; r < s; r++)
      e.push(i[r]);
  }
  e.push(t.pictureParameterSets.length);
  for (const i of t.pictureParameterSets) {
    const s = i.byteLength;
    e.push(s >> 8), e.push(s & 255);
    for (let r = 0; r < s; r++)
      e.push(i[r]);
  }
  if (t.avcProfileIndication === 100 || t.avcProfileIndication === 110 || t.avcProfileIndication === 122 || t.avcProfileIndication === 144) {
    f(t.chromaFormat !== null), f(t.bitDepthLumaMinus8 !== null), f(t.bitDepthChromaMinus8 !== null), f(t.sequenceParameterSetExt !== null), e.push(252 | t.chromaFormat & 3), e.push(248 | t.bitDepthLumaMinus8 & 7), e.push(248 | t.bitDepthChromaMinus8 & 7), e.push(t.sequenceParameterSetExt.length);
    for (const i of t.sequenceParameterSetExt) {
      const s = i.byteLength;
      e.push(s >> 8), e.push(s & 255);
      for (let r = 0; r < s; r++)
        e.push(i[r]);
    }
  }
  return new Uint8Array(e);
}, ns = 32, os = 33, cs = 34, nn = 39, on = 40, Ue = (t) => t[0] >> 1 & 63, yr = (t) => {
  try {
    const e = Ui(t), i = e.filter((M) => Ue(M) === ns), s = e.filter((M) => Ue(M) === os), r = e.filter((M) => Ue(M) === cs), a = e.filter(
      (M) => Ue(M) === nn || Ue(M) === on
    );
    if (s.length === 0 || r.length === 0) return null;
    const n = s[0], o = new Se(ui(n));
    o.skipBits(16), o.readBits(4);
    const c = o.readBits(3), l = o.readBits(1), {
      general_profile_space: d,
      general_tier_flag: h,
      general_profile_idc: u,
      general_profile_compatibility_flags: m,
      general_constraint_indicator_flags: p,
      general_level_idc: g
    } = cn(o, c);
    I(o);
    const w = I(o);
    w === 3 && o.skipBits(1), I(o), I(o), o.readBits(1) && (I(o), I(o), I(o), I(o));
    const b = I(o), x = I(o);
    I(o);
    const k = o.readBits(1) ? 0 : c;
    for (let M = k; M <= c; M++)
      I(o), I(o), I(o);
    I(o), I(o), I(o), I(o), I(o), I(o), o.readBits(1) && o.readBits(1) && ln(o), o.skipBits(1), o.skipBits(1), o.readBits(1) && (o.skipBits(4), o.skipBits(4), I(o), I(o), o.skipBits(1));
    const y = I(o);
    if (dn(o, y), o.readBits(1)) {
      const M = I(o);
      for (let A = 0; A < M; A++)
        I(o), o.skipBits(1);
    }
    o.skipBits(1), o.skipBits(1);
    let T = 0;
    o.readBits(1) && (T = un(o, c));
    let P = 0;
    if (r.length > 0) {
      const M = r[0], A = new Se(ui(M));
      A.skipBits(16), I(A), I(A), A.skipBits(1), A.skipBits(1), A.skipBits(3), A.skipBits(1), A.skipBits(1), I(A), I(A), ot(A), A.skipBits(1), A.skipBits(1), A.readBits(1) && I(A), ot(A), ot(A), A.skipBits(1), A.skipBits(1), A.skipBits(1), A.skipBits(1);
      const j = A.readBits(1), Ie = A.readBits(1);
      !j && !Ie ? P = 0 : j && !Ie ? P = 2 : !j && Ie ? P = 3 : P = 0;
    }
    const F = [
      ...i.length ? [
        {
          arrayCompleteness: 1,
          nalUnitType: ns,
          nalUnits: i
        }
      ] : [],
      ...s.length ? [
        {
          arrayCompleteness: 1,
          nalUnitType: os,
          nalUnits: s
        }
      ] : [],
      ...r.length ? [
        {
          arrayCompleteness: 1,
          nalUnitType: cs,
          nalUnits: r
        }
      ] : [],
      ...a.length ? [
        {
          arrayCompleteness: 1,
          nalUnitType: Ue(a[0]),
          nalUnits: a
        }
      ] : []
    ];
    return {
      configurationVersion: 1,
      generalProfileSpace: d,
      generalTierFlag: h,
      generalProfileIdc: u,
      generalProfileCompatibilityFlags: m,
      generalConstraintIndicatorFlags: p,
      generalLevelIdc: g,
      minSpatialSegmentationIdc: T,
      parallelismType: P,
      chromaFormatIdc: w,
      bitDepthLumaMinus8: b,
      bitDepthChromaMinus8: x,
      avgFrameRate: 0,
      constantFrameRate: 0,
      numTemporalLayers: c + 1,
      temporalIdNested: l,
      lengthSizeMinusOne: 3,
      arrays: F
    };
  } catch (e) {
    return console.error("Error building HEVC Decoder Configuration Record:", e), null;
  }
}, cn = (t, e) => {
  const i = t.readBits(2), s = t.readBits(1), r = t.readBits(5);
  let a = 0;
  for (let d = 0; d < 32; d++)
    a = a << 1 | t.readBits(1);
  const n = new Uint8Array(6);
  for (let d = 0; d < 6; d++)
    n[d] = t.readBits(8);
  const o = t.readBits(8), c = [], l = [];
  for (let d = 0; d < e; d++)
    c.push(t.readBits(1)), l.push(t.readBits(1));
  if (e > 0)
    for (let d = e; d < 8; d++)
      t.skipBits(2);
  for (let d = 0; d < e; d++)
    c[d] && t.skipBits(88), l[d] && t.skipBits(8);
  return {
    general_profile_space: i,
    general_tier_flag: s,
    general_profile_idc: r,
    general_profile_compatibility_flags: a,
    general_constraint_indicator_flags: n,
    general_level_idc: o
  };
}, ln = (t) => {
  for (let e = 0; e < 4; e++)
    for (let i = 0; i < (e === 3 ? 2 : 6); i++)
      if (!t.readBits(1))
        I(t);
      else {
        const r = Math.min(64, 1 << 4 + (e << 1));
        e > 1 && ot(t);
        for (let a = 0; a < r; a++)
          ot(t);
      }
}, dn = (t, e) => {
  const i = [];
  for (let s = 0; s < e; s++)
    i[s] = hn(t, s, e, i);
}, hn = (t, e, i, s) => {
  let r = 0, a = 0, n = 0;
  if (e !== 0 && (a = t.readBits(1)), a) {
    if (e === i) {
      const c = I(t);
      n = e - (c + 1);
    } else
      n = e - 1;
    t.readBits(1), I(t);
    const o = s[n] ?? 0;
    for (let c = 0; c <= o; c++)
      t.readBits(1) || t.readBits(1);
    r = s[n];
  } else {
    const o = I(t), c = I(t);
    for (let l = 0; l < o; l++)
      I(t), t.readBits(1);
    for (let l = 0; l < c; l++)
      I(t), t.readBits(1);
    r = o + c;
  }
  return r;
}, un = (t, e) => {
  if (t.readBits(1) && t.readBits(8) === 255 && (t.readBits(16), t.readBits(16)), t.readBits(1) && t.readBits(1), t.readBits(1) && (t.readBits(3), t.readBits(1), t.readBits(1) && (t.readBits(8), t.readBits(8), t.readBits(8))), t.readBits(1) && (I(t), I(t)), t.readBits(1), t.readBits(1), t.readBits(1), t.readBits(1) && (I(t), I(t), I(t), I(t)), t.readBits(1) && (t.readBits(32), t.readBits(32), t.readBits(1) && I(t), t.readBits(1) && fn(t, !0, e)), t.readBits(1)) {
    t.readBits(1), t.readBits(1), t.readBits(1);
    const i = I(t);
    return I(t), I(t), I(t), I(t), i;
  }
  return 0;
}, fn = (t, e, i) => {
  let s = !1, r = !1, a = !1;
  s = t.readBits(1) === 1, r = t.readBits(1) === 1, (s || r) && (a = t.readBits(1) === 1, a && (t.readBits(8), t.readBits(5), t.readBits(1), t.readBits(5)), t.readBits(4), t.readBits(4), a && t.readBits(4), t.readBits(5), t.readBits(5), t.readBits(5));
  for (let n = 0; n <= i; n++) {
    const o = t.readBits(1) === 1;
    let c = !0;
    o || (c = t.readBits(1) === 1);
    let l = !1;
    c ? I(t) : l = t.readBits(1) === 1;
    let d = 1;
    l || (d = I(t) + 1), s && ls(t, d, a), r && ls(t, d, a);
  }
}, ls = (t, e, i) => {
  for (let s = 0; s < e; s++)
    I(t), I(t), i && (I(t), I(t)), t.readBits(1);
}, mn = (t) => {
  const e = [];
  e.push(t.configurationVersion), e.push(
    (t.generalProfileSpace & 3) << 6 | (t.generalTierFlag & 1) << 5 | t.generalProfileIdc & 31
  ), e.push(t.generalProfileCompatibilityFlags >>> 24 & 255), e.push(t.generalProfileCompatibilityFlags >>> 16 & 255), e.push(t.generalProfileCompatibilityFlags >>> 8 & 255), e.push(t.generalProfileCompatibilityFlags & 255), e.push(...t.generalConstraintIndicatorFlags), e.push(t.generalLevelIdc & 255), e.push(240 | t.minSpatialSegmentationIdc >> 8 & 15), e.push(t.minSpatialSegmentationIdc & 255), e.push(252 | t.parallelismType & 3), e.push(252 | t.chromaFormatIdc & 3), e.push(248 | t.bitDepthLumaMinus8 & 7), e.push(248 | t.bitDepthChromaMinus8 & 7), e.push(t.avgFrameRate >> 8 & 255), e.push(t.avgFrameRate & 255), e.push(
    (t.constantFrameRate & 3) << 6 | (t.numTemporalLayers & 7) << 3 | (t.temporalIdNested & 1) << 2 | t.lengthSizeMinusOne & 3
  ), e.push(t.arrays.length & 255);
  for (const i of t.arrays) {
    e.push(
      (i.arrayCompleteness & 1) << 7 | 0 | i.nalUnitType & 63
    ), e.push(i.nalUnits.length >> 8 & 255), e.push(i.nalUnits.length & 255);
    for (const s of i.nalUnits) {
      e.push(s.length >> 8 & 255), e.push(s.length & 255);
      for (let r = 0; r < s.length; r++)
        e.push(s[r]);
    }
  }
  return new Uint8Array(e);
}, br = (t) => {
  const e = t[t.length - 1];
  if (e && (e & 224) === 192) {
    const P = ((e & 24) >> 3) + 1, z = 2 + ((e & 7) + 1) * P;
    if (t[t.length - z] !== e)
      return null;
    let M = 0;
    const A = t.length - z + 1;
    for (let j = 0; j < P; j++) {
      if (!t[A + j]) return null;
      M |= t[A + j] << 8 * j;
    }
    t = t.subarray(0, M);
  }
  const i = new Se(t);
  if (i.readBits(2) !== 2)
    return null;
  const r = i.readBits(1), n = (i.readBits(1) << 1) + r;
  if (n === 3 && i.skipBits(1), i.readBits(1) === 1 || i.readBits(1) !== 0 || (i.skipBits(2), i.readBits(24) !== 4817730))
    return null;
  let d = 8;
  n >= 2 && (d = i.readBits(1) ? 12 : 10);
  const h = i.readBits(3);
  let u = 0, m = 0;
  if (h !== 7)
    if (m = i.readBits(1), n === 1 || n === 3) {
      const F = i.readBits(1), z = i.readBits(1);
      u = !F && !z ? 3 : F && !z ? 2 : 1, i.skipBits(1);
    } else
      u = 1;
  else
    u = 3, m = 1;
  const p = i.readBits(16), g = i.readBits(16), w = p + 1, b = g + 1, x = w * b;
  let S = H(ke).level;
  for (const P of ke)
    if (x <= P.maxPictureSize) {
      S = P.level;
      break;
    }
  return {
    profile: n,
    level: S,
    bitDepth: d,
    chromaSubsampling: u,
    videoFullRangeFlag: m,
    colourPrimaries: h === 2 ? 1 : h === 1 ? 6 : 2,
    transferCharacteristics: h === 2 ? 1 : h === 1 ? 6 : 2,
    matrixCoefficients: h === 7 ? 0 : h === 2 ? 1 : h === 1 ? 6 : 2
  };
}, kr = (t) => {
  const e = new Se(t), i = () => {
    let s = 0;
    for (let r = 0; r < 8; r++) {
      const a = e.readAlignedByte();
      if (a === void 0) return 0;
      if (s |= (a & 127) << r * 7, !(a & 128))
        break;
      if (r === 7 && a & 128)
        return null;
    }
    return s >= 2 ** 32 - 1 ? null : s;
  };
  for (; e.getBitsLeft() >= 8; ) {
    const s = e.readBits(8), r = s >> 3 & 15, a = s >> 2 & 1, n = s >> 1 & 1;
    a && e.skipBits(8);
    let o;
    if (n) {
      const c = i();
      if (c === null) return null;
      o = c;
    } else
      o = Math.floor(e.getBitsLeft() / 8);
    if (r === 1) {
      const c = e.readBits(3);
      e.readBits(1);
      const l = e.readBits(1);
      let d = 0, h = 0, u = 0;
      if (l)
        d = e.readBits(5);
      else {
        if (e.readBits(1) && (e.skipBits(32), e.skipBits(32), e.readBits(1)))
          return null;
        const k = e.readBits(1);
        k && (u = e.readBits(5), e.skipBits(32), e.skipBits(5), e.skipBits(5));
        const y = e.readBits(5);
        for (let T = 0; T <= y; T++) {
          e.skipBits(12);
          const P = e.readBits(5);
          if (T === 0 && (d = P), P > 7) {
            const z = e.readBits(1);
            T === 0 && (h = z);
          }
          if (k && e.readBits(1)) {
            const M = u + 1;
            e.skipBits(M), e.skipBits(M), e.skipBits(1);
          }
          e.readBits(1) && e.skipBits(4);
        }
      }
      const m = e.readBits(1);
      let p = 8;
      c === 2 && m ? p = e.readBits(1) ? 12 : 10 : c <= 2 && (p = m ? 10 : 8);
      let g = 0;
      c !== 1 && (g = e.readBits(1));
      let w = 1, b = 1, x = 0;
      return g || (c === 0 ? (w = 1, b = 1) : c === 1 ? (w = 0, b = 0) : p === 12 && (w = e.readBits(1), w && (b = e.readBits(1))), w && b && (x = e.readBits(2))), {
        profile: c,
        level: d,
        tier: h,
        bitDepth: p,
        monochrome: g,
        chromaSubsamplingX: w,
        chromaSubsamplingY: b,
        chromaSamplePosition: x
      };
    }
    e.skipBits(o * 8);
  }
  return null;
}, Vt = (t) => {
  const e = Re(t), i = e.getUint8(9), s = e.getUint16(10, !0), r = e.getUint32(12, !0), a = e.getInt16(16, !0), n = e.getUint8(18);
  let o = null;
  return n && (o = t.subarray(19, 21 + i)), {
    outputChannelCount: i,
    preSkip: s,
    inputSampleRate: r,
    outputGain: a,
    channelMappingFamily: n,
    channelMappingTable: o
  };
}, pn = [
  480,
  960,
  1920,
  2880,
  480,
  960,
  1920,
  2880,
  480,
  960,
  1920,
  2880,
  480,
  960,
  480,
  960,
  120,
  240,
  480,
  960,
  120,
  240,
  480,
  960,
  120,
  240,
  480,
  960,
  120,
  240,
  480,
  960
], gn = (t) => {
  const e = t[0] >> 3;
  return {
    durationInSamples: pn[e]
  };
}, Sr = (t) => {
  if (t.length < 7)
    throw new Error("Setup header is too short.");
  if (t[0] !== 5)
    throw new Error("Wrong packet type in Setup header.");
  if (String.fromCharCode(...t.slice(1, 7)) !== "vorbis")
    throw new Error("Invalid packet signature in Setup header.");
  const i = t.length, s = new Uint8Array(i);
  for (let h = 0; h < i; h++)
    s[h] = t[i - 1 - h];
  const r = new Se(s);
  let a = 0;
  for (; r.getBitsLeft() > 97; )
    if (r.readBits(1) === 1) {
      a = r.pos;
      break;
    }
  if (a === 0)
    throw new Error("Invalid Setup header: framing bit not found.");
  let n = 0, o = !1, c = 0;
  for (; r.getBitsLeft() >= 97; ) {
    const h = r.pos, u = r.readBits(8), m = r.readBits(16), p = r.readBits(16);
    if (u > 63 || m !== 0 || p !== 0) {
      r.pos = h;
      break;
    }
    if (r.skipBits(1), n++, n > 64)
      break;
    r.clone().readBits(6) + 1 === n && (o = !0, c = n);
  }
  if (!o)
    throw new Error("Invalid Setup header: mode header not found.");
  if (c > 63)
    throw new Error(`Unsupported mode count: ${c}.`);
  const l = c;
  r.pos = 0, r.skipBits(a);
  const d = Array(l).fill(0);
  for (let h = l - 1; h >= 0; h--)
    r.skipBits(40), d[h] = r.readBits(1);
  return { modeBlockflags: d };
}, ds = class {
  constructor(t) {
    this.writer = t, this.helper = new Uint8Array(8), this.helperView = new DataView(this.helper.buffer), this.offsets = /* @__PURE__ */ new WeakMap();
  }
  writeU32(t) {
    this.helperView.setUint32(0, t, !1), this.writer.write(this.helper.subarray(0, 4));
  }
  writeU64(t) {
    this.helperView.setUint32(0, Math.floor(t / 2 ** 32), !1), this.helperView.setUint32(4, t, !1), this.writer.write(this.helper.subarray(0, 8));
  }
  writeAscii(t) {
    for (let e = 0; e < t.length; e++)
      this.helperView.setUint8(e % 8, t.charCodeAt(e)), e % 8 === 7 && this.writer.write(this.helper);
    t.length % 8 !== 0 && this.writer.write(this.helper.subarray(0, t.length % 8));
  }
  writeBox(t) {
    if (this.offsets.set(t, this.writer.getPos()), t.contents && !t.children)
      this.writeBoxHeader(t, t.size ?? t.contents.byteLength + 8), this.writer.write(t.contents);
    else {
      const e = this.writer.getPos();
      if (this.writeBoxHeader(t, 0), t.contents && this.writer.write(t.contents), t.children)
        for (const r of t.children) r && this.writeBox(r);
      const i = this.writer.getPos(), s = t.size ?? i - e;
      this.writer.seek(e), this.writeBoxHeader(t, s), this.writer.seek(i);
    }
  }
  writeBoxHeader(t, e) {
    this.writeU32(t.largeSize ? 1 : e), this.writeAscii(t.type), t.largeSize && this.writeU64(e);
  }
  measureBoxHeader(t) {
    return 8 + (t.largeSize ? 8 : 0);
  }
  patchBox(t) {
    const e = this.offsets.get(t);
    f(e !== void 0);
    const i = this.writer.getPos();
    this.writer.seek(e), this.writeBox(t), this.writer.seek(i);
  }
  measureBox(t) {
    if (t.contents && !t.children)
      return this.measureBoxHeader(t) + t.contents.byteLength;
    {
      let e = this.measureBoxHeader(t);
      if (t.contents && (e += t.contents.byteLength), t.children)
        for (const i of t.children) i && (e += this.measureBox(i));
      return e;
    }
  }
}, D = new Uint8Array(8), ce = new DataView(D.buffer), W = (t) => [(t % 256 + 256) % 256], R = (t) => (ce.setUint16(0, t, !1), [D[0], D[1]]), vr = (t) => (ce.setInt16(0, t, !1), [D[0], D[1]]), Tr = (t) => (ce.setUint32(0, t, !1), [D[1], D[2], D[3]]), C = (t) => (ce.setUint32(0, t, !1), [D[0], D[1], D[2], D[3]]), ye = (t) => (ce.setInt32(0, t, !1), [D[0], D[1], D[2], D[3]]), Pe = (t) => (ce.setUint32(0, Math.floor(t / 2 ** 32), !1), ce.setUint32(4, t, !1), [D[0], D[1], D[2], D[3], D[4], D[5], D[6], D[7]]), xr = (t) => (ce.setInt16(0, 2 ** 8 * t, !1), [D[0], D[1]]), de = (t) => (ce.setInt32(0, 2 ** 16 * t, !1), [D[0], D[1], D[2], D[3]]), Xt = (t) => (ce.setInt32(0, 2 ** 30 * t, !1), [D[0], D[1], D[2], D[3]]), Kt = (t, e) => {
  const i = [];
  let s = t;
  do {
    let r = s & 127;
    s >>= 7, i.length > 0 && (r |= 128), i.push(r);
  } while (s > 0 || e);
  return i.reverse();
}, Y = (t, e = !1) => {
  const i = Array(t.length).fill(null).map((s, r) => t.charCodeAt(r));
  return e && i.push(0), i;
}, Di = (t) => {
  let e = null;
  for (const i of t)
    (!e || i.timestamp > e.timestamp) && (e = i);
  return e;
}, Cr = (t) => {
  const e = t * (Math.PI / 180), i = Math.round(Math.cos(e)), s = Math.round(Math.sin(e));
  return [
    i,
    s,
    0,
    -s,
    i,
    0,
    0,
    0,
    1
  ];
}, _r = Cr(0), Ir = (t) => [
  de(t[0]),
  de(t[1]),
  Xt(t[2]),
  de(t[3]),
  de(t[4]),
  Xt(t[5]),
  de(t[6]),
  de(t[7]),
  Xt(t[8])
], U = (t, e, i) => ({
  type: t,
  contents: e && new Uint8Array(e.flat(10)),
  children: i
}), V = (t, e, i, s, r) => U(
  t,
  [W(e), Tr(i), s ?? []],
  r
), wn = (t) => t.isMov ? U("ftyp", [
  Y("qt  "),
  // Major brand
  C(512),
  // Minor version
  // Compatible brands
  Y("qt  ")
]) : t.fragmented ? U("ftyp", [
  Y("iso5"),
  // Major brand
  C(512),
  // Minor version
  // Compatible brands
  Y("iso5"),
  Y("iso6"),
  Y("mp41")
]) : U("ftyp", [
  Y("isom"),
  // Major brand
  C(512),
  // Minor version
  // Compatible brands
  Y("isom"),
  t.holdsAvc ? Y("avc1") : [],
  Y("mp41")
]), Gt = (t) => ({ type: "mdat", largeSize: t }), yt = (t, e, i = !1) => U("moov", void 0, [
  yn(e, t),
  ...t.map((s) => bn(s, e)),
  i ? to(t) : null
]), yn = (t, e) => {
  const i = Q(Math.max(
    0,
    ...e.filter((n) => n.samples.length > 0).map((n) => {
      const o = Di(n.samples);
      return o.timestamp + o.duration;
    })
  ), mi), s = Math.max(0, ...e.map((n) => n.track.id)) + 1, r = !Qe(t) || !Qe(i), a = r ? Pe : C;
  return V("mvhd", +r, 0, [
    a(t),
    // Creation time
    a(t),
    // Modification time
    C(mi),
    // Timescale
    a(i),
    // Duration
    de(1),
    // Preferred rate
    xr(1),
    // Preferred volume
    Array(10).fill(0),
    // Reserved
    Ir(_r),
    // Matrix
    Array(24).fill(0),
    // Pre-defined
    C(s)
    // Next track ID
  ]);
}, bn = (t, e) => U("trak", void 0, [
  kn(t, e),
  Sn(t, e)
]), kn = (t, e) => {
  const i = Di(t.samples), s = Q(
    i ? i.timestamp + i.duration : 0,
    mi
  ), r = !Qe(e) || !Qe(s), a = r ? Pe : C;
  let n;
  if (t.type === "video") {
    const o = t.track.metadata.rotation;
    n = Cr(o ?? 0);
  } else
    n = _r;
  return V("tkhd", +r, 3, [
    a(e),
    // Creation time
    a(e),
    // Modification time
    C(t.track.id),
    // Track ID
    C(0),
    // Reserved
    a(s),
    // Duration
    Array(8).fill(0),
    // Reserved
    R(0),
    // Layer
    R(t.track.id),
    // Alternate group
    xr(t.type === "audio" ? 1 : 0),
    // Volume
    R(0),
    // Reserved
    Ir(n),
    // Matrix
    de(t.type === "video" ? t.info.width : 0),
    // Track width
    de(t.type === "video" ? t.info.height : 0)
    // Track height
  ]);
}, Sn = (t, e) => U("mdia", void 0, [
  vn(t, e),
  Cn(t),
  _n(t)
]), vn = (t, e) => {
  const i = Di(t.samples), s = Q(
    i ? i.timestamp + i.duration : 0,
    t.timescale
  ), r = !Qe(e) || !Qe(s), a = r ? Pe : C;
  let n = 0;
  for (const o of t.track.metadata.languageCode ?? fe)
    n <<= 5, n += o.charCodeAt(0) - 96;
  return V("mdhd", +r, 0, [
    a(e),
    // Creation time
    a(e),
    // Modification time
    C(t.timescale),
    // Timescale
    a(s),
    // Duration
    R(n),
    // Language
    R(0)
    // Quality
  ]);
}, Tn = {
  video: "vide",
  audio: "soun",
  subtitle: "text"
}, xn = {
  video: "VideoHandler",
  audio: "SoundHandler",
  subtitle: "TextHandler"
}, Cn = (t) => V("hdlr", 0, 0, [
  Y("mhlr"),
  // Component type
  Y(Tn[t.type]),
  // Component subtype
  C(0),
  // Component manufacturer
  C(0),
  // Component flags
  C(0),
  // Component flags mask
  Y(xn[t.type], !0)
  // Component name
]), _n = (t) => U("minf", void 0, [
  Fn[t.type](),
  Rn(),
  Mn(t)
]), In = () => V("vmhd", 0, 1, [
  R(0),
  // Graphics mode
  R(0),
  // Opcolor R
  R(0),
  // Opcolor G
  R(0)
  // Opcolor B
]), En = () => V("smhd", 0, 0, [
  R(0),
  // Balance
  R(0)
  // Reserved
]), Pn = () => V("nmhd", 0, 0), Fn = {
  video: In,
  audio: En,
  subtitle: Pn
}, Rn = () => U("dinf", void 0, [
  Bn()
]), Bn = () => V("dref", 0, 0, [
  C(1)
  // Entry count
], [
  On()
]), On = () => V("url ", 0, 1), Mn = (t) => {
  const e = t.compositionTimeOffsetTable.length > 1 || t.compositionTimeOffsetTable.some((i) => i.sampleCompositionTimeOffset !== 0);
  return U("stbl", void 0, [
    An(t),
    Xn(t),
    e ? Zn(t) : null,
    e ? eo(t) : null,
    Gn(t),
    Yn(t),
    Jn(t),
    Kn(t)
  ]);
}, An = (t) => {
  let e;
  if (t.type === "video")
    e = zn(
      po[t.track.source._codec],
      t
    );
  else if (t.type === "audio") {
    const i = Pr[t.track.source._codec];
    f(i), e = Ln(
      i,
      t
    );
  } else t.type === "subtitle" && (e = qn(
    yo[t.track.source._codec],
    t
  ));
  return f(e), V("stsd", 0, 0, [
    C(1)
    // Entry count
  ], [
    e
  ]);
}, zn = (t, e) => U(t, [
  Array(6).fill(0),
  // Reserved
  R(1),
  // Data reference index
  R(0),
  // Pre-defined
  R(0),
  // Reserved
  Array(12).fill(0),
  // Pre-defined
  R(e.info.width),
  // Width
  R(e.info.height),
  // Height
  C(4718592),
  // Horizontal resolution
  C(4718592),
  // Vertical resolution
  C(0),
  // Reserved
  R(1),
  // Frame count
  Array(32).fill(0),
  // Compressor name
  R(24),
  // Depth
  vr(65535)
  // Pre-defined
], [
  go[e.track.source._codec](e),
  Js(e.info.decoderConfig.colorSpace) ? Un(e) : null
]), Un = (t) => U("colr", [
  Y("nclx"),
  // Colour type
  R(Ke[t.info.decoderConfig.colorSpace.primaries]),
  // Colour primaries
  R(Ge[t.info.decoderConfig.colorSpace.transfer]),
  // Transfer characteristics
  R(Ye[t.info.decoderConfig.colorSpace.matrix]),
  // Matrix coefficients
  W((t.info.decoderConfig.colorSpace.fullRange ? 1 : 0) << 7)
  // Full range flag
]), Dn = (t) => t.info.decoderConfig && U("avcC", [
  // For AVC, description is an AVCDecoderConfigurationRecord, so nothing else to do here
  ...te(t.info.decoderConfig.description)
]), Nn = (t) => t.info.decoderConfig && U("hvcC", [
  // For HEVC, description is an HEVCDecoderConfigurationRecord, so nothing else to do here
  ...te(t.info.decoderConfig.description)
]), hs = (t) => {
  if (!t.info.decoderConfig)
    return null;
  const e = t.info.decoderConfig, i = e.codec.split("."), s = Number(i[1]), r = Number(i[2]), a = Number(i[3]), n = i[4] ? Number(i[4]) : 1, o = i[8] ? Number(i[8]) : Number(e.colorSpace?.fullRange ?? 0), c = (a << 4) + (n << 1) + o, l = i[5] ? Number(i[5]) : e.colorSpace?.primaries ? Ke[e.colorSpace.primaries] : 2, d = i[6] ? Number(i[6]) : e.colorSpace?.transfer ? Ge[e.colorSpace.transfer] : 2, h = i[7] ? Number(i[7]) : e.colorSpace?.matrix ? Ye[e.colorSpace.matrix] : 2;
  return V("vpcC", 1, 0, [
    W(s),
    // Profile
    W(r),
    // Level
    W(c),
    // Bit depth, chroma subsampling, full range
    W(l),
    // Colour primaries
    W(d),
    // Transfer characteristics
    W(h),
    // Matrix coefficients
    R(0)
    // Codec initialization data size
  ]);
}, Vn = (t) => U("av1C", nr(t.info.decoderConfig.codec)), Ln = (t, e) => {
  let i = 0, s, r = 16;
  if (K.includes(e.track.source._codec)) {
    const a = e.track.source._codec, { sampleSize: n } = Be(a);
    r = 8 * n, r > 16 && (i = 1);
  }
  return i === 0 ? s = [
    Array(6).fill(0),
    // Reserved
    R(1),
    // Data reference index
    R(i),
    // Version
    R(0),
    // Revision level
    C(0),
    // Vendor
    R(e.info.numberOfChannels),
    // Number of channels
    R(r),
    // Sample size (bits)
    R(0),
    // Compression ID
    R(0),
    // Packet size
    R(e.info.sampleRate < 2 ** 16 ? e.info.sampleRate : 0),
    // Sample rate (upper)
    R(0)
    // Sample rate (lower)
  ] : s = [
    Array(6).fill(0),
    // Reserved
    R(1),
    // Data reference index
    R(i),
    // Version
    R(0),
    // Revision level
    C(0),
    // Vendor
    R(e.info.numberOfChannels),
    // Number of channels
    R(Math.min(r, 16)),
    // Sample size (bits)
    R(0),
    // Compression ID
    R(0),
    // Packet size
    R(e.info.sampleRate < 2 ** 16 ? e.info.sampleRate : 0),
    // Sample rate (upper)
    R(0),
    // Sample rate (lower)
    C(1),
    // Samples per packet (must be 1 for uncompressed formats)
    C(r / 8),
    // Bytes per packet
    C(e.info.numberOfChannels * r / 8),
    // Bytes per frame
    C(2)
    // Bytes per sample (constant in FFmpeg)
  ], U(t, s, [
    wo[e.track.source._codec]?.(e) ?? null
  ]);
}, Yt = (t) => {
  let e;
  switch (t.track.source._codec) {
    case "aac":
      e = 64;
      break;
    case "mp3":
      e = 107;
      break;
    case "vorbis":
      e = 221;
      break;
    default:
      throw new Error(`Unhandled audio codec: ${t.track.source._codec}`);
  }
  let i = [
    ...W(e),
    // Object type indication
    ...W(21),
    // stream type(6bits)=5 audio, flags(2bits)=1
    ...Tr(0),
    // 24bit buffer size
    ...C(0),
    // max bitrate
    ...C(0)
    // avg bitrate
  ];
  if (t.info.decoderConfig.description) {
    const s = te(t.info.decoderConfig.description);
    i = [
      ...i,
      ...W(5),
      // TAG(5) = DecoderSpecificInfo
      ...Kt(s.byteLength),
      ...s
    ];
  }
  return i = [
    ...R(1),
    // ES_ID = 1
    ...W(0),
    // flags etc = 0
    ...W(4),
    // TAG(4) = ES Descriptor
    ...Kt(i.length),
    ...i,
    ...W(6),
    // TAG(6)
    ...W(1),
    // length
    ...W(2)
    // data
  ], i = [
    ...W(3),
    // TAG(3) = Object Descriptor
    ...Kt(i.length),
    ...i
  ], V("esds", 0, 0, i);
}, De = (t) => U("wave", void 0, [
  Wn(t),
  Hn(t),
  U("\0\0\0\0")
  // NULL tag at the end
]), Wn = (t) => U("frma", [
  Y(Pr[t.track.source._codec])
]), Hn = (t) => {
  const { littleEndian: e } = Be(t.track.source._codec);
  return U("enda", [
    R(+e)
  ]);
}, $n = (t) => {
  let e = t.info.numberOfChannels, i = 3840, s = t.info.sampleRate, r = 0, a = 0, n = new Uint8Array(0);
  const o = t.info.decoderConfig?.description;
  if (o) {
    f(o.byteLength >= 18);
    const c = te(o), l = Vt(c);
    e = l.outputChannelCount, i = l.preSkip, s = l.inputSampleRate, r = l.outputGain, a = l.channelMappingFamily, l.channelMappingTable && (n = l.channelMappingTable);
  }
  return U("dOps", [
    W(0),
    // Version
    W(e),
    // OutputChannelCount
    R(i),
    // PreSkip
    C(s),
    // InputSampleRate
    vr(r),
    // OutputGain
    W(a),
    // ChannelMappingFamily
    ...n
  ]);
}, jn = (t) => {
  const e = t.info.decoderConfig?.description;
  f(e);
  const i = te(e);
  return V("dfLa", 0, 0, [
    ...i.subarray(4)
  ]);
}, qn = (t, e) => U(t, [
  Array(6).fill(0),
  // Reserved
  R(1)
  // Data reference index
], [
  bo[e.track.source._codec](e)
]), Qn = (t) => U("vttC", [
  ...he.encode(t.info.config.description)
]), Xn = (t) => V("stts", 0, 0, [
  C(t.timeToSampleTable.length),
  // Number of entries
  t.timeToSampleTable.map((e) => [
    // Time-to-sample table
    C(e.sampleCount),
    // Sample count
    C(e.sampleDelta)
    // Sample duration
  ])
]), Kn = (t) => {
  if (t.samples.every((i) => i.type === "key")) return null;
  const e = [...t.samples.entries()].filter(([, i]) => i.type === "key");
  return V("stss", 0, 0, [
    C(e.length),
    // Number of entries
    e.map(([i]) => C(i + 1))
    // Sync sample table
  ]);
}, Gn = (t) => V("stsc", 0, 0, [
  C(t.compactlyCodedChunkTable.length),
  // Number of entries
  t.compactlyCodedChunkTable.map((e) => [
    // Sample-to-chunk table
    C(e.firstChunk),
    // First chunk
    C(e.samplesPerChunk),
    // Samples per chunk
    C(1)
    // Sample description index
  ])
]), Yn = (t) => {
  if (t.type === "audio" && t.info.requiresPcmTransformation) {
    const { sampleSize: e } = Be(t.track.source._codec);
    return V("stsz", 0, 0, [
      C(e * t.info.numberOfChannels),
      // Sample size
      C(t.samples.reduce((i, s) => i + Q(s.duration, t.timescale), 0))
    ]);
  }
  return V("stsz", 0, 0, [
    C(0),
    // Sample size (0 means non-constant size)
    C(t.samples.length),
    // Number of entries
    t.samples.map((e) => C(e.size))
    // Sample size table
  ]);
}, Jn = (t) => t.finalizedChunks.length > 0 && H(t.finalizedChunks).offset >= 2 ** 32 ? V("co64", 0, 0, [
  C(t.finalizedChunks.length),
  // Number of entries
  t.finalizedChunks.map((e) => Pe(e.offset))
  // Chunk offset table
]) : V("stco", 0, 0, [
  C(t.finalizedChunks.length),
  // Number of entries
  t.finalizedChunks.map((e) => C(e.offset))
  // Chunk offset table
]), Zn = (t) => V("ctts", 1, 0, [
  C(t.compositionTimeOffsetTable.length),
  // Number of entries
  t.compositionTimeOffsetTable.map((e) => [
    // Time-to-sample table
    C(e.sampleCount),
    // Sample count
    ye(e.sampleCompositionTimeOffset)
    // Sample offset
  ])
]), eo = (t) => {
  let e = 1 / 0, i = -1 / 0, s = 1 / 0, r = -1 / 0;
  f(t.compositionTimeOffsetTable.length > 0), f(t.samples.length > 0);
  for (let n = 0; n < t.compositionTimeOffsetTable.length; n++) {
    const o = t.compositionTimeOffsetTable[n];
    e = Math.min(e, o.sampleCompositionTimeOffset), i = Math.max(i, o.sampleCompositionTimeOffset);
  }
  for (let n = 0; n < t.samples.length; n++) {
    const o = t.samples[n];
    s = Math.min(
      s,
      Q(o.timestamp, t.timescale)
    ), r = Math.max(
      r,
      Q(o.timestamp + o.duration, t.timescale)
    );
  }
  const a = Math.max(-e, 0);
  return r >= 2 ** 31 ? null : V("cslg", 0, 0, [
    ye(a),
    // Composition to DTS shift
    ye(e),
    // Least decode to display delta
    ye(i),
    // Greatest decode to display delta
    ye(s),
    // Composition start time
    ye(r)
    // Composition end time
  ]);
}, to = (t) => U("mvex", void 0, t.map(io)), io = (t) => V("trex", 0, 0, [
  C(t.track.id),
  // Track ID
  C(1),
  // Default sample description index
  C(0),
  // Default sample duration
  C(0),
  // Default sample size
  C(0)
  // Default sample flags
]), us = (t, e) => U("moof", void 0, [
  so(t),
  ...e.map(ro)
]), so = (t) => V("mfhd", 0, 0, [
  C(t)
  // Sequence number
]), Er = (t) => {
  let e = 0, i = 0;
  const s = 0, r = 0, a = t.type === "delta";
  return i |= +a, a ? e |= 1 : e |= 2, e << 24 | i << 16 | s << 8 | r;
}, ro = (t) => U("traf", void 0, [
  ao(t),
  no(t),
  oo(t)
]), ao = (t) => {
  f(t.currentChunk);
  let e = 0;
  e |= 8, e |= 16, e |= 32, e |= 131072;
  const i = t.currentChunk.samples[1] ?? t.currentChunk.samples[0], s = {
    duration: i.timescaleUnitsToNextSample,
    size: i.size,
    flags: Er(i)
  };
  return V("tfhd", 0, e, [
    C(t.track.id),
    // Track ID
    C(s.duration),
    // Default sample duration
    C(s.size),
    // Default sample size
    C(s.flags)
    // Default sample flags
  ]);
}, no = (t) => (f(t.currentChunk), V("tfdt", 1, 0, [
  Pe(Q(t.currentChunk.startTimestamp, t.timescale))
  // Base Media Decode Time
])), oo = (t) => {
  f(t.currentChunk);
  const e = t.currentChunk.samples.map((g) => g.timescaleUnitsToNextSample), i = t.currentChunk.samples.map((g) => g.size), s = t.currentChunk.samples.map(Er), r = t.currentChunk.samples.map((g) => Q(g.timestamp - g.decodeTimestamp, t.timescale)), a = new Set(e), n = new Set(i), o = new Set(s), c = new Set(r), l = o.size === 2 && s[0] !== s[1], d = a.size > 1, h = n.size > 1, u = !l && o.size > 1, m = c.size > 1 || [...c].some((g) => g !== 0);
  let p = 0;
  return p |= 1, p |= 4 * +l, p |= 256 * +d, p |= 512 * +h, p |= 1024 * +u, p |= 2048 * +m, V("trun", 1, p, [
    C(t.currentChunk.samples.length),
    // Sample count
    C(t.currentChunk.offset - t.currentChunk.moofOffset || 0),
    // Data offset
    l ? C(s[0]) : [],
    t.currentChunk.samples.map((g, w) => [
      d ? C(e[w]) : [],
      // Sample duration
      h ? C(i[w]) : [],
      // Sample size
      u ? C(s[w]) : [],
      // Sample flags
      // Sample composition time offsets
      m ? ye(r[w]) : []
    ])
  ]);
}, co = (t) => U("mfra", void 0, [
  ...t.map(lo),
  ho()
]), lo = (t, e) => V("tfra", 1, 0, [
  C(t.track.id),
  // Track ID
  C(63),
  // This specifies that traf number, trun number and sample number are 32-bit ints
  C(t.finalizedChunks.length),
  // Number of entries
  t.finalizedChunks.map((s) => [
    Pe(Q(s.samples[0].timestamp, t.timescale)),
    // Time (in presentation time)
    Pe(s.moofOffset),
    // moof offset
    C(e + 1),
    // traf number
    C(1),
    // trun number
    C(1)
    // Sample number
  ])
]), ho = () => V("mfro", 0, 0, [
  // This value needs to be overwritten manually from the outside, where the actual size of the enclosing mfra box
  // is known
  C(0)
  // Size
]), uo = () => U("vtte"), fo = (t, e, i, s, r) => U("vttc", void 0, [
  r !== null ? U("vsid", [ye(r)]) : null,
  i !== null ? U("iden", [...he.encode(i)]) : null,
  e !== null ? U("ctim", [...he.encode(gr(e))]) : null,
  s !== null ? U("sttg", [...he.encode(s)]) : null,
  U("payl", [...he.encode(t)])
]), mo = (t) => U("vtta", [...he.encode(t)]), po = {
  avc: "avc1",
  hevc: "hvc1",
  vp8: "vp08",
  vp9: "vp09",
  av1: "av01"
}, go = {
  avc: Dn,
  hevc: Nn,
  vp8: hs,
  vp9: hs,
  av1: Vn
}, Pr = {
  aac: "mp4a",
  mp3: "mp4a",
  opus: "Opus",
  vorbis: "mp4a",
  flac: "fLaC",
  ulaw: "ulaw",
  alaw: "alaw",
  "pcm-u8": "raw ",
  "pcm-s8": "sowt",
  "pcm-s16": "sowt",
  "pcm-s16be": "twos",
  "pcm-s24": "in24",
  "pcm-s24be": "in24",
  "pcm-s32": "in32",
  "pcm-s32be": "in32",
  "pcm-f32": "fl32",
  "pcm-f32be": "fl32"
}, wo = {
  aac: Yt,
  mp3: Yt,
  opus: $n,
  vorbis: Yt,
  flac: jn,
  "pcm-s24": De,
  "pcm-s24be": De,
  "pcm-s32": De,
  "pcm-s32be": De,
  "pcm-f32": De,
  "pcm-f32be": De
}, yo = {
  webvtt: "wvtt"
}, bo = {
  webvtt: Qn
}, Ni = class {
  constructor(t) {
    this.mutex = new ft(), this.firstMediaStreamTimestamp = null, this.trackTimestampInfo = /* @__PURE__ */ new WeakMap(), this.output = t;
  }
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  onTrackClose(t) {
  }
  validateAndNormalizeTimestamp(t, e, i) {
    e += t.source._timestampOffset;
    let s = this.trackTimestampInfo.get(t);
    if (!s) {
      if (!i)
        throw new Error("First frame must be a key frame.");
      s = {
        maxTimestamp: e,
        maxTimestampBeforeLastKeyFrame: e
      }, this.trackTimestampInfo.set(t, s);
    }
    if (e < 0)
      throw new Error(`Timestamps must be non-negative (got ${e}s).`);
    if (i && (s.maxTimestampBeforeLastKeyFrame = s.maxTimestamp), e < s.maxTimestampBeforeLastKeyFrame)
      throw new Error(
        `Timestamps cannot be smaller than the highest timestamp of the previous run (a run begins with a key frame and ends right before the next key frame). Got ${e}s, but highest timestamp is ${s.maxTimestampBeforeLastKeyFrame}s.`
      );
    return s.maxTimestamp = Math.max(s.maxTimestamp, e), e;
  }
}, Fr = class {
  constructor() {
    this.ensureMonotonicity = !1, this.trackedWrites = null, this.trackedStart = -1, this.trackedEnd = -1;
  }
  start() {
  }
  maybeTrackWrites(t) {
    if (!this.trackedWrites)
      return;
    let e = this.getPos();
    if (e < this.trackedStart) {
      if (e + t.byteLength <= this.trackedStart)
        return;
      t = t.subarray(this.trackedStart - e), e = 0;
    }
    const i = e + t.byteLength - this.trackedStart;
    let s = this.trackedWrites.byteLength;
    for (; s < i; )
      s *= 2;
    if (s !== this.trackedWrites.byteLength) {
      const r = new Uint8Array(s);
      r.set(this.trackedWrites, 0), this.trackedWrites = r;
    }
    this.trackedWrites.set(t, e - this.trackedStart), this.trackedEnd = Math.max(this.trackedEnd, e + t.byteLength);
  }
  startTrackingWrites() {
    this.trackedWrites = new Uint8Array(2 ** 10), this.trackedStart = this.getPos(), this.trackedEnd = this.trackedStart;
  }
  stopTrackingWrites() {
    if (!this.trackedWrites)
      throw new Error("Internal error: Can't get tracked writes since nothing was tracked.");
    const e = {
      data: this.trackedWrites.subarray(0, this.trackedEnd - this.trackedStart),
      start: this.trackedStart,
      end: this.trackedEnd
    };
    return this.trackedWrites = null, e;
  }
}, Jt = 2 ** 16, Zt = 2 ** 32, Rr = class extends Fr {
  constructor(t) {
    if (super(), this.pos = 0, this.maxPos = 0, this.target = t, this.supportsResize = "resize" in new ArrayBuffer(0), this.supportsResize)
      try {
        this.buffer = new ArrayBuffer(Jt, { maxByteLength: Zt });
      } catch {
        this.buffer = new ArrayBuffer(Jt), this.supportsResize = !1;
      }
    else
      this.buffer = new ArrayBuffer(Jt);
    this.bytes = new Uint8Array(this.buffer);
  }
  ensureSize(t) {
    let e = this.buffer.byteLength;
    for (; e < t; ) e *= 2;
    if (e !== this.buffer.byteLength) {
      if (e > Zt)
        throw new Error(
          `ArrayBuffer exceeded maximum size of ${Zt} bytes. Please consider using another target.`
        );
      if (this.supportsResize)
        this.buffer.resize(e);
      else {
        const i = new ArrayBuffer(e), s = new Uint8Array(i);
        s.set(this.bytes, 0), this.buffer = i, this.bytes = s;
      }
    }
  }
  write(t) {
    this.maybeTrackWrites(t), this.ensureSize(this.pos + t.byteLength), this.bytes.set(t, this.pos), this.pos += t.byteLength, this.maxPos = Math.max(this.maxPos, this.pos);
  }
  seek(t) {
    this.pos = t;
  }
  getPos() {
    return this.pos;
  }
  async flush() {
  }
  async finalize() {
    this.ensureSize(this.pos), this.target.buffer = this.buffer.slice(0, Math.max(this.maxPos, this.pos));
  }
  async close() {
  }
  getSlice(t, e) {
    return this.bytes.slice(t, e);
  }
}, ko = 2 ** 24, So = 2, vo = class extends Fr {
  constructor(t) {
    super(), this.pos = 0, this.sections = [], this.lastWriteEnd = 0, this.lastFlushEnd = 0, this.writer = null, this.chunks = [], this.target = t, this.chunked = t._options.chunked ?? !1, this.chunkSize = t._options.chunkSize ?? ko;
  }
  start() {
    this.writer = this.target._writable.getWriter();
  }
  write(t) {
    if (this.pos > this.lastWriteEnd) {
      const e = this.pos - this.lastWriteEnd;
      this.pos = this.lastWriteEnd, this.write(new Uint8Array(e));
    }
    this.maybeTrackWrites(t), this.sections.push({
      data: t.slice(),
      start: this.pos
    }), this.pos += t.byteLength, this.lastWriteEnd = Math.max(this.lastWriteEnd, this.pos);
  }
  seek(t) {
    this.pos = t;
  }
  getPos() {
    return this.pos;
  }
  async flush() {
    if (this.pos > this.lastWriteEnd) {
      const i = this.pos - this.lastWriteEnd;
      this.pos = this.lastWriteEnd, this.write(new Uint8Array(i));
    }
    if (f(this.writer), this.sections.length === 0) return;
    const t = [], e = [...this.sections].sort((i, s) => i.start - s.start);
    t.push({
      start: e[0].start,
      size: e[0].data.byteLength
    });
    for (let i = 1; i < e.length; i++) {
      const s = t[t.length - 1], r = e[i];
      r.start <= s.start + s.size ? s.size = Math.max(s.size, r.start + r.data.byteLength - s.start) : t.push({
        start: r.start,
        size: r.data.byteLength
      });
    }
    for (const i of t) {
      i.data = new Uint8Array(i.size);
      for (const s of this.sections)
        i.start <= s.start && s.start < i.start + i.size && i.data.set(s.data, s.start - i.start);
      if (this.writer.desiredSize !== null && this.writer.desiredSize <= 0 && await this.writer.ready, this.chunked)
        this.writeDataIntoChunks(i.data, i.start), this.tryToFlushChunks();
      else {
        if (this.ensureMonotonicity && i.start !== this.lastFlushEnd)
          throw new Error("Internal error: Monotonicity violation.");
        this.writer.write({
          type: "write",
          data: i.data,
          position: i.start
        }), this.lastFlushEnd = i.start + i.data.byteLength;
      }
    }
    this.sections.length = 0;
  }
  writeDataIntoChunks(t, e) {
    let i = this.chunks.findIndex((o) => o.start <= e && e < o.start + this.chunkSize);
    i === -1 && (i = this.createChunk(e));
    const s = this.chunks[i], r = e - s.start, a = t.subarray(0, Math.min(this.chunkSize - r, t.byteLength));
    s.data.set(a, r);
    const n = {
      start: r,
      end: r + a.byteLength
    };
    if (this.insertSectionIntoChunk(s, n), s.written[0].start === 0 && s.written[0].end === this.chunkSize && (s.shouldFlush = !0), this.chunks.length > So) {
      for (let o = 0; o < this.chunks.length - 1; o++)
        this.chunks[o].shouldFlush = !0;
      this.tryToFlushChunks();
    }
    a.byteLength < t.byteLength && this.writeDataIntoChunks(t.subarray(a.byteLength), e + a.byteLength);
  }
  insertSectionIntoChunk(t, e) {
    let i = 0, s = t.written.length - 1, r = -1;
    for (; i <= s; ) {
      const a = Math.floor(i + (s - i + 1) / 2);
      t.written[a].start <= e.start ? (i = a + 1, r = a) : s = a - 1;
    }
    for (t.written.splice(r + 1, 0, e), (r === -1 || t.written[r].end < e.start) && r++; r < t.written.length - 1 && t.written[r].end >= t.written[r + 1].start; )
      t.written[r].end = Math.max(t.written[r].end, t.written[r + 1].end), t.written.splice(r + 1, 1);
  }
  createChunk(t) {
    const i = {
      start: Math.floor(t / this.chunkSize) * this.chunkSize,
      data: new Uint8Array(this.chunkSize),
      written: [],
      shouldFlush: !1
    };
    return this.chunks.push(i), this.chunks.sort((s, r) => s.start - r.start), this.chunks.indexOf(i);
  }
  tryToFlushChunks(t = !1) {
    f(this.writer);
    for (let e = 0; e < this.chunks.length; e++) {
      const i = this.chunks[e];
      if (!(!i.shouldFlush && !t)) {
        for (const s of i.written) {
          const r = i.start + s.start;
          if (this.ensureMonotonicity && r !== this.lastFlushEnd)
            throw new Error("Internal error: Monotonicity violation.");
          this.writer.write({
            type: "write",
            data: i.data.subarray(s.start, s.end),
            position: r
          }), this.lastFlushEnd = i.start + s.end;
        }
        this.chunks.splice(e--, 1);
      }
    }
  }
  finalize() {
    return this.chunked && this.tryToFlushChunks(!0), f(this.writer), this.writer.close();
  }
  async close() {
    return this.writer?.close();
  }
}, Vi = class {
  constructor() {
    this._output = null;
  }
}, fi = class extends Vi {
  constructor() {
    super(...arguments), this.buffer = null;
  }
  /** @internal */
  _createWriter() {
    return new Rr(this);
  }
}, ei = class extends Vi {
  constructor(t, e = {}) {
    if (super(), !(t instanceof WritableStream))
      throw new TypeError("StreamTarget requires a WritableStream instance.");
    if (e != null && typeof e != "object")
      throw new TypeError("StreamTarget options, when provided, must be an object.");
    if (e.chunked !== void 0 && typeof e.chunked != "boolean")
      throw new TypeError("options.chunked, when provided, must be a boolean.");
    if (e.chunkSize !== void 0 && (!Number.isInteger(e.chunkSize) || e.chunkSize < 1024))
      throw new TypeError("options.chunkSize, when provided, must be an integer and not smaller than 1024.");
    this._writable = t, this._options = e;
  }
  /** @internal */
  _createWriter() {
    return new vo(this);
  }
}, mi = 1e3, To = 2082844800, Q = (t, e, i = !0) => {
  const s = t * e;
  return i ? Math.round(s) : s;
}, xo = class extends Ni {
  constructor(t, e) {
    super(t), this.auxTarget = new fi(), this.auxWriter = this.auxTarget._createWriter(), this.auxBoxWriter = new ds(this.auxWriter), this.mdat = null, this.trackDatas = [], this.creationTime = Math.floor(Date.now() / 1e3) + To, this.finalizedChunks = [], this.nextFragmentNumber = 1, this.maxWrittenTimestamp = -1 / 0, this.format = e, this.writer = t._writer, this.boxWriter = new ds(this.writer), this.isMov = e instanceof jr;
    const i = this.writer instanceof Rr ? "in-memory" : !1;
    this.fastStart = e._options.fastStart ?? i, this.isFragmented = this.fastStart === "fragmented", (this.fastStart === "in-memory" || this.isFragmented) && (this.writer.ensureMonotonicity = !0), this.minimumFragmentDuration = e._options.minimumFragmentDuration ?? 1;
  }
  async start() {
    const t = await this.mutex.acquire(), e = this.output._tracks.some((i) => i.type === "video" && i.source._codec === "avc");
    if (this.format._options.onFtyp && this.writer.startTrackingWrites(), this.boxWriter.writeBox(wn({
      isMov: this.isMov,
      holdsAvc: e,
      fragmented: this.isFragmented
    })), this.format._options.onFtyp) {
      const { data: i, start: s } = this.writer.stopTrackingWrites();
      this.format._options.onFtyp(i, s);
    }
    this.fastStart === "in-memory" ? this.mdat = Gt(!1) : this.isFragmented || (this.format._options.onMdat && this.writer.startTrackingWrites(), this.mdat = Gt(!0), this.boxWriter.writeBox(this.mdat)), await this.writer.flush(), t();
  }
  getVideoTrackData(t, e, i) {
    const s = this.trackDatas.find((c) => c.track === t);
    if (s)
      return s;
    ur(i), f(i), f(i.decoderConfig);
    const r = { ...i.decoderConfig };
    f(r.codedWidth !== void 0), f(r.codedHeight !== void 0);
    let a = !1;
    if (t.source._codec === "avc" && !r.description) {
      const c = wr(e.data);
      if (!c)
        throw new Error(
          "Couldn't extract an AVCDecoderConfigurationRecord from the AVC packet. Make sure the packets are in Annex B format (as specified in ITU-T-REC-H.264) when not providing a description, or provide a description (must be an AVCDecoderConfigurationRecord as specified in ISO 14496-15) and ensure the packets are in AVCC format."
        );
      r.description = an(c), a = !0;
    } else if (t.source._codec === "hevc" && !r.description) {
      const c = yr(e.data);
      if (!c)
        throw new Error(
          "Couldn't extract an HEVCDecoderConfigurationRecord from the HEVC packet. Make sure the packets are in Annex B format (as specified in ITU-T-REC-H.265) when not providing a description, or provide a description (must be an HEVCDecoderConfigurationRecord as specified in ISO 14496-15) and ensure the packets are in HEVC format."
        );
      r.description = mn(c), a = !0;
    }
    const n = Va(1 / (t.metadata.frameRate ?? 57600), 1e6).denominator, o = {
      track: t,
      type: "video",
      info: {
        width: r.codedWidth,
        height: r.codedHeight,
        decoderConfig: r,
        requiresAnnexBTransformation: a
      },
      timescale: n,
      samples: [],
      sampleQueue: [],
      timestampProcessingQueue: [],
      timeToSampleTable: [],
      compositionTimeOffsetTable: [],
      lastTimescaleUnits: null,
      lastSample: null,
      finalizedChunks: [],
      currentChunk: null,
      compactlyCodedChunkTable: []
    };
    return this.trackDatas.push(o), this.trackDatas.sort((c, l) => c.track.id - l.track.id), o;
  }
  getAudioTrackData(t, e) {
    const i = this.trackDatas.find((r) => r.track === t);
    if (i)
      return i;
    zi(e), f(e), f(e.decoderConfig);
    const s = {
      track: t,
      type: "audio",
      info: {
        numberOfChannels: e.decoderConfig.numberOfChannels,
        sampleRate: e.decoderConfig.sampleRate,
        decoderConfig: e.decoderConfig,
        requiresPcmTransformation: !this.isFragmented && K.includes(t.source._codec)
      },
      timescale: e.decoderConfig.sampleRate,
      samples: [],
      sampleQueue: [],
      timestampProcessingQueue: [],
      timeToSampleTable: [],
      compositionTimeOffsetTable: [],
      lastTimescaleUnits: null,
      lastSample: null,
      finalizedChunks: [],
      currentChunk: null,
      compactlyCodedChunkTable: []
    };
    return this.trackDatas.push(s), this.trackDatas.sort((r, a) => r.track.id - a.track.id), s;
  }
  getSubtitleTrackData(t, e) {
    const i = this.trackDatas.find((r) => r.track === t);
    if (i)
      return i;
    fr(e), f(e), f(e.config);
    const s = {
      track: t,
      type: "subtitle",
      info: {
        config: e.config
      },
      timescale: 1e3,
      // Reasonable
      samples: [],
      sampleQueue: [],
      timestampProcessingQueue: [],
      timeToSampleTable: [],
      compositionTimeOffsetTable: [],
      lastTimescaleUnits: null,
      lastSample: null,
      finalizedChunks: [],
      currentChunk: null,
      compactlyCodedChunkTable: [],
      lastCueEndTimestamp: 0,
      cueQueue: [],
      nextSourceId: 0,
      cueToSourceId: /* @__PURE__ */ new WeakMap()
    };
    return this.trackDatas.push(s), this.trackDatas.sort((r, a) => r.track.id - a.track.id), s;
  }
  async addEncodedVideoPacket(t, e, i) {
    const s = await this.mutex.acquire();
    try {
      const r = this.getVideoTrackData(t, e, i);
      let a = e.data;
      if (r.info.requiresAnnexBTransformation) {
        const c = rn(a);
        if (!c)
          throw new Error(
            "Failed to transform packet data. Make sure all packets are provided in Annex B format, as specified in ITU-T-REC-H.264 and ITU-T-REC-H.265."
          );
        a = c;
      }
      const n = this.validateAndNormalizeTimestamp(
        r.track,
        e.timestamp,
        e.type === "key"
      ), o = this.createSampleForTrack(
        r,
        a,
        n,
        e.duration,
        e.type
      );
      await this.registerSample(r, o);
    } finally {
      s();
    }
  }
  async addEncodedAudioPacket(t, e, i) {
    const s = await this.mutex.acquire();
    try {
      const r = this.getAudioTrackData(t, i), a = this.validateAndNormalizeTimestamp(
        r.track,
        e.timestamp,
        e.type === "key"
      ), n = this.createSampleForTrack(
        r,
        e.data,
        a,
        e.duration,
        e.type
      );
      r.info.requiresPcmTransformation && await this.maybePadWithSilence(r, a), await this.registerSample(r, n);
    } finally {
      s();
    }
  }
  async maybePadWithSilence(t, e) {
    const i = H(t.samples), s = i ? i.timestamp + i.duration : 0, r = e - s, a = Q(r, t.timescale);
    if (a > 0) {
      const { sampleSize: n, silentValue: o } = Be(
        t.info.decoderConfig.codec
      ), c = a * t.info.numberOfChannels, l = new Uint8Array(n * c).fill(o), d = this.createSampleForTrack(
        t,
        new Uint8Array(l.buffer),
        s,
        r,
        "key"
      );
      await this.registerSample(t, d);
    }
  }
  async addSubtitleCue(t, e, i) {
    const s = await this.mutex.acquire();
    try {
      const r = this.getSubtitleTrackData(t, i);
      this.validateAndNormalizeTimestamp(r.track, e.timestamp, !0), t.source._codec === "webvtt" && (r.cueQueue.push(e), await this.processWebVTTCues(r, e.timestamp));
    } finally {
      s();
    }
  }
  async processWebVTTCues(t, e) {
    for (; t.cueQueue.length > 0; ) {
      const i = /* @__PURE__ */ new Set([]);
      for (const c of t.cueQueue)
        f(c.timestamp <= e), f(t.lastCueEndTimestamp <= c.timestamp + c.duration), i.add(Math.max(c.timestamp, t.lastCueEndTimestamp)), i.add(c.timestamp + c.duration);
      const s = [...i].sort((c, l) => c - l), r = s[0], a = s[1] ?? r;
      if (e < a)
        break;
      if (t.lastCueEndTimestamp < r) {
        this.auxWriter.seek(0);
        const c = uo();
        this.auxBoxWriter.writeBox(c);
        const l = this.auxWriter.getSlice(0, this.auxWriter.getPos()), d = this.createSampleForTrack(
          t,
          l,
          t.lastCueEndTimestamp,
          r - t.lastCueEndTimestamp,
          "key"
        );
        await this.registerSample(t, d), t.lastCueEndTimestamp = r;
      }
      this.auxWriter.seek(0);
      for (let c = 0; c < t.cueQueue.length; c++) {
        const l = t.cueQueue[c];
        if (l.timestamp >= a)
          break;
        Pt.lastIndex = 0;
        const d = Pt.test(l.text), h = l.timestamp + l.duration;
        let u = t.cueToSourceId.get(l);
        if (u === void 0 && a < h && (u = t.nextSourceId++, t.cueToSourceId.set(l, u)), l.notes) {
          const p = mo(l.notes);
          this.auxBoxWriter.writeBox(p);
        }
        const m = fo(
          l.text,
          d ? r : null,
          l.identifier ?? null,
          l.settings ?? null,
          u ?? null
        );
        this.auxBoxWriter.writeBox(m), h === a && t.cueQueue.splice(c--, 1);
      }
      const n = this.auxWriter.getSlice(0, this.auxWriter.getPos()), o = this.createSampleForTrack(t, n, r, a - r, "key");
      await this.registerSample(t, o), t.lastCueEndTimestamp = a;
    }
  }
  createSampleForTrack(t, e, i, s, r) {
    return {
      timestamp: i,
      decodeTimestamp: i,
      // This may be refined later
      duration: s,
      data: e,
      size: e.byteLength,
      type: r,
      timescaleUnitsToNextSample: Q(s, t.timescale)
      // Will be refined
    };
  }
  processTimestamps(t) {
    if (t.timestampProcessingQueue.length === 0)
      return;
    if (t.type === "audio" && t.info.requiresPcmTransformation) {
      let i = 0;
      for (let s = 0; s < t.timestampProcessingQueue.length; s++) {
        const r = t.timestampProcessingQueue[s], a = Q(r.duration, t.timescale);
        i += a;
      }
      if (t.timeToSampleTable.length === 0)
        t.timeToSampleTable.push({
          sampleCount: i,
          sampleDelta: 1
        });
      else {
        const s = H(t.timeToSampleTable);
        s.sampleCount += i;
      }
      t.timestampProcessingQueue.length = 0;
      return;
    }
    const e = t.timestampProcessingQueue.map((i) => i.timestamp).sort((i, s) => i - s);
    for (let i = 0; i < t.timestampProcessingQueue.length; i++) {
      const s = t.timestampProcessingQueue[i];
      s.decodeTimestamp = e[i], !this.isFragmented && t.lastTimescaleUnits === null && (s.decodeTimestamp = 0);
      const r = Q(s.timestamp - s.decodeTimestamp, t.timescale), a = Q(s.duration, t.timescale);
      if (t.lastTimescaleUnits !== null) {
        f(t.lastSample);
        const n = Q(s.decodeTimestamp, t.timescale, !1), o = Math.round(n - t.lastTimescaleUnits);
        if (f(o >= 0), t.lastTimescaleUnits += o, t.lastSample.timescaleUnitsToNextSample = o, !this.isFragmented) {
          let c = H(t.timeToSampleTable);
          if (f(c), c.sampleCount === 1) {
            c.sampleDelta = o;
            const d = t.timeToSampleTable[t.timeToSampleTable.length - 2];
            d && d.sampleDelta === o && (d.sampleCount++, t.timeToSampleTable.pop(), c = d);
          } else c.sampleDelta !== o && (c.sampleCount--, t.timeToSampleTable.push(c = {
            sampleCount: 1,
            sampleDelta: o
          }));
          c.sampleDelta === a ? c.sampleCount++ : t.timeToSampleTable.push({
            sampleCount: 1,
            sampleDelta: a
          });
          const l = H(t.compositionTimeOffsetTable);
          f(l), l.sampleCompositionTimeOffset === r ? l.sampleCount++ : t.compositionTimeOffsetTable.push({
            sampleCount: 1,
            sampleCompositionTimeOffset: r
          });
        }
      } else
        t.lastTimescaleUnits = Q(s.decodeTimestamp, t.timescale, !1), this.isFragmented || (t.timeToSampleTable.push({
          sampleCount: 1,
          sampleDelta: a
        }), t.compositionTimeOffsetTable.push({
          sampleCount: 1,
          sampleCompositionTimeOffset: r
        }));
      t.lastSample = s;
    }
    t.timestampProcessingQueue.length = 0;
  }
  async registerSample(t, e) {
    this.isFragmented ? (t.sampleQueue.push(e), await this.interleaveSamples()) : await this.addSampleToTrack(t, e);
  }
  async addSampleToTrack(t, e) {
    e.type === "key" && this.processTimestamps(t), this.isFragmented || t.samples.push(e);
    let i = !1;
    if (!t.currentChunk)
      i = !0;
    else {
      const s = e.timestamp - t.currentChunk.startTimestamp;
      if (this.isFragmented) {
        const r = this.trackDatas.every((a) => {
          if (t === a)
            return e.type === "key";
          const n = a.sampleQueue[0];
          return n ? n.type === "key" : a.track.source._closed;
        });
        s >= this.minimumFragmentDuration && r && e.timestamp > this.maxWrittenTimestamp && (i = !0, await this.finalizeFragment());
      } else
        i = s >= 0.5;
    }
    i && (t.currentChunk && await this.finalizeCurrentChunk(t), t.currentChunk = {
      startTimestamp: e.timestamp,
      samples: [],
      offset: null,
      moofOffset: null
    }), f(t.currentChunk), t.currentChunk.samples.push(e), t.timestampProcessingQueue.push(e), this.isFragmented && (this.maxWrittenTimestamp = Math.max(this.maxWrittenTimestamp, e.timestamp));
  }
  async finalizeCurrentChunk(t) {
    if (f(!this.isFragmented), !t.currentChunk) return;
    t.finalizedChunks.push(t.currentChunk), this.finalizedChunks.push(t.currentChunk);
    let e = t.currentChunk.samples.length;
    if (t.type === "audio" && t.info.requiresPcmTransformation && (e = t.currentChunk.samples.reduce((i, s) => i + Q(s.duration, t.timescale), 0)), (t.compactlyCodedChunkTable.length === 0 || H(t.compactlyCodedChunkTable).samplesPerChunk !== e) && t.compactlyCodedChunkTable.push({
      firstChunk: t.finalizedChunks.length,
      // 1-indexed
      samplesPerChunk: e
    }), this.fastStart === "in-memory") {
      t.currentChunk.offset = 0;
      return;
    }
    t.currentChunk.offset = this.writer.getPos();
    for (const i of t.currentChunk.samples)
      f(i.data), this.writer.write(i.data), i.data = null;
    await this.writer.flush();
  }
  async interleaveSamples(t = !1) {
    if (f(this.isFragmented), !t) {
      for (const e of this.output._tracks)
        if (!e.source._closed && !this.trackDatas.some((i) => i.track === e))
          return;
    }
    e:
      for (; ; ) {
        let e = null, i = 1 / 0;
        for (const r of this.trackDatas) {
          if (!t && r.sampleQueue.length === 0 && !r.track.source._closed)
            break e;
          r.sampleQueue.length > 0 && r.sampleQueue[0].timestamp < i && (e = r, i = r.sampleQueue[0].timestamp);
        }
        if (!e)
          break;
        const s = e.sampleQueue.shift();
        await this.addSampleToTrack(e, s);
      }
  }
  async finalizeFragment(t = !0) {
    f(this.isFragmented);
    const e = this.nextFragmentNumber++;
    if (e === 1) {
      this.format._options.onMoov && this.writer.startTrackingWrites();
      const u = yt(this.trackDatas, this.creationTime, !0);
      if (this.boxWriter.writeBox(u), this.format._options.onMoov) {
        const { data: m, start: p } = this.writer.stopTrackingWrites();
        this.format._options.onMoov(m, p);
      }
    }
    const i = this.trackDatas.filter((u) => u.currentChunk), s = us(e, i), r = this.writer.getPos(), a = r + this.boxWriter.measureBox(s), n = 16;
    let o = a + n, c = 1 / 0;
    for (const u of i) {
      u.currentChunk.offset = o, u.currentChunk.moofOffset = r;
      for (const m of u.currentChunk.samples)
        o += m.size;
      c = Math.min(c, u.currentChunk.startTimestamp);
    }
    const l = o - a;
    this.format._options.onMoof && this.writer.startTrackingWrites();
    const d = us(e, i);
    if (this.boxWriter.writeBox(d), this.format._options.onMoof) {
      const { data: u, start: m } = this.writer.stopTrackingWrites();
      this.format._options.onMoof(u, m, c);
    }
    f(this.writer.getPos() === a), this.format._options.onMdat && this.writer.startTrackingWrites();
    const h = Gt(l >= 2 ** 32);
    h.size = l, this.boxWriter.writeBox(h), this.writer.seek(a + n);
    for (const u of i)
      for (const m of u.currentChunk.samples)
        this.writer.write(m.data), m.data = null;
    if (this.format._options.onMdat) {
      const { data: u, start: m } = this.writer.stopTrackingWrites();
      this.format._options.onMdat(u, m);
    }
    for (const u of i)
      u.finalizedChunks.push(u.currentChunk), this.finalizedChunks.push(u.currentChunk), u.currentChunk = null;
    t && await this.writer.flush();
  }
  // eslint-disable-next-line @typescript-eslint/no-misused-promises
  async onTrackClose(t) {
    const e = await this.mutex.acquire();
    if (t.type === "subtitle" && t.source._codec === "webvtt") {
      const i = this.trackDatas.find((s) => s.track === t);
      i && await this.processWebVTTCues(i, 1 / 0);
    }
    this.isFragmented && await this.interleaveSamples(), e();
  }
  /** Finalizes the file, making it ready for use. Must be called after all video and audio chunks have been added. */
  async finalize() {
    const t = await this.mutex.acquire();
    for (const e of this.trackDatas)
      e.type === "subtitle" && e.track.source._codec === "webvtt" && await this.processWebVTTCues(e, 1 / 0);
    if (this.isFragmented)
      await this.interleaveSamples(!0), await this.finalizeFragment(!1);
    else
      for (const e of this.trackDatas)
        this.processTimestamps(e), await this.finalizeCurrentChunk(e);
    if (this.fastStart === "in-memory") {
      f(this.mdat);
      let e;
      for (let s = 0; s < 2; s++) {
        const r = yt(this.trackDatas, this.creationTime), a = this.boxWriter.measureBox(r);
        e = this.boxWriter.measureBox(this.mdat);
        let n = this.writer.getPos() + a + e;
        for (const o of this.finalizedChunks) {
          o.offset = n;
          for (const { data: c } of o.samples)
            f(c), n += c.byteLength, e += c.byteLength;
        }
        if (n < 2 ** 32) break;
        e >= 2 ** 32 && (this.mdat.largeSize = !0);
      }
      this.format._options.onMoov && this.writer.startTrackingWrites();
      const i = yt(this.trackDatas, this.creationTime);
      if (this.boxWriter.writeBox(i), this.format._options.onMoov) {
        const { data: s, start: r } = this.writer.stopTrackingWrites();
        this.format._options.onMoov(s, r);
      }
      this.format._options.onMdat && this.writer.startTrackingWrites(), this.mdat.size = e, this.boxWriter.writeBox(this.mdat);
      for (const s of this.finalizedChunks)
        for (const r of s.samples)
          f(r.data), this.writer.write(r.data), r.data = null;
      if (this.format._options.onMdat) {
        const { data: s, start: r } = this.writer.stopTrackingWrites();
        this.format._options.onMdat(s, r);
      }
    } else if (this.isFragmented) {
      const e = this.writer.getPos(), i = co(this.trackDatas);
      this.boxWriter.writeBox(i);
      const s = this.writer.getPos() - e;
      this.writer.seek(this.writer.getPos() - 4), this.boxWriter.writeU32(s);
    } else {
      f(this.mdat);
      const e = this.boxWriter.offsets.get(this.mdat);
      f(e !== void 0);
      const i = this.writer.getPos() - e;
      if (this.mdat.size = i, this.mdat.largeSize = i >= 2 ** 32, this.boxWriter.patchBox(this.mdat), this.format._options.onMdat) {
        const { data: r, start: a } = this.writer.stopTrackingWrites();
        this.format._options.onMdat(r, a);
      }
      this.format._options.onMoov && this.writer.startTrackingWrites();
      const s = yt(this.trackDatas, this.creationTime);
      if (this.boxWriter.writeBox(s), this.format._options.onMoov) {
        const { data: r, start: a } = this.writer.stopTrackingWrites();
        this.format._options.onMoov(r, a);
      }
    }
    t();
  }
}, pi = class {
  constructor(t) {
    this.value = t;
  }
}, gi = class {
  constructor(t) {
    this.value = t;
  }
}, Br = class {
  constructor(t) {
    this.value = t;
  }
}, Co = [
  440786851,
  408125543
  /* Segment */
], _o = [
  17138,
  17139,
  290298740,
  357149030,
  524531317,
  374648427,
  475249515,
  423732329,
  272869232,
  307544935
  /* Tags */
], Or = [
  ...Co,
  ..._o
], fs = (t) => t < 256 ? 1 : t < 65536 ? 2 : t < 1 << 24 ? 3 : t < 2 ** 32 ? 4 : t < 2 ** 40 ? 5 : 6, ms = (t) => t >= -64 && t < 64 ? 1 : t >= -8192 && t < 8192 ? 2 : t >= -1048576 && t < 1 << 20 ? 3 : t >= -134217728 && t < 1 << 27 ? 4 : t >= -17179869184 && t < 2 ** 34 ? 5 : 6, Io = (t) => {
  if (t < 127)
    return 1;
  if (t < 16383)
    return 2;
  if (t < (1 << 21) - 1)
    return 3;
  if (t < (1 << 28) - 1)
    return 4;
  if (t < 2 ** 35 - 1)
    return 5;
  if (t < 2 ** 42 - 1)
    return 6;
  throw new Error("EBML varint size not supported " + t);
}, Eo = class {
  constructor(t) {
    this.writer = t, this.helper = new Uint8Array(8), this.helperView = new DataView(this.helper.buffer), this.offsets = /* @__PURE__ */ new WeakMap(), this.dataOffsets = /* @__PURE__ */ new WeakMap();
  }
  writeByte(t) {
    this.helperView.setUint8(0, t), this.writer.write(this.helper.subarray(0, 1));
  }
  writeFloat32(t) {
    this.helperView.setFloat32(0, t, !1), this.writer.write(this.helper.subarray(0, 4));
  }
  writeFloat64(t) {
    this.helperView.setFloat64(0, t, !1), this.writer.write(this.helper);
  }
  writeUnsignedInt(t, e = fs(t)) {
    let i = 0;
    switch (e) {
      case 6:
        this.helperView.setUint8(i++, t / 2 ** 40 | 0);
      case 5:
        this.helperView.setUint8(i++, t / 2 ** 32 | 0);
      case 4:
        this.helperView.setUint8(i++, t >> 24);
      case 3:
        this.helperView.setUint8(i++, t >> 16);
      case 2:
        this.helperView.setUint8(i++, t >> 8);
      case 1:
        this.helperView.setUint8(i++, t);
        break;
      default:
        throw new Error("Bad unsigned int size " + e);
    }
    this.writer.write(this.helper.subarray(0, i));
  }
  writeSignedInt(t, e = ms(t)) {
    t < 0 && (t += 2 ** (e * 8)), this.writeUnsignedInt(t, e);
  }
  writeVarInt(t, e = Io(t)) {
    let i = 0;
    switch (e) {
      case 1:
        this.helperView.setUint8(i++, 128 | t);
        break;
      case 2:
        this.helperView.setUint8(i++, 64 | t >> 8), this.helperView.setUint8(i++, t);
        break;
      case 3:
        this.helperView.setUint8(i++, 32 | t >> 16), this.helperView.setUint8(i++, t >> 8), this.helperView.setUint8(i++, t);
        break;
      case 4:
        this.helperView.setUint8(i++, 16 | t >> 24), this.helperView.setUint8(i++, t >> 16), this.helperView.setUint8(i++, t >> 8), this.helperView.setUint8(i++, t);
        break;
      case 5:
        this.helperView.setUint8(i++, 8 | t / 2 ** 32 & 7), this.helperView.setUint8(i++, t >> 24), this.helperView.setUint8(i++, t >> 16), this.helperView.setUint8(i++, t >> 8), this.helperView.setUint8(i++, t);
        break;
      case 6:
        this.helperView.setUint8(i++, 4 | t / 2 ** 40 & 3), this.helperView.setUint8(i++, t / 2 ** 32 | 0), this.helperView.setUint8(i++, t >> 24), this.helperView.setUint8(i++, t >> 16), this.helperView.setUint8(i++, t >> 8), this.helperView.setUint8(i++, t);
        break;
      default:
        throw new Error("Bad EBML varint size " + e);
    }
    this.writer.write(this.helper.subarray(0, i));
  }
  // Assumes the string is ASCII
  writeString(t) {
    this.writer.write(new Uint8Array(t.split("").map((e) => e.charCodeAt(0))));
  }
  writeEBML(t) {
    if (t !== null) {
      if (t instanceof Uint8Array)
        this.writer.write(t);
      else if (Array.isArray(t))
        for (const e of t)
          this.writeEBML(e);
      else if (this.offsets.set(t, this.writer.getPos()), this.writeUnsignedInt(t.id), Array.isArray(t.data)) {
        const e = this.writer.getPos(), i = t.size === -1 ? 1 : t.size ?? 4;
        t.size === -1 ? this.writeByte(255) : this.writer.seek(this.writer.getPos() + i);
        const s = this.writer.getPos();
        if (this.dataOffsets.set(t, s), this.writeEBML(t.data), t.size !== -1) {
          const r = this.writer.getPos() - s, a = this.writer.getPos();
          this.writer.seek(e), this.writeVarInt(r, i), this.writer.seek(a);
        }
      } else if (typeof t.data == "number") {
        const e = t.size ?? fs(t.data);
        this.writeVarInt(e), this.writeUnsignedInt(t.data, e);
      } else if (typeof t.data == "string")
        this.writeVarInt(t.data.length), this.writeString(t.data);
      else if (t.data instanceof Uint8Array)
        this.writeVarInt(t.data.byteLength, t.size), this.writer.write(t.data);
      else if (t.data instanceof pi)
        this.writeVarInt(4), this.writeFloat32(t.data.value);
      else if (t.data instanceof gi)
        this.writeVarInt(8), this.writeFloat64(t.data.value);
      else if (t.data instanceof Br) {
        const e = t.size ?? ms(t.data.value);
        this.writeVarInt(e), this.writeSignedInt(t.data.value, e);
      }
    }
  }
}, Mr = 8, Ar = 2, je = 4 + Mr, wi = class {
  constructor(t) {
    this.reader = t, this.pos = 0;
  }
  readBytes(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    return this.pos += t, new Uint8Array(e.buffer, i, t);
  }
  readU8() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 1);
    return this.pos++, t.getUint8(e);
  }
  readS16() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 2);
    return this.pos += 2, t.getInt16(e, !1);
  }
  readVarIntSize() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 1), i = t.getUint8(e);
    let s = 1, r = 128;
    for (; !(i & r) && s < 8; )
      s++, r >>= 1;
    return s;
  }
  readVarInt() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 1), i = t.getUint8(e);
    let s = 1, r = 128;
    for (; !(i & r) && s < Mr; )
      s++, r >>= 1;
    const { view: a, offset: n } = this.reader.getViewAndOffset(this.pos, this.pos + s);
    let o = i & r - 1;
    for (let c = 1; c < s; c++)
      o *= 256, o += a.getUint8(n + c);
    return this.pos += s, o;
  }
  readUnsignedInt(t) {
    if (t < 1 || t > 8)
      throw new Error("Bad unsigned int size " + t);
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    let s = 0;
    for (let r = 0; r < t; r++)
      s *= 256, s += e.getUint8(i + r);
    return this.pos += t, s;
  }
  readSignedInt(t) {
    let e = this.readUnsignedInt(t);
    return e & 1 << t * 8 - 1 && (e -= 2 ** (t * 8)), e;
  }
  readFloat(t) {
    if (t === 0)
      return 0;
    if (t !== 4 && t !== 8)
      throw new Error("Bad float size " + t);
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t), s = t === 4 ? e.getFloat32(i, !1) : e.getFloat64(i, !1);
    return this.pos += t, s;
  }
  readString(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    return this.pos += t, String.fromCharCode(...new Uint8Array(e.buffer, i, t));
  }
  readElementId() {
    const t = this.readVarIntSize();
    return this.readUnsignedInt(t);
  }
  readElementSize() {
    let t = this.readU8();
    return t === 255 ? t = null : (this.pos--, t = this.readVarInt(), t === 72057594037927940 && (t = null)), t;
  }
  readElementHeader() {
    const t = this.readElementId(), e = this.readElementSize();
    return { id: t, size: e };
  }
  /** Returns the byte offset in the file of the next element with a matching ID. */
  async searchForNextElementId(t, e) {
    const s = new Set(t);
    for (; this.pos < e - je; ) {
      this.reader.rangeIsLoaded(this.pos, this.pos + je) || await this.reader.loadRange(this.pos, Math.min(this.pos + 1048576, e));
      const r = this.pos, a = this.readElementHeader();
      if (s.has(a.id))
        return r;
      We(a.size), this.pos += a.size;
    }
    return null;
  }
}, ae = {
  avc: "V_MPEG4/ISO/AVC",
  hevc: "V_MPEGH/ISO/HEVC",
  vp8: "V_VP8",
  vp9: "V_VP9",
  av1: "V_AV1",
  aac: "A_AAC",
  mp3: "A_MPEG/L3",
  opus: "A_OPUS",
  vorbis: "A_VORBIS",
  flac: "A_FLAC",
  "pcm-u8": "A_PCM/INT/LIT",
  "pcm-s16": "A_PCM/INT/LIT",
  "pcm-s16be": "A_PCM/INT/BIG",
  "pcm-s24": "A_PCM/INT/LIT",
  "pcm-s24be": "A_PCM/INT/BIG",
  "pcm-s32": "A_PCM/INT/LIT",
  "pcm-s32be": "A_PCM/INT/BIG",
  "pcm-f32": "A_PCM/FLOAT/IEEE",
  webvtt: "S_TEXT/WEBVTT"
};
function We(t) {
  if (t === null)
    throw new Error("Undefined element size is used in a place where it is not supported.");
}
var Po = -32768, Fo = 2 ** 15 - 1, ps = "https://github.com/Vanilagy/webm-muxer", gs = 6, ws = 5, Ro = {
  video: 1,
  audio: 2,
  subtitle: 17
}, Bo = class extends Ni {
  constructor(t, e) {
    super(t), this.trackDatas = [], this.segment = null, this.segmentInfo = null, this.seekHead = null, this.tracksElement = null, this.segmentDuration = null, this.cues = null, this.currentCluster = null, this.currentClusterStartMsTimestamp = null, this.currentClusterMaxMsTimestamp = null, this.trackDatasInCurrentCluster = /* @__PURE__ */ new Map(), this.duration = 0, this.writer = t._writer, this.format = e, this.ebmlWriter = new Eo(this.writer), this.format._options.streamable && (this.writer.ensureMonotonicity = !0);
  }
  async start() {
    const t = await this.mutex.acquire();
    this.writeEBMLHeader(), this.format._options.streamable || this.createSeekHead(), this.createSegmentInfo(), this.createCues(), await this.writer.flush(), t();
  }
  writeEBMLHeader() {
    this.format._options.onEbmlHeader && this.writer.startTrackingWrites();
    const t = { id: 440786851, data: [
      { id: 17030, data: 1 },
      { id: 17143, data: 1 },
      { id: 17138, data: 4 },
      { id: 17139, data: 8 },
      { id: 17026, data: this.format instanceof qr ? "webm" : "matroska" },
      { id: 17031, data: 2 },
      { id: 17029, data: 2 }
    ] };
    if (this.ebmlWriter.writeEBML(t), this.format._options.onEbmlHeader) {
      const { data: e, start: i } = this.writer.stopTrackingWrites();
      this.format._options.onEbmlHeader(e, i);
    }
  }
  /**
   * Creates a SeekHead element which is positioned near the start of the file and allows the media player to seek to
   * relevant sections more easily. Since we don't know the positions of those sections yet, we'll set them later.
   */
  createSeekHead() {
    const t = new Uint8Array([28, 83, 187, 107]), e = new Uint8Array([21, 73, 169, 102]), i = new Uint8Array([22, 84, 174, 107]), s = { id: 290298740, data: [
      { id: 19899, data: [
        { id: 21419, data: t },
        { id: 21420, size: 5, data: 0 }
      ] },
      { id: 19899, data: [
        { id: 21419, data: e },
        { id: 21420, size: 5, data: 0 }
      ] },
      { id: 19899, data: [
        { id: 21419, data: i },
        { id: 21420, size: 5, data: 0 }
      ] }
    ] };
    this.seekHead = s;
  }
  createSegmentInfo() {
    const t = { id: 17545, data: new gi(0) };
    this.segmentDuration = t;
    const e = { id: 357149030, data: [
      { id: 2807729, data: 1e6 },
      { id: 19840, data: ps },
      { id: 22337, data: ps },
      this.format._options.streamable ? null : t
    ] };
    this.segmentInfo = e;
  }
  createTracks() {
    const t = { id: 374648427, data: [] };
    this.tracksElement = t;
    for (const e of this.trackDatas) {
      const i = ae[e.track.source._codec];
      f(i);
      let s = 0;
      if (e.type === "audio" && e.track.source._codec === "opus") {
        s = 1e6 * 80;
        const r = e.info.decoderConfig.description;
        if (r) {
          const a = te(r), n = Vt(a);
          s = Math.round(1e9 * (n.preSkip / Ai));
        }
      }
      t.data.push({ id: 174, data: [
        { id: 215, data: e.track.id },
        { id: 29637, data: e.track.id },
        { id: 131, data: Ro[e.type] },
        { id: 156, data: 0 },
        { id: 2274716, data: e.track.metadata.languageCode ?? fe },
        { id: 134, data: i },
        { id: 22186, data: 0 },
        { id: 22203, data: s },
        e.type === "video" ? this.videoSpecificTrackInfo(e) : null,
        e.type === "audio" ? this.audioSpecificTrackInfo(e) : null,
        e.type === "subtitle" ? this.subtitleSpecificTrackInfo(e) : null
      ] });
    }
  }
  videoSpecificTrackInfo(t) {
    const { frameRate: e, rotation: i } = t.track.metadata, s = [
      t.info.decoderConfig.description ? {
        id: 25506,
        data: te(t.info.decoderConfig.description)
      } : null,
      e ? {
        id: 2352003,
        data: 1e9 / e
      } : null
    ], r = i ? Fi(-i) : 0, a = t.info.decoderConfig.colorSpace, n = { id: 224, data: [
      { id: 176, data: t.info.width },
      { id: 186, data: t.info.height },
      Js(a) ? {
        id: 21936,
        data: [
          {
            id: 21937,
            data: Ye[a.matrix]
          },
          {
            id: 21946,
            data: Ge[a.transfer]
          },
          {
            id: 21947,
            data: Ke[a.primaries]
          },
          {
            id: 21945,
            data: a.fullRange ? 2 : 1
          }
        ]
      } : null,
      r ? {
        id: 30320,
        data: [
          {
            id: 30321,
            data: 0
            // rectangular
          },
          {
            id: 30325,
            data: new pi((r + 180) % 360 - 180)
            // [0, 270] -> [-180, 90]
          }
        ]
      } : null
    ] };
    return s.push(n), s;
  }
  audioSpecificTrackInfo(t) {
    const e = K.includes(t.track.source._codec) ? Be(t.track.source._codec) : null;
    return [
      t.info.decoderConfig.description ? {
        id: 25506,
        data: te(t.info.decoderConfig.description)
      } : null,
      { id: 225, data: [
        { id: 181, data: new pi(t.info.sampleRate) },
        { id: 159, data: t.info.numberOfChannels },
        e ? { id: 25188, data: 8 * e.sampleSize } : null
      ] }
    ];
  }
  subtitleSpecificTrackInfo(t) {
    return [
      { id: 25506, data: he.encode(t.info.config.description) }
    ];
  }
  createSegment() {
    const t = {
      id: 408125543,
      size: this.format._options.streamable ? -1 : gs,
      data: [
        this.format._options.streamable ? null : this.seekHead,
        this.segmentInfo,
        this.tracksElement
      ]
    };
    if (this.segment = t, this.format._options.onSegmentHeader && this.writer.startTrackingWrites(), this.ebmlWriter.writeEBML(t), this.format._options.onSegmentHeader) {
      const { data: e, start: i } = this.writer.stopTrackingWrites();
      this.format._options.onSegmentHeader(e, i);
    }
  }
  createCues() {
    this.cues = { id: 475249515, data: [] };
  }
  get segmentDataOffset() {
    return f(this.segment), this.ebmlWriter.dataOffsets.get(this.segment);
  }
  getVideoTrackData(t, e) {
    const i = this.trackDatas.find((r) => r.track === t);
    if (i)
      return i;
    ur(e), f(e), f(e.decoderConfig), f(e.decoderConfig.codedWidth !== void 0), f(e.decoderConfig.codedHeight !== void 0);
    const s = {
      track: t,
      type: "video",
      info: {
        width: e.decoderConfig.codedWidth,
        height: e.decoderConfig.codedHeight,
        decoderConfig: e.decoderConfig
      },
      chunkQueue: [],
      lastWrittenMsTimestamp: null
    };
    return t.source._codec === "vp9" ? s.info.decoderConfig = {
      ...s.info.decoderConfig,
      description: new Uint8Array(
        ja(s.info.decoderConfig.codec)
      )
    } : t.source._codec === "av1" && (s.info.decoderConfig = {
      ...s.info.decoderConfig,
      description: new Uint8Array(
        nr(s.info.decoderConfig.codec)
      )
    }), this.trackDatas.push(s), this.trackDatas.sort((r, a) => r.track.id - a.track.id), s;
  }
  getAudioTrackData(t, e) {
    const i = this.trackDatas.find((r) => r.track === t);
    if (i)
      return i;
    zi(e), f(e), f(e.decoderConfig);
    const s = {
      track: t,
      type: "audio",
      info: {
        numberOfChannels: e.decoderConfig.numberOfChannels,
        sampleRate: e.decoderConfig.sampleRate,
        decoderConfig: e.decoderConfig
      },
      chunkQueue: [],
      lastWrittenMsTimestamp: null
    };
    return this.trackDatas.push(s), this.trackDatas.sort((r, a) => r.track.id - a.track.id), s;
  }
  getSubtitleTrackData(t, e) {
    const i = this.trackDatas.find((r) => r.track === t);
    if (i)
      return i;
    fr(e), f(e), f(e.config);
    const s = {
      track: t,
      type: "subtitle",
      info: {
        config: e.config
      },
      chunkQueue: [],
      lastWrittenMsTimestamp: null
    };
    return this.trackDatas.push(s), this.trackDatas.sort((r, a) => r.track.id - a.track.id), s;
  }
  async addEncodedVideoPacket(t, e, i) {
    const s = await this.mutex.acquire();
    try {
      const r = this.getVideoTrackData(t, i), a = e.type === "key";
      let n = this.validateAndNormalizeTimestamp(r.track, e.timestamp, a), o = e.duration;
      t.metadata.frameRate !== void 0 && (n = ni(n, 1 / t.metadata.frameRate), o = ni(o, 1 / t.metadata.frameRate));
      const c = this.createInternalChunk(e.data, n, o, e.type);
      t.source._codec === "vp9" && this.fixVP9ColorSpace(r, c), r.chunkQueue.push(c), await this.interleaveChunks();
    } finally {
      s();
    }
  }
  async addEncodedAudioPacket(t, e, i) {
    const s = await this.mutex.acquire();
    try {
      const r = this.getAudioTrackData(t, i), a = e.type === "key", n = this.validateAndNormalizeTimestamp(r.track, e.timestamp, a), o = this.createInternalChunk(e.data, n, e.duration, e.type);
      r.chunkQueue.push(o), await this.interleaveChunks();
    } finally {
      s();
    }
  }
  async addSubtitleCue(t, e, i) {
    const s = await this.mutex.acquire();
    try {
      const r = this.getSubtitleTrackData(t, i), a = this.validateAndNormalizeTimestamp(r.track, e.timestamp, !0);
      let n = e.text;
      const o = Math.round(a * 1e3);
      Pt.lastIndex = 0, n = n.replace(Pt, (h) => {
        const m = sn(h.slice(1, -1)) - o;
        return `<${gr(m)}>`;
      });
      const c = he.encode(n), l = `${e.settings ?? ""}
${e.identifier ?? ""}
${e.notes ?? ""}`, d = this.createInternalChunk(
        c,
        a,
        e.duration,
        "key",
        l.trim() ? he.encode(l) : null
      );
      r.chunkQueue.push(d), await this.interleaveChunks();
    } finally {
      s();
    }
  }
  async interleaveChunks(t = !1) {
    if (!t) {
      for (const e of this.output._tracks)
        if (!e.source._closed && !this.trackDatas.some((i) => i.track === e))
          return;
    }
    e:
      for (; ; ) {
        let e = null, i = 1 / 0;
        for (const r of this.trackDatas) {
          if (!t && r.chunkQueue.length === 0 && !r.track.source._closed)
            break e;
          r.chunkQueue.length > 0 && r.chunkQueue[0].timestamp < i && (e = r, i = r.chunkQueue[0].timestamp);
        }
        if (!e)
          break;
        const s = e.chunkQueue.shift();
        this.writeBlock(e, s);
      }
    t || await this.writer.flush();
  }
  /**
   * Due to [a bug in Chromium](https://bugs.chromium.org/p/chromium/issues/detail?id=1377842), VP9 streams often
  	 * lack color space information. This method patches in that information.
   */
  fixVP9ColorSpace(t, e) {
    if (e.type !== "key" || !t.info.decoderConfig.colorSpace || !t.info.decoderConfig.colorSpace.matrix) return;
    const i = new Se(e.data);
    if (i.readBits(2) !== 2) return;
    const s = i.readBits(1), a = (i.readBits(1) << 1) + s;
    if (a === 3 && i.skipBits(1), i.readBits(1) || i.readBits(1) !== 0 || (i.skipBits(2), i.readBits(24) !== 4817730)) return;
    a >= 2 && i.skipBits(1);
    const l = {
      rgb: 7,
      bt709: 2,
      bt470bg: 1,
      smpte170m: 3
    }[t.info.decoderConfig.colorSpace.matrix];
    Ra(e.data, i.pos, i.pos + 3, l);
  }
  /** Converts a read-only external chunk into an internal one for easier use. */
  createInternalChunk(t, e, i, s, r = null) {
    return {
      data: t,
      type: s,
      timestamp: e,
      duration: i,
      additions: r
    };
  }
  /** Writes a block containing media data to the file. */
  writeBlock(t, e) {
    this.segment || (this.createTracks(), this.createSegment());
    const i = Math.round(1e3 * e.timestamp), s = this.trackDatas.every((l) => {
      if (t === l)
        return e.type === "key";
      const d = l.chunkQueue[0];
      return d ? d.type === "key" : l.track.source._closed;
    });
    let r = !1;
    if (!this.currentCluster)
      r = !0;
    else {
      f(this.currentClusterStartMsTimestamp !== null), f(this.currentClusterMaxMsTimestamp !== null);
      const l = i - this.currentClusterStartMsTimestamp;
      r = s && i > this.currentClusterMaxMsTimestamp && l >= 1e3 * (this.format._options.minimumClusterDuration ?? 1) || l > Fo;
    }
    r && this.createNewCluster(i);
    const a = i - this.currentClusterStartMsTimestamp;
    if (a < Po)
      return;
    const n = new Uint8Array(4), o = new DataView(n.buffer);
    o.setUint8(0, 128 | t.track.id), o.setInt16(1, a, !1);
    const c = Math.round(1e3 * e.duration);
    if (c === 0 && !e.additions) {
      o.setUint8(3, +(e.type === "key") << 7);
      const l = { id: 163, data: [
        n,
        e.data
      ] };
      this.ebmlWriter.writeEBML(l);
    } else {
      const l = { id: 160, data: [
        { id: 161, data: [
          n,
          e.data
        ] },
        e.type === "delta" ? {
          id: 251,
          data: new Br(t.lastWrittenMsTimestamp - i)
        } : null,
        e.additions ? { id: 30113, data: [
          { id: 166, data: [
            { id: 165, data: e.additions },
            { id: 238, data: 1 }
          ] }
        ] } : null,
        c > 0 ? { id: 155, data: c } : null
      ] };
      this.ebmlWriter.writeEBML(l);
    }
    this.duration = Math.max(this.duration, i + c), t.lastWrittenMsTimestamp = i, this.trackDatasInCurrentCluster.has(t) || this.trackDatasInCurrentCluster.set(t, {
      firstMsTimestamp: i
    }), this.currentClusterMaxMsTimestamp = Math.max(this.currentClusterMaxMsTimestamp, i);
  }
  /** Creates a new Cluster element to contain media chunks. */
  createNewCluster(t) {
    this.currentCluster && this.finalizeCurrentCluster(), this.format._options.onCluster && this.writer.startTrackingWrites(), this.currentCluster = {
      id: 524531317,
      size: this.format._options.streamable ? -1 : ws,
      data: [
        { id: 231, data: t }
      ]
    }, this.ebmlWriter.writeEBML(this.currentCluster), this.currentClusterStartMsTimestamp = t, this.currentClusterMaxMsTimestamp = t, this.trackDatasInCurrentCluster.clear();
  }
  finalizeCurrentCluster() {
    if (f(this.currentCluster), !this.format._options.streamable) {
      const s = this.writer.getPos() - this.ebmlWriter.dataOffsets.get(this.currentCluster), r = this.writer.getPos();
      this.writer.seek(this.ebmlWriter.offsets.get(this.currentCluster) + 4), this.ebmlWriter.writeVarInt(s, ws), this.writer.seek(r);
    }
    if (this.format._options.onCluster) {
      f(this.currentClusterStartMsTimestamp !== null);
      const { data: s, start: r } = this.writer.stopTrackingWrites();
      this.format._options.onCluster(s, r, this.currentClusterStartMsTimestamp / 1e3);
    }
    const t = this.ebmlWriter.offsets.get(this.currentCluster) - this.segmentDataOffset, e = /* @__PURE__ */ new Map();
    for (const [s, { firstMsTimestamp: r }] of this.trackDatasInCurrentCluster)
      e.has(r) || e.set(r, []), e.get(r).push(s);
    const i = [...e.entries()].sort((s, r) => s[0] - r[0]);
    for (const [s, r] of i)
      f(this.cues), this.cues.data.push({ id: 187, data: [
        { id: 179, data: s },
        // Create CueTrackPositions for each track that starts at this timestamp
        ...r.map((a) => ({ id: 183, data: [
          { id: 247, data: a.track.id },
          { id: 241, data: t }
        ] }))
      ] });
  }
  // eslint-disable-next-line @typescript-eslint/no-misused-promises
  async onTrackClose() {
    const t = await this.mutex.acquire();
    await this.interleaveChunks(), t();
  }
  /** Finalizes the file, making it ready for use. Must be called after all media chunks have been added. */
  async finalize() {
    const t = await this.mutex.acquire();
    if (this.segment || (this.createTracks(), this.createSegment()), await this.interleaveChunks(!0), this.currentCluster && this.finalizeCurrentCluster(), f(this.cues), this.ebmlWriter.writeEBML(this.cues), !this.format._options.streamable) {
      const e = this.writer.getPos(), i = this.writer.getPos() - this.segmentDataOffset;
      this.writer.seek(this.ebmlWriter.offsets.get(this.segment) + 4), this.ebmlWriter.writeVarInt(i, gs), this.segmentDuration.data = new gi(this.duration), this.writer.seek(this.ebmlWriter.offsets.get(this.segmentDuration)), this.ebmlWriter.writeEBML(this.segmentDuration), this.seekHead.data[0].data[1].data = this.ebmlWriter.offsets.get(this.cues) - this.segmentDataOffset, this.seekHead.data[1].data[1].data = this.ebmlWriter.offsets.get(this.segmentInfo) - this.segmentDataOffset, this.seekHead.data[2].data[1].data = this.ebmlWriter.offsets.get(this.tracksElement) - this.segmentDataOffset, this.writer.seek(this.ebmlWriter.offsets.get(this.seekHead)), this.ebmlWriter.writeEBML(this.seekHead), this.writer.seek(e);
    }
    t();
  }
}, Oo = 4, Mo = {
  // Layer 3
  1: [-1, 32, 40, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256, 320, -1],
  // Layer 2
  2: [-1, 32, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256, 320, 384, -1],
  // Layer 1
  3: [-1, 32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448, -1]
}, Ao = {
  // Layer 3
  1: [-1, 32, 48, 56, 64, 80, 96, 112, 128, 144, 160, 176, 192, 224, 256, -1],
  // Layer 2
  2: [-1, 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160, -1],
  // Layer 1
  3: [-1, 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160, -1]
}, zo = {
  // MPEG Version 2.5
  0: [11025, 12e3, 8e3, -1],
  // MPEG Version 2 (ISO/IEC 13818-3)
  2: [22050, 24e3, 16e3, -1],
  // MPEG Version 1 (ISO/IEC 11172-3)
  3: [44100, 48e3, 32e3, -1]
}, Uo = 1483304551, Do = 1231971951, No = (t, e, i, s) => Math.floor(t === 3 ? (12 * e / i + s) * 4 : 144 * e / i + s), Vo = (t, e) => t === 3 ? e === 3 ? 21 : 36 : e === 3 ? 13 : 21, Lo = (t, e) => {
  const i = e.pos, s = t >>> 24, r = t >>> 16 & 255, a = t >>> 8 & 255, n = t & 255;
  if (s !== 255 && r !== 255 && a !== 255 && n !== 255)
    return e.pos += 4, null;
  if (e.pos += 1, (r & 224) !== 224)
    return null;
  const o = r >> 3 & 3, c = r >> 1 & 3, l = a >> 4 & 15, d = a >> 2 & 3, h = a >> 1 & 1, u = n >> 6 & 3, m = n >> 4 & 3, p = n >> 3 & 1, g = n >> 2 & 1, w = n & 3, b = o === 3 ? Mo[c]?.[l] : Ao[c]?.[l];
  if (!b || b === -1)
    return null;
  const x = b * 1e3, S = zo[o]?.[d];
  if (!S || S === -1)
    return null;
  const k = No(c, x, S, h);
  if (e.fileSize !== null && e.fileSize - i < k)
    return null;
  let y;
  return o === 3 ? y = c === 3 ? 384 : 1152 : c === 3 ? y = 384 : c === 2 ? y = 1152 : y = 576, {
    startPos: i,
    totalSize: k,
    mpegVersionId: o,
    layer: c,
    bitrate: x,
    frequencyIndex: d,
    sampleRate: S,
    channel: u,
    modeExtension: m,
    copyright: p,
    original: g,
    emphasis: w,
    audioSamplesInFrame: y
  };
}, yi = 1399285583, Wo = 79764919, zr = new Uint32Array(256);
for (let t = 0; t < 256; t++) {
  let e = t << 24;
  for (let i = 0; i < 8; i++)
    e = e & 2147483648 ? e << 1 ^ Wo : e << 1;
  zr[t] = e >>> 0 & 4294967295;
}
var Ur = (t) => {
  const e = Re(t), i = e.getUint32(22, !0);
  e.setUint32(22, 0, !0);
  let s = 0;
  for (let r = 0; r < t.length; r++) {
    const a = t[r];
    s = (s << 8 ^ zr[s >>> 24 ^ a]) >>> 0;
  }
  return e.setUint32(22, i, !0), s;
}, Dr = (t, e, i) => {
  let s = 0, r = null;
  if (t.length > 0)
    if (e.codec === "vorbis") {
      f(e.vorbisInfo);
      const a = e.vorbisInfo.modeBlockflags.length, o = (1 << Da(a - 1)) - 1 << 1, c = (t[0] & o) >> 1;
      if (c >= e.vorbisInfo.modeBlockflags.length)
        throw new Error("Invalid mode number.");
      let l = i;
      const d = e.vorbisInfo.modeBlockflags[c];
      if (r = e.vorbisInfo.blocksizes[d], d === 1) {
        const h = (o | 1) + 1, u = t[0] & h ? 1 : 0;
        l = e.vorbisInfo.blocksizes[u];
      }
      s = l !== null ? l + r >> 2 : 0;
    } else e.codec === "opus" && (s = gn(t).durationInSamples);
  return {
    durationInSamples: s,
    vorbisBlockSize: r
  };
}, ct = 27, qe = 282, Nr = qe + 255 * 255, Vr = class {
  constructor(t) {
    this.reader = t, this.pos = 0;
  }
  readBytes(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    return this.pos += t, new Uint8Array(e.buffer, i, t);
  }
  readU8() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 1);
    return this.pos += 1, t.getUint8(e);
  }
  readU32() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 4);
    return this.pos += 4, t.getUint32(e, !0);
  }
  readI32() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 4);
    return this.pos += 4, t.getInt32(e, !0);
  }
  readI64() {
    const t = this.readU32();
    return this.readI32() * 4294967296 + t;
  }
  readAscii(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    this.pos += t;
    let s = "";
    for (let r = 0; r < t; r++)
      s += String.fromCharCode(e.getUint8(i + r));
    return s;
  }
  readPageHeader() {
    const t = this.pos;
    if (this.readU32() !== yi)
      return null;
    this.pos += 1;
    const i = this.readU8(), s = this.readI64(), r = this.readU32(), a = this.readU32(), n = this.readU32(), o = this.readU8(), c = new Uint8Array(o);
    for (let u = 0; u < o; u++)
      c[u] = this.readU8();
    const l = 27 + o, d = c.reduce((u, m) => u + m, 0), h = l + d;
    return {
      headerStartPos: t,
      totalSize: h,
      dataStartPos: t + l,
      dataSize: d,
      headerType: i,
      granulePosition: s,
      serialNumber: r,
      sequenceNumber: a,
      checksum: n,
      lacingValues: c
    };
  }
  findNextPageHeader(t) {
    for (; this.pos < t - 3; ) {
      const e = this.readU32(), i = e & 255, s = e >>> 8 & 255, r = e >>> 16 & 255, a = e >>> 24 & 255, n = 79;
      if (!(i !== n && s !== n && r !== n && a !== n)) {
        if (this.pos -= 4, e === yi)
          return !0;
        this.pos += 1;
      }
    }
    return !1;
  }
}, Ho = 8192, $o = class extends Ni {
  constructor(t, e) {
    super(t), this.trackDatas = [], this.bosPagesWritten = !1, this.pageBytes = new Uint8Array(Nr), this.pageView = new DataView(this.pageBytes.buffer), this.format = e, this.writer = t._writer, this.writer.ensureMonotonicity = !0;
  }
  async start() {
  }
  addEncodedVideoPacket() {
    throw new Error("Video tracks are not supported.");
  }
  getTrackData(t, e) {
    const i = this.trackDatas.find((a) => a.track === t);
    if (i)
      return i;
    let s;
    do
      s = Math.floor(2 ** 32 * Math.random());
    while (this.trackDatas.some((a) => a.serialNumber === s));
    f(t.source._codec === "vorbis" || t.source._codec === "opus"), zi(e), f(e), f(e.decoderConfig);
    const r = {
      track: t,
      serialNumber: s,
      internalSampleRate: t.source._codec === "opus" ? Ai : e.decoderConfig.sampleRate,
      codecInfo: {
        codec: t.source._codec,
        vorbisInfo: null,
        opusInfo: null
      },
      vorbisLastBlocksize: null,
      packetQueue: [],
      currentTimestampInSamples: 0,
      pagesWritten: 0,
      currentGranulePosition: 0,
      currentLacingValues: [],
      currentPageData: [],
      currentPageSize: 27,
      currentPageStartsWithFreshPacket: !0
    };
    return this.queueHeaderPackets(r, e), this.trackDatas.push(r), r;
  }
  queueHeaderPackets(t, e) {
    if (f(e.decoderConfig), t.track.source._codec === "vorbis") {
      f(e.decoderConfig.description);
      const i = te(e.decoderConfig.description);
      if (i[0] !== 2)
        throw new TypeError("First byte of Vorbis decoder description must be 2.");
      let s = 1;
      const r = () => {
        let m = 0;
        for (; ; ) {
          const p = i[s++];
          if (p === void 0)
            throw new TypeError("Vorbis decoder description is too short.");
          if (m += p, p < 255)
            return m;
        }
      }, a = r(), n = r();
      if (i.length - s <= 0)
        throw new TypeError("Vorbis decoder description is too short.");
      const c = i.subarray(s, s += a), l = i.subarray(s, s += n), d = i.subarray(s);
      t.packetQueue.push({
        data: c,
        endGranulePosition: 0,
        timestamp: 0,
        forcePageFlush: !0
      }, {
        data: l,
        endGranulePosition: 0,
        timestamp: 0,
        forcePageFlush: !1
      }, {
        data: d,
        endGranulePosition: 0,
        timestamp: 0,
        forcePageFlush: !0
        // The last header packet must flush the page
      });
      const u = Re(c).getUint8(28);
      t.codecInfo.vorbisInfo = {
        blocksizes: [
          1 << (u & 15),
          1 << (u >> 4)
        ],
        modeBlockflags: Sr(d).modeBlockflags
      };
    } else if (t.track.source._codec === "opus") {
      if (!e.decoderConfig.description)
        throw new TypeError("For Ogg, Opus decoder description is required.");
      const i = te(e.decoderConfig.description), s = new Uint8Array(16), r = new DataView(s.buffer);
      r.setUint32(0, 1332770163, !1), r.setUint32(4, 1415669619, !1), r.setUint32(8, 0, !0), r.setUint32(12, 0, !0), t.packetQueue.push({
        data: i,
        endGranulePosition: 0,
        timestamp: 0,
        forcePageFlush: !0
      }, {
        data: s,
        endGranulePosition: 0,
        timestamp: 0,
        forcePageFlush: !0
        // The last header packet must flush the page
      }), t.codecInfo.opusInfo = {
        preSkip: Vt(i).preSkip
      };
    }
  }
  async addEncodedAudioPacket(t, e, i) {
    const s = await this.mutex.acquire();
    try {
      const r = this.getTrackData(t, i);
      this.validateAndNormalizeTimestamp(r.track, e.timestamp, e.type === "key");
      const a = r.currentTimestampInSamples, { durationInSamples: n, vorbisBlockSize: o } = Dr(
        e.data,
        r.codecInfo,
        r.vorbisLastBlocksize
      );
      r.currentTimestampInSamples += n, r.vorbisLastBlocksize = o, r.packetQueue.push({
        data: e.data,
        endGranulePosition: r.currentTimestampInSamples,
        timestamp: a / r.internalSampleRate,
        forcePageFlush: !1
      }), await this.interleavePages();
    } finally {
      s();
    }
  }
  addSubtitleCue() {
    throw new Error("Subtitle tracks are not supported.");
  }
  async interleavePages(t = !1) {
    if (!this.bosPagesWritten) {
      for (const e of this.output._tracks)
        if (!e.source._closed && !this.trackDatas.some((i) => i.track === e))
          return;
      for (const e of this.trackDatas)
        for (; e.packetQueue.length > 0; ) {
          const i = e.packetQueue.shift();
          if (this.writePacket(e, i, !1), i.forcePageFlush)
            break;
        }
      this.bosPagesWritten = !0;
    }
    e:
      for (; ; ) {
        let e = null, i = 1 / 0;
        for (const a of this.trackDatas) {
          if (!t && a.packetQueue.length <= 1 && !a.track.source._closed)
            break e;
          a.packetQueue.length > 0 && a.packetQueue[0].timestamp < i && (e = a, i = a.packetQueue[0].timestamp);
        }
        if (!e)
          break;
        const s = e.packetQueue.shift(), r = e.packetQueue.length === 0;
        this.writePacket(e, s, r);
      }
    t || await this.writer.flush();
  }
  writePacket(t, e, i) {
    let s = e.data.length, r = 0, a = 0;
    for (; ; ) {
      t.currentLacingValues.length === 0 && r > 0 && (t.currentPageStartsWithFreshPacket = !1);
      const o = Math.min(255, s);
      t.currentLacingValues.push(o), t.currentPageSize++, a += o;
      const c = s < 255;
      if (t.currentLacingValues.length === 255) {
        const l = e.data.subarray(r, a);
        if (r = a, t.currentPageData.push(l), t.currentPageSize += l.length, this.writePage(t, i && c), c)
          return;
      }
      if (c)
        break;
      s -= 255;
    }
    const n = e.data.subarray(r);
    t.currentPageData.push(n), t.currentPageSize += n.length, t.currentGranulePosition = e.endGranulePosition, (t.currentPageSize >= Ho || e.forcePageFlush) && this.writePage(t, i);
  }
  writePage(t, e) {
    this.pageView.setUint32(0, yi, !0), this.pageView.setUint8(4, 0);
    let i = 0;
    t.currentPageStartsWithFreshPacket || (i |= 1), t.pagesWritten === 0 && (i |= 2), e && (i |= 4), this.pageView.setUint8(5, i);
    const s = t.currentLacingValues.every((o) => o === 255) ? -1 : t.currentGranulePosition;
    Ua(this.pageView, 6, s), this.pageView.setUint32(14, t.serialNumber, !0), this.pageView.setUint32(18, t.pagesWritten, !0), this.pageView.setUint32(22, 0, !0), this.pageView.setUint8(26, t.currentLacingValues.length), this.pageBytes.set(t.currentLacingValues, 27);
    let r = 27 + t.currentLacingValues.length;
    for (const o of t.currentPageData)
      this.pageBytes.set(o, r), r += o.length;
    const a = this.pageBytes.subarray(0, r), n = Ur(a);
    if (this.pageView.setUint32(22, n, !0), t.pagesWritten++, t.currentLacingValues.length = 0, t.currentPageData.length = 0, t.currentPageSize = 27, t.currentPageStartsWithFreshPacket = !0, this.format._options.onPage && this.writer.startTrackingWrites(), this.writer.write(a), this.format._options.onPage) {
      const { data: o, start: c } = this.writer.stopTrackingWrites();
      this.format._options.onPage(o, c, t.track.source);
    }
  }
  // eslint-disable-next-line @typescript-eslint/no-misused-promises
  async onTrackClose() {
    const t = await this.mutex.acquire();
    await this.interleavePages(), t();
  }
  async finalize() {
    const t = await this.mutex.acquire();
    await this.interleavePages(!0);
    for (const e of this.trackDatas)
      e.currentLacingValues.length > 0 && this.writePage(e, !0);
    t();
  }
}, mt = class {
  constructor(t) {
    this.input = t;
  }
}, me = new Uint8Array(0), G = class bi {
  constructor(e, i, s, r, a = -1, n) {
    if (this.data = e, this.type = i, this.timestamp = s, this.duration = r, this.sequenceNumber = a, e === me && n === void 0)
      throw new Error(
        "Internal error: byteLength must be explicitly provided when constructing metadata-only packets."
      );
    if (n === void 0 && (n = e.byteLength), !(e instanceof Uint8Array))
      throw new TypeError("data must be a Uint8Array.");
    if (i !== "key" && i !== "delta")
      throw new TypeError('type must be either "key" or "delta".');
    if (!Number.isFinite(s))
      throw new TypeError("timestamp must be a number.");
    if (!Number.isFinite(r) || r < 0)
      throw new TypeError("duration must be a non-negative number.");
    if (!Number.isFinite(a))
      throw new TypeError("sequenceNumber must be a number.");
    if (!Number.isInteger(n) || n < 0)
      throw new TypeError("byteLength must be a non-negative integer.");
    this.byteLength = n;
  }
  /** If this packet is a metadata-only packet. Metadata-only packets don't contain their packet data. */
  get isMetadataOnly() {
    return this.data === me;
  }
  /** The timestamp of this packet in microseconds. */
  get microsecondTimestamp() {
    return Math.trunc(be * this.timestamp);
  }
  /** The duration of this packet in microseconds. */
  get microsecondDuration() {
    return Math.trunc(be * this.duration);
  }
  /** Converts this packet to an EncodedVideoChunk for use with the WebCodecs API. */
  toEncodedVideoChunk() {
    if (this.isMetadataOnly)
      throw new TypeError("Metadata-only packets cannot be converted to a video chunk.");
    if (typeof EncodedVideoChunk > "u")
      throw new Error("Your browser does not support EncodedVideoChunk.");
    return new EncodedVideoChunk({
      data: this.data,
      type: this.type,
      timestamp: this.microsecondTimestamp,
      duration: this.microsecondDuration
    });
  }
  /** Converts this packet to an EncodedAudioChunk for use with the WebCodecs API. */
  toEncodedAudioChunk() {
    if (this.isMetadataOnly)
      throw new TypeError("Metadata-only packets cannot be converted to an audio chunk.");
    if (typeof EncodedAudioChunk > "u")
      throw new Error("Your browser does not support EncodedAudioChunk.");
    return new EncodedAudioChunk({
      data: this.data,
      type: this.type,
      timestamp: this.microsecondTimestamp,
      duration: this.microsecondDuration
    });
  }
  /**
   * Creates an EncodedPacket from an EncodedVideoChunk or EncodedAudioChunk. This method is useful for converting
   * chunks from the WebCodecs API to EncodedPackets.
   */
  static fromEncodedChunk(e) {
    if (!(e instanceof EncodedVideoChunk || e instanceof EncodedAudioChunk))
      throw new TypeError("chunk must be an EncodedVideoChunk or EncodedAudioChunk.");
    const i = new Uint8Array(e.byteLength);
    return e.copyTo(i), new bi(
      i,
      e.type,
      e.timestamp / 1e6,
      (e.duration ?? 0) / 1e6
    );
  }
  /** Clones this packet while optionally updating timing information. */
  clone(e) {
    if (e !== void 0 && (typeof e != "object" || e === null))
      throw new TypeError("options, when provided, must be an object.");
    if (e?.timestamp !== void 0 && !Number.isFinite(e.timestamp))
      throw new TypeError("options.timestamp, when provided, must be a number.");
    if (e?.duration !== void 0 && !Number.isFinite(e.duration))
      throw new TypeError("options.duration, when provided, must be a number.");
    return new bi(
      this.data,
      this.type,
      e?.timestamp ?? this.timestamp,
      e?.duration ?? this.duration,
      this.sequenceNumber,
      this.byteLength
    );
  }
}, jo = (t) => {
  let s = t, r = 4096, a = 0, n = 12, o = 0;
  for (s < 0 && (s = -s, a = 128), s += 33, s > 8191 && (s = 8191); (s & r) !== r && n >= 5; )
    r >>= 1, n--;
  return o = s >> n - 4 & 15, ~(a | n - 5 << 4 | o) & 255;
}, qo = (t) => {
  let i = 0, s = 0, r = ~t;
  r & 128 && (r &= -129, i = -1), s = ((r & 240) >> 4) + 5;
  const a = (1 << s | (r & 15) << s - 4 | 1 << s - 5) - 33;
  return i === 0 ? a : -a;
}, Qo = (t) => {
  let i = 2048, s = 0, r = 11, a = 0, n = t;
  for (n < 0 && (n = -n, s = 128), n > 4095 && (n = 4095); (n & i) !== i && r >= 5; )
    i >>= 1, r--;
  return a = n >> (r === 4 ? 1 : r - 4) & 15, (s | r - 4 << 4 | a) ^ 85;
}, Xo = (t) => {
  let e = 0, i = 0, s = t ^ 85;
  s & 128 && (s &= -129, e = -1), i = ((s & 240) >> 4) + 4;
  let r = 0;
  return i !== 4 ? r = 1 << i | (s & 15) << i - 4 | 1 << i - 5 : r = s << 1 | 1, e === 0 ? r : -r;
}, ki = class rt {
  constructor(e, i) {
    if (this._closed = !1, e instanceof ArrayBuffer || ArrayBuffer.isView(e)) {
      if (!i || typeof i != "object")
        throw new TypeError("init must be an object.");
      if (!("format" in i) || typeof i.format != "string")
        throw new TypeError("init.format must be a string.");
      if (!Number.isInteger(i.codedWidth) || i.codedWidth <= 0)
        throw new TypeError("init.codedWidth must be a positive integer.");
      if (!Number.isInteger(i.codedHeight) || i.codedHeight <= 0)
        throw new TypeError("init.codedHeight must be a positive integer.");
      if (i.rotation !== void 0 && ![0, 90, 180, 270].includes(i.rotation))
        throw new TypeError("init.rotation, when provided, must be 0, 90, 180, or 270.");
      if (!Number.isFinite(i.timestamp))
        throw new TypeError("init.timestamp must be a number.");
      if (i.duration !== void 0 && (!Number.isFinite(i.duration) || i.duration < 0))
        throw new TypeError("init.duration, when provided, must be a non-negative number.");
      this._data = te(e).slice(), this.format = i.format, this.codedWidth = i.codedWidth, this.codedHeight = i.codedHeight, this.rotation = i.rotation ?? 0, this.timestamp = i.timestamp, this.duration = i.duration ?? 0, this.colorSpace = new VideoColorSpace(i.colorSpace);
    } else if (typeof VideoFrame < "u" && e instanceof VideoFrame) {
      if (i?.rotation !== void 0 && ![0, 90, 180, 270].includes(i.rotation))
        throw new TypeError("init.rotation, when provided, must be 0, 90, 180, or 270.");
      if (i?.timestamp !== void 0 && !Number.isFinite(i?.timestamp))
        throw new TypeError("init.timestamp, when provided, must be a number.");
      if (i?.duration !== void 0 && (!Number.isFinite(i.duration) || i.duration < 0))
        throw new TypeError("init.duration, when provided, must be a non-negative number.");
      this._data = e, this.format = e.format, this.codedWidth = e.codedWidth, this.codedHeight = e.codedHeight, this.rotation = i?.rotation ?? 0, this.timestamp = i?.timestamp ?? e.timestamp / 1e6, this.duration = i?.duration ?? (e.duration ?? 0) / 1e6, this.colorSpace = e.colorSpace;
    } else if (typeof HTMLImageElement < "u" && e instanceof HTMLImageElement || typeof SVGImageElement < "u" && e instanceof SVGImageElement || typeof ImageBitmap < "u" && e instanceof ImageBitmap || typeof HTMLVideoElement < "u" && e instanceof HTMLVideoElement || typeof HTMLCanvasElement < "u" && e instanceof HTMLCanvasElement || typeof OffscreenCanvas < "u" && e instanceof OffscreenCanvas) {
      if (!i || typeof i != "object")
        throw new TypeError("init must be an object.");
      if (i.rotation !== void 0 && ![0, 90, 180, 270].includes(i.rotation))
        throw new TypeError("init.rotation, when provided, must be 0, 90, 180, or 270.");
      if (!Number.isFinite(i.timestamp))
        throw new TypeError("init.timestamp must be a number.");
      if (i.duration !== void 0 && (!Number.isFinite(i.duration) || i.duration < 0))
        throw new TypeError("init.duration, when provided, must be a non-negative number.");
      if (typeof VideoFrame < "u")
        return new rt(
          new VideoFrame(e, {
            timestamp: Math.trunc(i.timestamp * be),
            duration: Math.trunc((i.duration ?? 0) * be)
          }),
          i
        );
      let s = 0, r = 0;
      if ("naturalWidth" in e ? (s = e.naturalWidth, r = e.naturalHeight) : "videoWidth" in e ? (s = e.videoWidth, r = e.videoHeight) : "width" in e && (s = Number(e.width), r = Number(e.height)), !s || !r)
        throw new TypeError("Could not determine dimensions.");
      const a = new OffscreenCanvas(s, r), n = a.getContext("2d", { alpha: !1, willReadFrequently: !0 });
      f(n), n.drawImage(e, 0, 0), this._data = a, this.format = "RGBX", this.codedWidth = s, this.codedHeight = r, this.rotation = i.rotation ?? 0, this.timestamp = i.timestamp, this.duration = i.duration ?? 0, this.colorSpace = new VideoColorSpace({
        matrix: "rgb",
        primaries: "bt709",
        transfer: "iec61966-2-1",
        fullRange: !0
      });
    } else
      throw new TypeError("Invalid data type: Must be a BufferSource or CanvasImageSource.");
  }
  /** The width of the frame in pixels after rotation. */
  get displayWidth() {
    return this.rotation % 180 === 0 ? this.codedWidth : this.codedHeight;
  }
  /** The height of the frame in pixels after rotation. */
  get displayHeight() {
    return this.rotation % 180 === 0 ? this.codedHeight : this.codedWidth;
  }
  /** The presentation timestamp of the frame in microseconds. */
  get microsecondTimestamp() {
    return Math.trunc(be * this.timestamp);
  }
  /** The duration of the frame in microseconds. */
  get microsecondDuration() {
    return Math.trunc(be * this.duration);
  }
  /** Clones this video sample. */
  clone() {
    if (this._closed)
      throw new Error("VideoSample is closed.");
    return f(this._data !== null), tt(this._data) ? new rt(this._data.clone(), {
      timestamp: this.timestamp,
      duration: this.duration
    }) : this._data instanceof Uint8Array ? new rt(this._data.slice(), {
      format: this.format,
      codedWidth: this.codedWidth,
      codedHeight: this.codedHeight,
      timestamp: this.timestamp,
      duration: this.duration,
      colorSpace: this.colorSpace
    }) : new rt(this._data, {
      format: this.format,
      codedWidth: this.codedWidth,
      codedHeight: this.codedHeight,
      timestamp: this.timestamp,
      duration: this.duration,
      colorSpace: this.colorSpace
    });
  }
  /**
   * Closes this video sample, releasing held resources. Video samples should be closed as soon as they are not
   * needed anymore.
   */
  close() {
    this._closed || (tt(this._data) ? this._data.close() : this._data = null, this._closed = !0);
  }
  /** Returns the number of bytes required to hold this video sample's pixel data. */
  allocationSize() {
    if (this._closed)
      throw new Error("VideoSample is closed.");
    return f(this._data !== null), tt(this._data) ? this._data.allocationSize() : this._data instanceof Uint8Array ? this._data.byteLength : this.codedWidth * this.codedHeight * 4;
  }
  /** Copies this video sample's pixel data to an ArrayBuffer or ArrayBufferView. */
  async copyTo(e) {
    if (!Ut(e))
      throw new TypeError("destination must be an ArrayBuffer or an ArrayBuffer view.");
    if (this._closed)
      throw new Error("VideoSample is closed.");
    if (f(this._data !== null), tt(this._data))
      await this._data.copyTo(e);
    else if (this._data instanceof Uint8Array)
      te(e).set(this._data);
    else {
      const s = this._data.getContext("2d", { alpha: !1 });
      f(s);
      const r = s.getImageData(0, 0, this.codedWidth, this.codedHeight);
      te(e).set(r.data);
    }
  }
  /**
   * Converts this video sample to a VideoFrame for use with the WebCodecs API. The VideoFrame returned by this
   * method *must* be closed separately from this video sample.
   */
  toVideoFrame() {
    if (this._closed)
      throw new Error("VideoSample is closed.");
    return f(this._data !== null), tt(this._data) ? new VideoFrame(this._data, {
      timestamp: this.microsecondTimestamp,
      duration: this.microsecondDuration || void 0
      // Drag 0 duration to undefined, glitches some codecs
    }) : this._data instanceof Uint8Array ? new VideoFrame(this._data, {
      format: this.format,
      codedWidth: this.codedWidth,
      codedHeight: this.codedHeight,
      timestamp: this.microsecondTimestamp,
      duration: this.microsecondDuration,
      colorSpace: this.colorSpace
    }) : new VideoFrame(this._data, {
      timestamp: this.microsecondTimestamp,
      duration: this.microsecondDuration
    });
  }
  /**
   * Draws the video sample to a 2D canvas context. Rotation metadata will be taken into account.
   *
   * @param dx - The x-coordinate in the destination canvas at which to place the top-left corner of the source image.
   * @param dy - The y-coordinate in the destination canvas at which to place the top-left corner of the source image.
   * @param dWidth - The width in pixels with which to draw the image in the destination canvas.
   * @param dHeight - The height in pixels with which to draw the image in the destination canvas.
   */
  draw(e, i, s, r = this.displayWidth, a = this.displayHeight) {
    if (!(typeof CanvasRenderingContext2D < "u" && e instanceof CanvasRenderingContext2D || typeof OffscreenCanvasRenderingContext2D < "u" && e instanceof OffscreenCanvasRenderingContext2D))
      throw new TypeError("context must be a CanvasRenderingContext2D or OffscreenCanvasRenderingContext2D.");
    if (!Number.isFinite(i))
      throw new TypeError("dx must be a number.");
    if (!Number.isFinite(s))
      throw new TypeError("dy must be a number.");
    if (!Number.isFinite(r) || r < 0)
      throw new TypeError("dWidth must be a non-negative number.");
    if (!Number.isFinite(a) || a < 0)
      throw new TypeError("dHeight must be a non-negative number.");
    if (this._closed)
      throw new Error("VideoSample is closed.");
    const n = this.toCanvasImageSource();
    e.save();
    const o = i + r / 2, c = s + a / 2;
    e.translate(o, c), e.rotate(this.rotation * Math.PI / 180);
    const l = this.rotation % 180 === 0 ? 1 : r / a;
    e.scale(1 / l, l), e.drawImage(
      n,
      -r / 2,
      -a / 2,
      r,
      a
    ), e.restore();
  }
  /**
   * Converts this video sample to a CanvasImageSource for drawing to a canvas.
   *
   * You must use the value returned by this method immediately, as any VideoFrame created internally will
   * automatically be closed in the next microtask.
   */
  toCanvasImageSource() {
    if (this._closed)
      throw new Error("VideoSample is closed.");
    if (f(this._data !== null), this._data instanceof Uint8Array) {
      const e = this.toVideoFrame();
      return queueMicrotask(() => e.close()), e;
    } else
      return this._data;
  }
  /** Sets the rotation metadata of this video sample. */
  setRotation(e) {
    if (![0, 90, 180, 270].includes(e))
      throw new TypeError("newRotation must be 0, 90, 180, or 270.");
    this.rotation = e;
  }
  /** Sets the presentation timestamp of this video sample, in seconds. */
  setTimestamp(e) {
    if (!Number.isFinite(e))
      throw new TypeError("newTimestamp must be a number.");
    this.timestamp = e;
  }
  /** Sets the duration of this video sample, in seconds. */
  setDuration(e) {
    if (!Number.isFinite(e) || e < 0)
      throw new TypeError("newDuration must be a non-negative number.");
    this.duration = e;
  }
}, tt = (t) => typeof VideoFrame < "u" && t instanceof VideoFrame, ti = /* @__PURE__ */ new Set(
  ["f32", "f32-planar", "s16", "s16-planar", "s32", "s32-planar", "u8", "u8-planar"]
), Ft = class Si {
  constructor(e) {
    if (this._closed = !1, st(e)) {
      if (e.format === null)
        throw new TypeError("AudioData with null format is not supported.");
      this._data = e, this.format = e.format, this.sampleRate = e.sampleRate, this.numberOfFrames = e.numberOfFrames, this.numberOfChannels = e.numberOfChannels, this.timestamp = e.timestamp / 1e6, this.duration = e.numberOfFrames / e.sampleRate;
    } else {
      if (!e || typeof e != "object")
        throw new TypeError("Invalid AudioDataInit: must be an object.");
      if (!ti.has(e.format))
        throw new TypeError("Invalid AudioDataInit: invalid format.");
      if (!Number.isFinite(e.sampleRate) || e.sampleRate <= 0)
        throw new TypeError("Invalid AudioDataInit: sampleRate must be > 0.");
      if (!Number.isInteger(e.numberOfChannels) || e.numberOfChannels === 0)
        throw new TypeError("Invalid AudioDataInit: numberOfChannels must be an integer > 0.");
      if (!Number.isFinite(e?.timestamp))
        throw new TypeError("init.timestamp must be a number.");
      const i = e.data.byteLength / (it(e.format) * e.numberOfChannels);
      if (!Number.isInteger(i))
        throw new TypeError("Invalid AudioDataInit: data size is not a multiple of frame size.");
      this.format = e.format, this.sampleRate = e.sampleRate, this.numberOfFrames = i, this.numberOfChannels = e.numberOfChannels, this.timestamp = e.timestamp, this.duration = i / e.sampleRate;
      let s;
      if (e.data instanceof ArrayBuffer)
        s = new Uint8Array(e.data);
      else if (ArrayBuffer.isView(e.data))
        s = new Uint8Array(e.data.buffer, e.data.byteOffset, e.data.byteLength);
      else
        throw new TypeError("Invalid AudioDataInit: data is not a BufferSource.");
      const r = this.numberOfFrames * this.numberOfChannels * it(this.format);
      if (s.byteLength < r)
        throw new TypeError("Invalid AudioDataInit: insufficient data size.");
      this._data = s;
    }
  }
  /** The presentation timestamp of the sample in microseconds. */
  get microsecondTimestamp() {
    return Math.trunc(be * this.timestamp);
  }
  /** The duration of the sample in microseconds. */
  get microsecondDuration() {
    return Math.trunc(be * this.duration);
  }
  /** Returns the number of bytes required to hold the audio sample's data as specified by the given options. */
  allocationSize(e) {
    if (!e || typeof e != "object")
      throw new TypeError("options must be an object.");
    if (!Number.isInteger(e.planeIndex) || e.planeIndex < 0)
      throw new TypeError("planeIndex must be a non-negative integer.");
    if (e.format !== void 0 && !ti.has(e.format))
      throw new TypeError("Invalid format.");
    if (e.frameOffset !== void 0 && (!Number.isInteger(e.frameOffset) || e.frameOffset < 0))
      throw new TypeError("frameOffset must be a non-negative integer.");
    if (e.frameCount !== void 0 && (!Number.isInteger(e.frameCount) || e.frameCount < 0))
      throw new TypeError("frameCount must be a non-negative integer.");
    if (this._closed)
      throw new Error("AudioSample is closed.");
    const i = e.format ?? this.format, s = e.frameOffset ?? 0;
    if (s >= this.numberOfFrames)
      throw new RangeError("frameOffset out of range");
    const r = e.frameCount !== void 0 ? e.frameCount : this.numberOfFrames - s;
    if (r > this.numberOfFrames - s)
      throw new RangeError("frameCount out of range");
    const a = it(i), n = bt(i);
    if (n && e.planeIndex >= this.numberOfChannels)
      throw new RangeError("planeIndex out of range");
    if (!n && e.planeIndex !== 0)
      throw new RangeError("planeIndex out of range");
    return (n ? r : r * this.numberOfChannels) * a;
  }
  /** Copies the audio sample's data to an ArrayBuffer or ArrayBufferView as specified by the given options. */
  copyTo(e, i) {
    if (!Ut(e))
      throw new TypeError("destination must be an ArrayBuffer or an ArrayBuffer view.");
    if (!i || typeof i != "object")
      throw new TypeError("options must be an object.");
    if (!Number.isInteger(i.planeIndex) || i.planeIndex < 0)
      throw new TypeError("planeIndex must be a non-negative integer.");
    if (i.format !== void 0 && !ti.has(i.format))
      throw new TypeError("Invalid format.");
    if (i.frameOffset !== void 0 && (!Number.isInteger(i.frameOffset) || i.frameOffset < 0))
      throw new TypeError("frameOffset must be a non-negative integer.");
    if (i.frameCount !== void 0 && (!Number.isInteger(i.frameCount) || i.frameCount < 0))
      throw new TypeError("frameCount must be a non-negative integer.");
    if (this._closed)
      throw new Error("AudioSample is closed.");
    const { planeIndex: s, format: r, frameCount: a, frameOffset: n } = i, o = r ?? this.format;
    if (!o) throw new Error("Destination format not determined");
    const c = this.numberOfFrames, l = this.numberOfChannels, d = n ?? 0;
    if (d >= c)
      throw new RangeError("frameOffset out of range");
    const h = a !== void 0 ? a : c - d;
    if (h > c - d)
      throw new RangeError("frameCount out of range");
    const u = it(o), m = bt(o);
    if (m && s >= l)
      throw new RangeError("planeIndex out of range");
    if (!m && s !== 0)
      throw new RangeError("planeIndex out of range");
    const g = (m ? h : h * l) * u;
    if (e.byteLength < g)
      throw new RangeError("Destination buffer is too small");
    const w = Re(e), b = Go(o);
    if (st(this._data))
      if (m)
        if (o === "f32-planar")
          this._data.copyTo(e, {
            planeIndex: s,
            frameOffset: d,
            frameCount: h,
            format: "f32-planar"
          });
        else {
          const x = new ArrayBuffer(h * 4), S = new Float32Array(x);
          this._data.copyTo(S, {
            planeIndex: s,
            frameOffset: d,
            frameCount: h,
            format: "f32-planar"
          });
          const k = new DataView(x);
          for (let y = 0; y < h; y++) {
            const T = y * u, P = k.getFloat32(y * 4, !0);
            b(w, T, P);
          }
        }
      else {
        const x = l, S = new Float32Array(h);
        for (let k = 0; k < x; k++) {
          this._data.copyTo(S, {
            planeIndex: k,
            frameOffset: d,
            frameCount: h,
            format: "f32-planar"
          });
          for (let y = 0; y < h; y++) {
            const P = (y * x + k) * u;
            b(w, P, S[y]);
          }
        }
      }
    else {
      const x = this._data, S = new DataView(x.buffer, x.byteOffset, x.byteLength), k = this.format, y = Ko(k), T = it(k), P = bt(k);
      for (let F = 0; F < h; F++)
        if (m) {
          const z = F * u;
          let M;
          P ? M = (s * c + (F + d)) * T : M = ((F + d) * l + s) * T;
          const A = y(S, M);
          b(w, z, A);
        } else
          for (let z = 0; z < l; z++) {
            const A = (F * l + z) * u;
            let j;
            P ? j = (z * c + (F + d)) * T : j = ((F + d) * l + z) * T;
            const Ie = y(S, j);
            b(w, A, Ie);
          }
    }
  }
  /** Clones this audio sample. */
  clone() {
    if (this._closed)
      throw new Error("AudioSample is closed.");
    if (st(this._data)) {
      const e = new Si(this._data.clone());
      return e.setTimestamp(this.timestamp), e;
    } else
      return new Si({
        format: this.format,
        sampleRate: this.sampleRate,
        numberOfFrames: this.numberOfFrames,
        numberOfChannels: this.numberOfChannels,
        timestamp: this.timestamp,
        data: this._data
      });
  }
  /**
   * Closes this audio sample, releasing held resources. Audio samples should be closed as soon as they are not
   * needed anymore.
   */
  close() {
    this._closed || (st(this._data) ? this._data.close() : this._data = new Uint8Array(0), this._closed = !0);
  }
  /**
   * Converts this audio sample to an AudioData for use with the WebCodecs API. The AudioData returned by this
   * method *must* be closed separately from this audio sample.
   */
  toAudioData() {
    if (this._closed)
      throw new Error("AudioSample is closed.");
    if (st(this._data)) {
      if (this._data.timestamp === this.microsecondTimestamp)
        return this._data.clone();
      if (bt(this.format)) {
        const e = this.allocationSize({ planeIndex: 0, format: this.format }), i = new ArrayBuffer(e * this.numberOfChannels);
        for (let s = 0; s < this.numberOfChannels; s++)
          this.copyTo(new Uint8Array(i, s * e, e), { planeIndex: s, format: this.format });
        return new AudioData({
          format: this.format,
          sampleRate: this.sampleRate,
          numberOfFrames: this.numberOfFrames,
          numberOfChannels: this.numberOfChannels,
          timestamp: this.microsecondTimestamp,
          data: i
        });
      } else {
        const e = new ArrayBuffer(this.allocationSize({ planeIndex: 0, format: this.format }));
        return this.copyTo(new DataView(e), { planeIndex: 0, format: this.format }), new AudioData({
          format: this.format,
          sampleRate: this.sampleRate,
          numberOfFrames: this.numberOfFrames,
          numberOfChannels: this.numberOfChannels,
          timestamp: this.microsecondTimestamp,
          data: e
        });
      }
    } else
      return new AudioData({
        format: this.format,
        sampleRate: this.sampleRate,
        numberOfFrames: this.numberOfFrames,
        numberOfChannels: this.numberOfChannels,
        timestamp: this.microsecondTimestamp,
        data: this._data
      });
  }
  /** Convert this audio sample to an AudioBuffer for use with the Web Audio API. */
  toAudioBuffer() {
    if (this._closed)
      throw new Error("AudioSample is closed.");
    const e = new AudioBuffer({
      numberOfChannels: this.numberOfChannels,
      length: this.numberOfFrames,
      sampleRate: this.sampleRate
    }), i = new Float32Array(this.allocationSize({ planeIndex: 0, format: "f32-planar" }) / 4);
    for (let s = 0; s < this.numberOfChannels; s++)
      this.copyTo(i, { planeIndex: s, format: "f32-planar" }), e.copyToChannel(i, s);
    return e;
  }
  /** Sets the presentation timestamp of this audio sample, in seconds. */
  setTimestamp(e) {
    if (!Number.isFinite(e))
      throw new TypeError("newTimestamp must be a number.");
    this.timestamp = e;
  }
}, it = (t) => {
  switch (t) {
    case "u8":
    case "u8-planar":
      return 1;
    case "s16":
    case "s16-planar":
      return 2;
    case "s32":
    case "s32-planar":
      return 4;
    case "f32":
    case "f32-planar":
      return 4;
    default:
      throw new Error("Unknown AudioSampleFormat");
  }
}, bt = (t) => {
  switch (t) {
    case "u8-planar":
    case "s16-planar":
    case "s32-planar":
    case "f32-planar":
      return !0;
    default:
      return !1;
  }
}, Ko = (t) => {
  switch (t) {
    case "u8":
    case "u8-planar":
      return (e, i) => (e.getUint8(i) - 128) / 128;
    case "s16":
    case "s16-planar":
      return (e, i) => e.getInt16(i, !0) / 32768;
    case "s32":
    case "s32-planar":
      return (e, i) => e.getInt32(i, !0) / 2147483648;
    case "f32":
    case "f32-planar":
      return (e, i) => e.getFloat32(i, !0);
  }
}, Go = (t) => {
  switch (t) {
    case "u8":
    case "u8-planar":
      return (e, i, s) => e.setUint8(i, J((s + 1) * 127.5, 0, 255));
    case "s16":
    case "s16-planar":
      return (e, i, s) => e.setInt16(i, J(Math.round(s * 32767), -32768, 32767), !0);
    case "s32":
    case "s32-planar":
      return (e, i, s) => e.setInt32(i, J(Math.round(s * 2147483647), -2147483648, 2147483647), !0);
    case "f32":
    case "f32-planar":
      return (e, i, s) => e.setFloat32(i, s, !0);
  }
}, st = (t) => typeof AudioData < "u" && t instanceof AudioData, Ne = (t) => {
  if (!t || typeof t != "object")
    throw new TypeError("options must be an object.");
  if (t.metadataOnly !== void 0 && typeof t.metadataOnly != "boolean")
    throw new TypeError("options.metadataOnly, when defined, must be a boolean.");
}, ue = (t) => {
  if (typeof t != "number" || Number.isNaN(t))
    throw new TypeError("timestamp must be a number.");
}, Li = class {
  constructor(t) {
    if (!(t instanceof Hi))
      throw new TypeError("track must be an InputTrack.");
    this._track = t;
  }
  /**
   * Retrieves the track's first packet (in decode order), or null if it has no packets. The first packet is very
   * likely to be a key packet.
   */
  getFirstPacket(t = {}) {
    return Ne(t), this._track._backing.getFirstPacket(t);
  }
  /**
   * Retrieves the packet corresponding to the given timestamp, in seconds. More specifically, returns the last packet
   * (in presentation order) with a start timestamp less than or equal to the given timestamp. This method can be
   * used to retrieve a track's last packet using `getPacket(Infinity)`. The method returns null if the timestamp
   * is before the first packet in the track.
   *
   * @param timestamp - The timestamp used for retrieval, in seconds.
   */
  getPacket(t, e = {}) {
    return ue(t), Ne(e), this._track._backing.getPacket(t, e);
  }
  /**
   * Retrieves the packet following the given packet (in decode order), or null if the given packet is the
   * last packet.
   */
  getNextPacket(t, e = {}) {
    if (!(t instanceof G))
      throw new TypeError("packet must be an EncodedPacket.");
    return Ne(e), this._track._backing.getNextPacket(t, e);
  }
  /**
   * Retrieves the key packet corresponding to the given timestamp, in seconds. More specifically, returns the last
   * key packet (in presentation order) with a start timestamp less than or equal to the given timestamp. A key packet
   * is a packet that doesn't require previous packets to be decoded. This method can be used to retrieve a track's
   * last key packet using `getKeyPacket(Infinity)`. The method returns null if the timestamp is before the first
   * key packet in the track.
   *
   * @param timestamp - The timestamp used for retrieval, in seconds.
   */
  getKeyPacket(t, e = {}) {
    return ue(t), Ne(e), this._track._backing.getKeyPacket(t, e);
  }
  /**
   * Retrieves the key packet following the given packet (in decode order), or null if the given packet is the last
   * key packet.
   */
  getNextKeyPacket(t, e = {}) {
    if (!(t instanceof G))
      throw new TypeError("packet must be an EncodedPacket.");
    return Ne(e), this._track._backing.getNextKeyPacket(t, e);
  }
  /**
   * Creates an async iterator that yields the packets in this track in decode order. To enable fast iteration, this
   * method will intelligently preload packets based on the speed of the consumer.
   *
   * @param startPacket - (optional) The packet from which iteration should begin. This packet will also be yielded.
   * @param endTimestamp - (optional) The timestamp at which iteration should end. This packet will _not_ be yielded.
   */
  packets(t, e, i = {}) {
    if (t !== void 0 && !(t instanceof G))
      throw new TypeError("startPacket must be an EncodedPacket.");
    if (t !== void 0 && t.isMetadataOnly && !i?.metadataOnly)
      throw new TypeError("startPacket can only be metadata-only if options.metadataOnly is enabled.");
    if (e !== void 0 && !(e instanceof G))
      throw new TypeError("endPacket must be an EncodedPacket.");
    Ne(i);
    const s = [];
    let { promise: r, resolve: a } = Z(), { promise: n, resolve: o } = Z(), c = !1, l = !1, d = null;
    const h = [], u = () => Math.max(2, h.length);
    return (async () => {
      let m = t ?? await this.getFirstPacket(i);
      for (; m && !l && !(e && m.sequenceNumber >= e?.sequenceNumber); ) {
        if (s.length > u()) {
          ({ promise: n, resolve: o } = Z()), await n;
          continue;
        }
        s.push(m), a(), { promise: r, resolve: a } = Z(), m = await this.getNextPacket(m, i);
      }
      c = !0, a();
    })().catch((m) => {
      d || (d = m, a());
    }), {
      async next() {
        for (; ; ) {
          if (l)
            return { value: void 0, done: !0 };
          if (d)
            throw d;
          if (s.length > 0) {
            const m = s.shift(), p = performance.now();
            for (h.push(p); h.length > 0 && p - h[0] >= 1e3; )
              h.shift();
            return o(), { value: m, done: !1 };
          } else {
            if (c)
              return { value: void 0, done: !0 };
            await r;
          }
        }
      },
      async return() {
        return l = !0, o(), a(), { value: void 0, done: !0 };
      },
      async throw(m) {
        throw m;
      },
      [Symbol.asyncIterator]() {
        return this;
      }
    };
  }
}, Wi = class {
  constructor(t, e) {
    this.onSample = t, this.onError = e;
  }
}, Lr = class {
  /** @internal */
  mediaSamplesInRange(t = 0, e = 1 / 0) {
    ue(t), ue(e);
    const i = [];
    let s = !1, r = null, { promise: a, resolve: n } = Z(), { promise: o, resolve: c } = Z(), l = !1, d = !1, h = !1, u = null;
    return (async () => {
      const m = new Error(), p = await this._createDecoder((k) => {
        if (c(), k.timestamp >= e && (d = !0), d) {
          k.close();
          return;
        }
        r && (k.timestamp > t ? (i.push(r), s = !0) : r.close()), k.timestamp >= t && (i.push(k), s = !0), r = s ? null : k, i.length > 0 && (n(), { promise: a, resolve: n } = Z());
      }, (k) => {
        u || (k.stack = m.stack, u = k, n());
      }), g = this._createPacketSink(), w = await g.getKeyPacket(t) ?? await g.getFirstPacket();
      if (!w)
        return;
      let b = w, x;
      if (e < 1 / 0) {
        const k = await g.getPacket(e), y = k ? k.type === "key" && k.timestamp === e ? k : await g.getNextKeyPacket(k) : null;
        y && (x = y);
      }
      const S = g.packets(w, x);
      for (await S.next(); b && !d; ) {
        const k = ys(i.length);
        if (i.length + p.getDecodeQueueSize() > k) {
          ({ promise: o, resolve: c } = Z()), await o;
          continue;
        }
        p.decode(b);
        const y = await S.next();
        if (y.done)
          break;
        b = y.value;
      }
      await S.return(), h || await p.flush(), p.close(), !s && r && i.push(r), l = !0, n();
    })().catch((m) => {
      u || (u = m, n());
    }), {
      async next() {
        for (; ; ) {
          if (h)
            return { value: void 0, done: !0 };
          if (u)
            throw u;
          if (i.length > 0) {
            const m = i.shift();
            return c(), { value: m, done: !1 };
          } else if (!l)
            await a;
          else
            return { value: void 0, done: !0 };
        }
      },
      async return() {
        h = !0, d = !0, c(), n(), r?.close();
        for (const m of i)
          m.close();
        return { value: void 0, done: !0 };
      },
      async throw(m) {
        throw m;
      },
      [Symbol.asyncIterator]() {
        return this;
      }
    };
  }
  /** @internal */
  mediaSamplesAtTimestamps(t) {
    Ma(t);
    const e = Oa(t), i = [], s = [];
    let { promise: r, resolve: a } = Z(), { promise: n, resolve: o } = Z(), c = !1, l = !1, d = null;
    const h = (u) => {
      s.push(u), a(), { promise: r, resolve: a } = Z();
    };
    return (async () => {
      const u = new Error(), m = await this._createDecoder((k) => {
        if (o(), l) {
          k.close();
          return;
        }
        let y = 0;
        for (; i.length > 0 && k.timestamp - i[0] > -1e-10; )
          y++, i.shift();
        if (y > 0)
          for (let T = 0; T < y; T++)
            h(T < y - 1 ? k.clone() : k);
        else
          k.close();
      }, (k) => {
        d || (k.stack = u.stack, d = k, a());
      }), p = this._createPacketSink();
      let g = null, w = null, b = -1;
      const x = async () => {
        f(w);
        let k = w;
        for (m.decode(k); k.sequenceNumber < b; ) {
          const y = ys(s.length);
          for (; s.length + m.getDecodeQueueSize() > y && !l; )
            ({ promise: n, resolve: o } = Z()), await n;
          if (l)
            break;
          const T = await p.getNextPacket(k);
          f(T), k = T, m.decode(T);
        }
        b = -1;
      }, S = async () => {
        await m.flush();
        for (let k = 0; k < i.length; k++)
          h(null);
        i.length = 0;
      };
      for await (const k of e) {
        if (ue(k), l)
          break;
        const y = await p.getPacket(k), T = y && await p.getKeyPacket(k);
        if (!T) {
          b !== -1 && (await x(), await S()), h(null), g = null;
          continue;
        }
        g && (T.sequenceNumber !== w.sequenceNumber || y.timestamp < g.timestamp) && (await x(), y.timestamp < g.timestamp && await S()), i.push(y.timestamp), b = Math.max(y.sequenceNumber, b), g = y, w = T;
      }
      l || (b !== -1 && await x(), await S()), m.close(), c = !0, a();
    })().catch((u) => {
      d || (d = u, a());
    }), {
      async next() {
        for (; ; ) {
          if (l)
            return { value: void 0, done: !0 };
          if (d)
            throw d;
          if (s.length > 0) {
            const u = s.shift();
            return f(u !== void 0), o(), { value: u, done: !1 };
          } else if (!c)
            await r;
          else
            return { value: void 0, done: !0 };
        }
      },
      async return() {
        l = !0, o(), a();
        for (const u of s)
          u?.close();
        return { value: void 0, done: !0 };
      },
      async throw(u) {
        throw u;
      },
      [Symbol.asyncIterator]() {
        return this;
      }
    };
  }
}, ys = (t) => t === 0 ? 40 : 8, Yo = class extends Wi {
  constructor(t, e, i, s, r, a) {
    super(t, e), this.rotation = r, this.timeResolution = a, this.decoder = null, this.customDecoder = null, this.customDecoderCallSerializer = new Dt(), this.customDecoderQueueSize = 0, this.sampleQueue = [];
    const n = (c) => {
      if (this.sampleQueue.length > 0 && c.timestamp >= H(this.sampleQueue).timestamp) {
        for (const d of this.sampleQueue)
          this.finalizeAndEmitSample(d);
        this.sampleQueue.length = 0;
      }
      const l = N(
        this.sampleQueue,
        c.timestamp,
        (d) => d.timestamp
      );
      this.sampleQueue.splice(l + 1, 0, c);
    }, o = rr.find((c) => c.supports(i, s));
    o ? (this.customDecoder = new o(), this.customDecoder.codec = i, this.customDecoder.config = s, this.customDecoder.onSample = (c) => {
      if (!(c instanceof ki))
        throw new TypeError("The argument passed to onSample must be a VideoSample.");
      n(c);
    }, this.customDecoderCallSerializer.call(() => this.customDecoder.init())) : (this.decoder = new VideoDecoder({
      output: (c) => n(new ki(c)),
      error: e
    }), this.decoder.configure(s));
  }
  finalizeAndEmitSample(t) {
    t.setTimestamp(Math.round(t.timestamp * this.timeResolution) / this.timeResolution), t.setDuration(Math.round(t.duration * this.timeResolution) / this.timeResolution), this.onSample(t);
  }
  getDecodeQueueSize() {
    return this.customDecoder ? this.customDecoderQueueSize : (f(this.decoder), this.decoder.decodeQueueSize);
  }
  decode(t) {
    this.customDecoder ? (this.customDecoderQueueSize++, this.customDecoderCallSerializer.call(() => this.customDecoder.decode(t)).then(() => this.customDecoderQueueSize--)) : (f(this.decoder), this.decoder.decode(t.toEncodedVideoChunk()));
  }
  async flush() {
    this.customDecoder ? await this.customDecoderCallSerializer.call(() => this.customDecoder.flush()) : (f(this.decoder), await this.decoder.flush());
    for (const t of this.sampleQueue)
      this.finalizeAndEmitSample(t);
    this.sampleQueue.length = 0;
  }
  close() {
    this.customDecoder ? this.customDecoderCallSerializer.call(() => this.customDecoder.close()) : (f(this.decoder), this.decoder.close());
    for (const t of this.sampleQueue)
      t.close();
    this.sampleQueue.length = 0;
  }
}, Wr = class extends Lr {
  constructor(t) {
    if (!(t instanceof pt))
      throw new TypeError("videoTrack must be an InputVideoTrack.");
    super(), this._videoTrack = t;
  }
  /** @internal */
  async _createDecoder(t, e) {
    if (!await this._videoTrack.canDecode())
      throw new Error(
        "This video track cannot be decoded by this browser. Make sure to check decodability before using a track."
      );
    const i = this._videoTrack.codec, s = this._videoTrack.rotation, r = await this._videoTrack.getDecoderConfig(), a = this._videoTrack.timeResolution;
    return f(i && r), new Yo(t, e, i, r, s, a);
  }
  /** @internal */
  _createPacketSink() {
    return new Li(this._videoTrack);
  }
  /**
   * Retrieves the video sample (frame) corresponding to the given timestamp, in seconds. More specifically, returns
   * the last video sample (in presentation order) with a start timestamp less than or equal to the given timestamp.
   * Returns null if the timestamp is before the track's first timestamp.
   *
   * @param timestamp - The timestamp used for retrieval, in seconds.
   */
  async getSample(t) {
    ue(t);
    for await (const e of this.mediaSamplesAtTimestamps([t]))
      return e;
    throw new Error("Internal error: Iterator returned nothing.");
  }
  /**
   * Creates an async iterator that yields the video samples (frames) of this track in presentation order. This method
   * will intelligently pre-decode a few frames ahead to enable fast iteration.
   *
   * @param startTimestamp - The timestamp in seconds at which to start yielding samples (inclusive).
   * @param endTimestamp - The timestamp in seconds at which to stop yielding samples (exclusive).
   */
  samples(t = 0, e = 1 / 0) {
    return this.mediaSamplesInRange(t, e);
  }
  /**
   * Creates an async iterator that yields a video sample (frame) for each timestamp in the argument. This method
   * uses an optimized decoding pipeline if these timestamps are monotonically sorted, decoding each packet at most
   * once, and is therefore more efficient than manually getting the sample for every timestamp. The iterator may
   * yield null if no frame is available for a given timestamp.
   *
   * @param timestamps - An iterable or async iterable of timestamps in seconds.
   */
  samplesAtTimestamps(t) {
    return this.mediaSamplesAtTimestamps(t);
  }
}, Jo = class {
  constructor(t, e = {}) {
    if (this._nextCanvasIndex = 0, !(t instanceof pt))
      throw new TypeError("videoTrack must be an InputVideoTrack.");
    if (e && typeof e != "object")
      throw new TypeError("options must be an object.");
    if (e.width !== void 0 && (!Number.isInteger(e.width) || e.width <= 0))
      throw new TypeError("options.width, when defined, must be a positive integer.");
    if (e.height !== void 0 && (!Number.isInteger(e.height) || e.height <= 0))
      throw new TypeError("options.height, when defined, must be a positive integer.");
    if (e.fit !== void 0 && !["fill", "contain", "cover"].includes(e.fit))
      throw new TypeError('options.fit, when provided, must be one of "fill", "contain", or "cover".');
    if (e.width !== void 0 && e.height !== void 0 && e.fit === void 0)
      throw new TypeError(
        "When both options.width and options.height are provided, options.fit must also be provided."
      );
    if (e.rotation !== void 0 && ![0, 90, 180, 270].includes(e.rotation))
      throw new TypeError("options.rotation, when provided, must be 0, 90, 180 or 270.");
    if (e.poolSize !== void 0 && (typeof e.poolSize != "number" || !Number.isInteger(e.poolSize) || e.poolSize < 0))
      throw new TypeError("poolSize must be a non-negative integer.");
    const i = e.rotation ?? t.rotation;
    let [s, r] = i % 180 === 0 ? [t.codedWidth, t.codedHeight] : [t.codedHeight, t.codedWidth];
    const a = s / r;
    e.width !== void 0 && e.height === void 0 ? (s = e.width, r = Math.round(s / a)) : e.width === void 0 && e.height !== void 0 ? (r = e.height, s = Math.round(r * a)) : e.width !== void 0 && e.height !== void 0 && (s = e.width, r = e.height), this._videoTrack = t, this._width = s, this._height = r, this._rotation = i, this._fit = e.fit ?? "fill", this._videoSampleSink = new Wr(t), this._canvasPool = Array.from({ length: e.poolSize ?? 0 }, () => null);
  }
  /** @internal */
  _videoSampleToWrappedCanvas(t) {
    let e = this._canvasPool[this._nextCanvasIndex];
    e || (typeof OffscreenCanvas < "u" ? e = new OffscreenCanvas(this._width, this._height) : (e = document.createElement("canvas"), e.width = this._width, e.height = this._height), this._canvasPool.length > 0 && (this._canvasPool[this._nextCanvasIndex] = e)), this._canvasPool.length > 0 && (this._nextCanvasIndex = (this._nextCanvasIndex + 1) % this._canvasPool.length);
    const i = e.getContext("2d", { alpha: !1 });
    f(i), i.resetTransform();
    let s, r, a, n;
    if (this._fit === "fill")
      s = 0, r = 0, a = this._width, n = this._height;
    else {
      const [l, d] = this._rotation % 180 === 0 ? [t.codedWidth, t.codedHeight] : [t.codedHeight, t.codedWidth], h = this._fit === "contain" ? Math.min(this._width / l, this._height / d) : Math.max(this._width / l, this._height / d);
      a = l * h, n = d * h, s = (this._width - a) / 2, r = (this._height - n) / 2;
    }
    const o = this._rotation % 180 === 0 ? 1 : a / n;
    i.translate(this._width / 2, this._height / 2), i.rotate(this._rotation * Math.PI / 180), i.scale(1 / o, o), i.translate(-this._width / 2, -this._height / 2), i.drawImage(t.toCanvasImageSource(), s, r, a, n);
    const c = {
      canvas: e,
      timestamp: t.timestamp,
      duration: t.duration
    };
    return t.close(), c;
  }
  /**
   * Retrieves a canvas with the video frame corresponding to the given timestamp, in seconds. More specifically,
   * returns the last video frame (in presentation order) with a start timestamp less than or equal to the given
   * timestamp. Returns null if the timestamp is before the track's first timestamp.
   *
   * @param timestamp - The timestamp used for retrieval, in seconds.
   */
  async getCanvas(t) {
    ue(t);
    const e = await this._videoSampleSink.getSample(t);
    return e && this._videoSampleToWrappedCanvas(e);
  }
  /**
   * Creates an async iterator that yields canvases with the video frames of this track in presentation order. This
   * method will intelligently pre-decode a few frames ahead to enable fast iteration.
   *
   * @param startTimestamp - The timestamp in seconds at which to start yielding canvases (inclusive).
   * @param endTimestamp - The timestamp in seconds at which to stop yielding canvases (exclusive).
   */
  canvases(t = 0, e = 1 / 0) {
    return _t(
      this._videoSampleSink.samples(t, e),
      (i) => this._videoSampleToWrappedCanvas(i)
    );
  }
  /**
   * Creates an async iterator that yields a canvas for each timestamp in the argument. This method uses an optimized
   * decoding pipeline if these timestamps are monotonically sorted, decoding each packet at most once, and is
   * therefore more efficient than manually getting the canvas for every timestamp. The iterator may yield null if
   * no frame is available for a given timestamp.
   *
   * @param timestamps - An iterable or async iterable of timestamps in seconds.
   */
  canvasesAtTimestamps(t) {
    return _t(
      this._videoSampleSink.samplesAtTimestamps(t),
      (e) => e && this._videoSampleToWrappedCanvas(e)
    );
  }
}, Zo = class extends Wi {
  constructor(t, e, i, s) {
    super(t, e), this.decoder = null, this.customDecoder = null, this.customDecoderCallSerializer = new Dt(), this.customDecoderQueueSize = 0;
    const r = (n) => {
      const o = s.sampleRate;
      n.setTimestamp(Math.round(n.timestamp * o) / o), t(n);
    }, a = ar.find((n) => n.supports(i, s));
    a ? (this.customDecoder = new a(), this.customDecoder.codec = i, this.customDecoder.config = s, this.customDecoder.onSample = (n) => {
      if (!(n instanceof Ft))
        throw new TypeError("The argument passed to onSample must be an AudioSample.");
      r(n);
    }, this.customDecoderCallSerializer.call(() => this.customDecoder.init())) : (this.decoder = new AudioDecoder({
      output: (n) => r(new Ft(n)),
      error: e
    }), this.decoder.configure(s));
  }
  getDecodeQueueSize() {
    return this.customDecoder ? this.customDecoderQueueSize : (f(this.decoder), this.decoder.decodeQueueSize);
  }
  decode(t) {
    this.customDecoder ? (this.customDecoderQueueSize++, this.customDecoderCallSerializer.call(() => this.customDecoder.decode(t)).then(() => this.customDecoderQueueSize--)) : (f(this.decoder), this.decoder.decode(t.toEncodedAudioChunk()));
  }
  flush() {
    return this.customDecoder ? this.customDecoderCallSerializer.call(() => this.customDecoder.flush()) : (f(this.decoder), this.decoder.flush());
  }
  close() {
    this.customDecoder ? this.customDecoderCallSerializer.call(() => this.customDecoder.close()) : (f(this.decoder), this.decoder.close());
  }
}, ec = class extends Wi {
  constructor(t, e, i) {
    super(t, e), this.decoderConfig = i, this.currentTimestamp = null, f(K.includes(i.codec)), this.codec = i.codec;
    const { dataType: s, sampleSize: r, littleEndian: a } = Be(this.codec);
    switch (this.inputSampleSize = r, r) {
      case 1:
        s === "unsigned" ? this.readInputValue = (n, o) => n.getUint8(o) - 2 ** 7 : s === "signed" ? this.readInputValue = (n, o) => n.getInt8(o) : s === "ulaw" ? this.readInputValue = (n, o) => qo(n.getUint8(o)) : s === "alaw" ? this.readInputValue = (n, o) => Xo(n.getUint8(o)) : f(!1);
        break;
      case 2:
        s === "unsigned" ? this.readInputValue = (n, o) => n.getUint16(o, a) - 2 ** 15 : s === "signed" ? this.readInputValue = (n, o) => n.getInt16(o, a) : f(!1);
        break;
      case 3:
        s === "unsigned" ? this.readInputValue = (n, o) => tr(n, o, a) - 2 ** 23 : s === "signed" ? this.readInputValue = (n, o) => Aa(n, o, a) : f(!1);
        break;
      case 4:
        s === "unsigned" ? this.readInputValue = (n, o) => n.getUint32(o, a) - 2 ** 31 : s === "signed" ? this.readInputValue = (n, o) => n.getInt32(o, a) : s === "float" ? this.readInputValue = (n, o) => n.getFloat32(o, a) : f(!1);
        break;
    }
    switch (r) {
      case 1:
        s === "ulaw" || s === "alaw" ? (this.outputSampleSize = 2, this.outputFormat = "s16", this.writeOutputValue = (n, o, c) => n.setInt16(o, c, !0)) : (this.outputSampleSize = 1, this.outputFormat = "u8", this.writeOutputValue = (n, o, c) => n.setUint8(o, c + 2 ** 7));
        break;
      case 2:
        this.outputSampleSize = 2, this.outputFormat = "s16", this.writeOutputValue = (n, o, c) => n.setInt16(o, c, !0);
        break;
      case 3:
        this.outputSampleSize = 4, this.outputFormat = "s32", this.writeOutputValue = (n, o, c) => n.setInt32(o, c << 8, !0);
        break;
      case 4:
        this.outputSampleSize = 4, s === "float" ? (this.outputFormat = "f32", this.writeOutputValue = (n, o, c) => n.setFloat32(o, c, !0)) : (this.outputFormat = "s32", this.writeOutputValue = (n, o, c) => n.setInt32(o, c, !0));
        break;
    }
  }
  getDecodeQueueSize() {
    return 0;
  }
  decode(t) {
    const e = Re(t.data), i = t.byteLength / this.decoderConfig.numberOfChannels / this.inputSampleSize, s = i * this.decoderConfig.numberOfChannels * this.outputSampleSize, r = new ArrayBuffer(s), a = new DataView(r);
    for (let l = 0; l < i * this.decoderConfig.numberOfChannels; l++) {
      const d = l * this.inputSampleSize, h = l * this.outputSampleSize, u = this.readInputValue(e, d);
      this.writeOutputValue(a, h, u);
    }
    const n = i / this.decoderConfig.sampleRate;
    (this.currentTimestamp === null || Math.abs(t.timestamp - this.currentTimestamp) >= n) && (this.currentTimestamp = t.timestamp);
    const o = this.currentTimestamp;
    this.currentTimestamp += n;
    const c = new Ft({
      format: this.outputFormat,
      data: r,
      numberOfChannels: this.decoderConfig.numberOfChannels,
      sampleRate: this.decoderConfig.sampleRate,
      numberOfFrames: i,
      timestamp: o
    });
    this.onSample(c);
  }
  async flush() {
  }
  close() {
  }
}, tc = class extends Lr {
  constructor(t) {
    if (!(t instanceof xe))
      throw new TypeError("audioTrack must be an InputAudioTrack.");
    super(), this._audioTrack = t;
  }
  /** @internal */
  async _createDecoder(t, e) {
    if (!await this._audioTrack.canDecode())
      throw new Error(
        "This audio track cannot be decoded by this browser. Make sure to check decodability before using a track."
      );
    const i = this._audioTrack.codec, s = await this._audioTrack.getDecoderConfig();
    return f(i && s), K.includes(s.codec) ? new ec(t, e, s) : new Zo(t, e, i, s);
  }
  /** @internal */
  _createPacketSink() {
    return new Li(this._audioTrack);
  }
  /**
   * Retrieves the audio sample corresponding to the given timestamp, in seconds. More specifically, returns
   * the last audio sample (in presentation order) with a start timestamp less than or equal to the given timestamp.
   * Returns null if the timestamp is before the track's first timestamp.
   *
   * @param timestamp - The timestamp used for retrieval, in seconds.
   */
  async getSample(t) {
    ue(t);
    for await (const e of this.mediaSamplesAtTimestamps([t]))
      return e;
    throw new Error("Internal error: Iterator returned nothing.");
  }
  /**
   * Creates an async iterator that yields the audio samples of this track in presentation order. This method
   * will intelligently pre-decode a few samples ahead to enable fast iteration.
   *
   * @param startTimestamp - The timestamp in seconds at which to start yielding samples (inclusive).
   * @param endTimestamp - The timestamp in seconds at which to stop yielding samples (exclusive).
   */
  samples(t = 0, e = 1 / 0) {
    return this.mediaSamplesInRange(t, e);
  }
  /**
   * Creates an async iterator that yields an audio sample for each timestamp in the argument. This method
   * uses an optimized decoding pipeline if these timestamps are monotonically sorted, decoding each packet at most
   * once, and is therefore more efficient than manually getting the sample for every timestamp. The iterator may
   * yield null if no sample is available for a given timestamp.
   *
   * @param timestamps - An iterable or async iterable of timestamps in seconds.
   */
  samplesAtTimestamps(t) {
    return this.mediaSamplesAtTimestamps(t);
  }
}, ic = class {
  constructor(t) {
    if (!(t instanceof xe))
      throw new TypeError("audioTrack must be an InputAudioTrack.");
    this._audioSampleSink = new tc(t);
  }
  /** @internal */
  _audioSampleToWrappedArrayBuffer(t) {
    return {
      buffer: t.toAudioBuffer(),
      timestamp: t.timestamp,
      duration: t.duration
    };
  }
  /**
   * Retrieves the audio buffer corresponding to the given timestamp, in seconds. More specifically, returns
   * the last audio buffer (in presentation order) with a start timestamp less than or equal to the given timestamp.
   * Returns null if the timestamp is before the track's first timestamp.
   *
   * @param timestamp - The timestamp used for retrieval, in seconds.
   */
  async getBuffer(t) {
    ue(t);
    const e = await this._audioSampleSink.getSample(t);
    return e && this._audioSampleToWrappedArrayBuffer(e);
  }
  /**
   * Creates an async iterator that yields audio buffers of this track in presentation order. This method
   * will intelligently pre-decode a few buffers ahead to enable fast iteration.
   *
   * @param startTimestamp - The timestamp in seconds at which to start yielding buffers (inclusive).
   * @param endTimestamp - The timestamp in seconds at which to stop yielding buffers (exclusive).
   */
  buffers(t = 0, e = 1 / 0) {
    return _t(
      this._audioSampleSink.samples(t, e),
      (i) => this._audioSampleToWrappedArrayBuffer(i)
    );
  }
  /**
   * Creates an async iterator that yields an audio buffer for each timestamp in the argument. This method
   * uses an optimized decoding pipeline if these timestamps are monotonically sorted, decoding each packet at most
   * once, and is therefore more efficient than manually getting the buffer for every timestamp. The iterator may
   * yield null if no buffer is available for a given timestamp.
   *
   * @param timestamps - An iterable or async iterable of timestamps in seconds.
   */
  buffersAtTimestamps(t) {
    return _t(
      this._audioSampleSink.samplesAtTimestamps(t),
      (e) => e && this._audioSampleToWrappedArrayBuffer(e)
    );
  }
}, Hi = class {
  /** @internal */
  constructor(t) {
    this._backing = t;
  }
  /** Returns true iff this track is a video track. */
  isVideoTrack() {
    return this instanceof pt;
  }
  /** Returns true iff this track is an audio track. */
  isAudioTrack() {
    return this instanceof xe;
  }
  /** The unique ID of this track in the input file. */
  get id() {
    return this._backing.getId();
  }
  /** The ISO 639-2/T language code for this track. If the language is unknown, this field is 'und' (undetermined). */
  get languageCode() {
    return this._backing.getLanguageCode();
  }
  /**
   * A positive number x such that all timestamps and durations of all packets of this track are
   * integer multiples of 1/x.
   */
  get timeResolution() {
    return this._backing.getTimeResolution();
  }
  /**
   * Returns the start timestamp of the first packet of this track, in seconds. While often near zero, this value
   * may be positive or even negative. A negative starting timestamp means the track's timing has been offset. Samples
   * with a negative timestamp should not be presented.
   */
  getFirstTimestamp() {
    return this._backing.getFirstTimestamp();
  }
  /** Returns the end timestamp of the last packet of this track, in seconds. */
  computeDuration() {
    return this._backing.computeDuration();
  }
  /** Computes aggregate packet statistics for this track, such as average packet rate or bitrate. */
  async computePacketStats() {
    const t = new Li(this);
    let e = 1 / 0, i = -1 / 0, s = 0, r = 0;
    for await (const a of t.packets(void 0, void 0, { metadataOnly: !0 }))
      e = Math.min(e, a.timestamp), i = Math.max(i, a.timestamp + a.duration), s++, r += a.byteLength;
    return {
      packetCount: s,
      averagePacketRate: s ? Number((s / (i - e)).toPrecision(16)) : 0,
      averageBitrate: s ? Number((8 * r / (i - e)).toPrecision(16)) : 0
    };
  }
}, pt = class extends Hi {
  /** @internal */
  constructor(t) {
    super(t), this._backing = t;
  }
  get type() {
    return "video";
  }
  get codec() {
    return this._backing.getCodec();
  }
  /** The width in pixels of the track's coded samples, before any transformations or rotations. */
  get codedWidth() {
    return this._backing.getCodedWidth();
  }
  /** The height in pixels of the track's coded samples, before any transformations or rotations. */
  get codedHeight() {
    return this._backing.getCodedHeight();
  }
  /** The angle in degrees by which the track's frames should be rotated (clockwise). */
  get rotation() {
    return this._backing.getRotation();
  }
  /** The width in pixels of the track's frames after rotation. */
  get displayWidth() {
    return this._backing.getRotation() % 180 === 0 ? this._backing.getCodedWidth() : this._backing.getCodedHeight();
  }
  /** The height in pixels of the track's frames after rotation. */
  get displayHeight() {
    return this._backing.getRotation() % 180 === 0 ? this._backing.getCodedHeight() : this._backing.getCodedWidth();
  }
  /** Returns the color space of the track's samples. */
  getColorSpace() {
    return this._backing.getColorSpace();
  }
  /** If this method returns true, the track's samples use a high dynamic range (HDR). */
  async hasHighDynamicRange() {
    const t = await this._backing.getColorSpace();
    return t.primaries === "bt2020" || t.primaries === "smpte432" || t.transfer === "pg" || t.transfer === "hlg" || t.matrix === "bt2020-ncl";
  }
  /** Returns the decoder configuration for decoding the track's packets using a VideoDecoder. */
  getDecoderConfig() {
    return this._backing.getDecoderConfig();
  }
  async getCodecParameterString() {
    return (await this._backing.getDecoderConfig())?.codec ?? null;
  }
  async canDecode() {
    try {
      const t = await this._backing.getDecoderConfig();
      if (!t)
        return !1;
      const e = this._backing.getCodec();
      return f(e !== null), rr.some((s) => s.supports(e, t)) ? !0 : typeof VideoDecoder > "u" ? !1 : (await VideoDecoder.isConfigSupported(t)).supported === !0;
    } catch (t) {
      return console.error("Error during decodability check:", t), !1;
    }
  }
}, xe = class extends Hi {
  /** @internal */
  constructor(t) {
    super(t), this._backing = t;
  }
  get type() {
    return "audio";
  }
  get codec() {
    return this._backing.getCodec();
  }
  /** The number of audio channels in the track. */
  get numberOfChannels() {
    return this._backing.getNumberOfChannels();
  }
  /** The track's audio sample rate in hertz. */
  get sampleRate() {
    return this._backing.getSampleRate();
  }
  /** Returns the decoder configuration for decoding the track's packets using an AudioDecoder. */
  getDecoderConfig() {
    return this._backing.getDecoderConfig();
  }
  async getCodecParameterString() {
    return (await this._backing.getDecoderConfig())?.codec ?? null;
  }
  async canDecode() {
    try {
      const t = await this._backing.getDecoderConfig();
      if (!t)
        return !1;
      const e = this._backing.getCodec();
      return f(e !== null), ar.some((i) => i.supports(e, t)) || t.codec.startsWith("pcm-") ? !0 : typeof AudioDecoder > "u" ? !1 : (await AudioDecoder.isConfigSupported(t)).supported === !0;
    } catch (t) {
      return console.error("Error during decodability check:", t), !1;
    }
  }
}, gt = class {
  constructor(t, e = 1 / 0) {
    this.source = t, this.maxStorableBytes = e, this.loadedSegments = [], this.loadingSegments = [], this.sourceSizePromise = null, this.nextAge = 0, this.totalStoredBytes = 0;
  }
  async loadRange(t, e) {
    if (e = Math.min(e, await this.source.getSize()), t >= e)
      return;
    const i = this.loadingSegments.find((o) => o.start <= t && o.end >= e);
    if (i) {
      await i.promise;
      return;
    }
    const s = N(
      this.loadedSegments,
      t,
      (o) => o.start
    );
    if (s !== -1)
      for (let o = s; o < this.loadedSegments.length; o++) {
        const c = this.loadedSegments[o];
        if (c.start > t)
          break;
        if (c.end >= e)
          return;
      }
    this.source.onread?.(t, e);
    const r = this.source._read(t, e), a = { start: t, end: e, promise: r };
    this.loadingSegments.push(a);
    const n = await r;
    Ba(this.loadingSegments, a), this.insertIntoLoadedSegments(t, n);
  }
  rangeIsLoaded(t, e) {
    if (e <= t)
      return !0;
    const i = N(this.loadedSegments, t, (s) => s.start);
    if (i === -1)
      return !1;
    for (let s = i; s < this.loadedSegments.length; s++) {
      const r = this.loadedSegments[s];
      if (r.start > t)
        break;
      if (r.end >= e)
        return !0;
    }
    return !1;
  }
  insertIntoLoadedSegments(t, e) {
    const i = {
      start: t,
      end: t + e.byteLength,
      bytes: e,
      view: new DataView(e.buffer),
      age: this.nextAge++
    };
    let s = N(this.loadedSegments, t, (r) => r.start);
    (s === -1 || this.loadedSegments[s].start < i.start) && s++, this.loadedSegments.splice(s, 0, i), this.totalStoredBytes += e.byteLength;
    for (let r = s + 1; r < this.loadedSegments.length; r++) {
      const a = this.loadedSegments[r];
      if (a.start >= i.end)
        break;
      i.start <= a.start && a.end <= i.end && (this.loadedSegments.splice(r, 1), r--);
    }
    for (; this.totalStoredBytes > this.maxStorableBytes && this.loadedSegments.length > 1; ) {
      let r = null, a = -1;
      for (let n = 0; n < this.loadedSegments.length; n++) {
        const o = this.loadedSegments[n];
        (!r || o.age < r.age) && (r = o, a = n);
      }
      f(r), this.totalStoredBytes -= r.bytes.byteLength, this.loadedSegments.splice(a, 1);
    }
  }
  getViewAndOffset(t, e) {
    const i = N(this.loadedSegments, t, (r) => r.start);
    let s = null;
    if (i !== -1)
      for (let r = i; r < this.loadedSegments.length; r++) {
        const a = this.loadedSegments[r];
        if (a.start > t)
          break;
        if (e <= a.end) {
          s = a;
          break;
        }
      }
    if (!s)
      throw new Error(`No segment loaded for range [${t}, ${e}).`);
    return s.age = this.nextAge++, {
      view: s.view,
      offset: s.bytes.byteOffset + t - s.start
    };
  }
  forgetRange(t, e) {
    if (e <= t)
      return;
    const i = N(this.loadedSegments, t, (r) => r.start);
    if (i === -1)
      return;
    const s = this.loadedSegments[i];
    s.start !== t || s.end !== e || (this.loadedSegments.splice(i, 1), this.totalStoredBytes -= s.bytes.byteLength);
  }
}, vi = class {
  constructor(t) {
    this.reader = t, this.pos = 0, this.littleEndian = !0;
  }
  readBytes(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    return this.pos += t, new Uint8Array(e.buffer, i, t);
  }
  readU16() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 2);
    return this.pos += 2, t.getUint16(e, this.littleEndian);
  }
  readU32() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 4);
    return this.pos += 4, t.getUint32(e, this.littleEndian);
  }
  readAscii(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    this.pos += t;
    let s = "";
    for (let r = 0; r < t; r++)
      s += String.fromCharCode(e.getUint8(i + r));
    return s;
  }
}, sc = class extends mt {
  constructor(t) {
    super(t), this.metadataPromise = null, this.dataStart = -1, this.dataSize = -1, this.audioInfo = null, this.tracks = [], this.metadataReader = new vi(t._mainReader), this.chunkReader = new vi(new gt(t.source, 64 * 2 ** 20));
  }
  async readMetadata() {
    return this.metadataPromise ??= (async () => {
      const t = this.metadataReader.readAscii(4);
      this.metadataReader.littleEndian = t === "RIFF";
      const e = this.metadataReader.readU32() + 8;
      if (this.metadataReader.readAscii(4) !== "WAVE")
        throw new Error("Invalid WAVE file - wrong format");
      for (this.metadataReader.pos = 12; this.metadataReader.pos < e; ) {
        await this.metadataReader.reader.loadRange(this.metadataReader.pos, this.metadataReader.pos + 8);
        const r = this.metadataReader.readAscii(4), a = this.metadataReader.readU32(), n = this.metadataReader.pos;
        r === "fmt " ? await this.parseFmtChunk(a) : r === "data" && (this.dataStart = this.metadataReader.pos, this.dataSize = a), this.metadataReader.pos = n + a + (a & 1);
      }
      if (!this.audioInfo)
        throw new Error('Invalid WAVE file - missing "fmt " chunk');
      if (this.dataStart === -1)
        throw new Error('Invalid WAVE file - missing "data" chunk');
      const s = this.audioInfo.blockSizeInBytes;
      this.dataSize = Math.floor(this.dataSize / s) * s, this.tracks.push(new xe(new rc(this)));
    })();
  }
  async parseFmtChunk(t) {
    await this.metadataReader.reader.loadRange(this.metadataReader.pos, this.metadataReader.pos + t);
    let e = this.metadataReader.readU16();
    const i = this.metadataReader.readU16(), s = this.metadataReader.readU32();
    this.metadataReader.pos += 4;
    const r = this.metadataReader.readU16();
    let a;
    if (t === 14 ? a = 8 : a = this.metadataReader.readU16(), t >= 18 && e !== 357) {
      const n = this.metadataReader.readU16(), o = t - 18;
      if (Math.min(o, n) >= 22 && e === 65534) {
        this.metadataReader.pos += 6;
        const l = this.metadataReader.readBytes(16);
        e = l[0] | l[1] << 8;
      }
    }
    (e === 7 || e === 6) && (a = 8), this.audioInfo = {
      format: e,
      numberOfChannels: i,
      sampleRate: s,
      sampleSizeInBytes: Math.ceil(a / 8),
      blockSizeInBytes: r
    };
  }
  getCodec() {
    if (f(this.audioInfo), this.audioInfo.format === 7)
      return "ulaw";
    if (this.audioInfo.format === 6)
      return "alaw";
    if (this.audioInfo.format === 1) {
      if (this.audioInfo.sampleSizeInBytes === 1)
        return "pcm-u8";
      if (this.audioInfo.sampleSizeInBytes === 2)
        return "pcm-s16";
      if (this.audioInfo.sampleSizeInBytes === 3)
        return "pcm-s24";
      if (this.audioInfo.sampleSizeInBytes === 4)
        return "pcm-s32";
    }
    return this.audioInfo.format === 3 && this.audioInfo.sampleSizeInBytes === 4 ? "pcm-f32" : null;
  }
  async getMimeType() {
    return "audio/wav";
  }
  async computeDuration() {
    return await this.readMetadata(), f(this.audioInfo), this.dataSize / this.audioInfo.blockSizeInBytes / this.audioInfo.sampleRate;
  }
  async getTracks() {
    return await this.readMetadata(), this.tracks;
  }
}, Ve = 2048, rc = class {
  constructor(t) {
    this.demuxer = t;
  }
  getId() {
    return 1;
  }
  getCodec() {
    return this.demuxer.getCodec();
  }
  async getDecoderConfig() {
    const t = this.demuxer.getCodec();
    return t ? (f(this.demuxer.audioInfo), {
      codec: t,
      numberOfChannels: this.demuxer.audioInfo.numberOfChannels,
      sampleRate: this.demuxer.audioInfo.sampleRate
    }) : null;
  }
  computeDuration() {
    return this.demuxer.computeDuration();
  }
  getNumberOfChannels() {
    return f(this.demuxer.audioInfo), this.demuxer.audioInfo.numberOfChannels;
  }
  getSampleRate() {
    return f(this.demuxer.audioInfo), this.demuxer.audioInfo.sampleRate;
  }
  getTimeResolution() {
    return f(this.demuxer.audioInfo), this.demuxer.audioInfo.sampleRate;
  }
  getLanguageCode() {
    return fe;
  }
  async getFirstTimestamp() {
    return 0;
  }
  async getPacketAtIndex(t, e) {
    f(this.demuxer.audioInfo);
    const i = t * Ve * this.demuxer.audioInfo.blockSizeInBytes;
    if (i >= this.demuxer.dataSize)
      return null;
    const s = Math.min(
      Ve * this.demuxer.audioInfo.blockSizeInBytes,
      this.demuxer.dataSize - i
    );
    let r;
    if (e.metadataOnly)
      r = me;
    else {
      const o = Ve * this.demuxer.audioInfo.blockSizeInBytes, c = Math.ceil(2 ** 19 / o) * o, l = Math.floor(i / c) * c, d = l + c;
      await this.demuxer.chunkReader.reader.loadRange(
        this.demuxer.dataStart + l,
        this.demuxer.dataStart + d
      ), this.demuxer.chunkReader.pos = this.demuxer.dataStart + i, r = this.demuxer.chunkReader.readBytes(s);
    }
    const a = t * Ve / this.demuxer.audioInfo.sampleRate, n = s / this.demuxer.audioInfo.blockSizeInBytes / this.demuxer.audioInfo.sampleRate;
    return new G(
      r,
      "key",
      a,
      n,
      t,
      s
    );
  }
  getFirstPacket(t) {
    return this.getPacketAtIndex(0, t);
  }
  getPacket(t, e) {
    f(this.demuxer.audioInfo);
    const i = Math.floor(t * this.demuxer.audioInfo.sampleRate / Ve);
    return this.getPacketAtIndex(i, e);
  }
  getNextPacket(t, e) {
    f(this.demuxer.audioInfo);
    const i = Math.round(t.timestamp * this.demuxer.audioInfo.sampleRate / Ve);
    return this.getPacketAtIndex(i + 1, e);
  }
  getKeyPacket(t, e) {
    return this.getPacket(t, e);
  }
  getNextKeyPacket(t, e) {
    return this.getNextPacket(t, e);
  }
}, Lt = class {
  /** Returns a list of video codecs that this output format can contain. */
  getSupportedVideoCodecs() {
    return this.getSupportedCodecs().filter((e) => re.includes(e));
  }
  /** Returns a list of audio codecs that this output format can contain. */
  getSupportedAudioCodecs() {
    return this.getSupportedCodecs().filter((e) => ne.includes(e));
  }
  /** Returns a list of subtitle codecs that this output format can contain. */
  getSupportedSubtitleCodecs() {
    return this.getSupportedCodecs().filter((e) => ve.includes(e));
  }
  /** @internal */
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  _codecUnsupportedHint(e) {
    return "";
  }
}, Hr = class extends Lt {
  constructor(t = {}) {
    if (!t || typeof t != "object")
      throw new TypeError("options must be an object.");
    if (t.fastStart !== void 0 && ![!1, "in-memory", "fragmented"].includes(t.fastStart))
      throw new TypeError('options.fastStart, when provided, must be false, "in-memory", or "fragmented".');
    if (t.minimumFragmentDuration !== void 0 && (!Number.isFinite(t.minimumFragmentDuration) || t.minimumFragmentDuration < 0))
      throw new TypeError("options.minimumFragmentDuration, when provided, must be a non-negative number.");
    if (t.onFtyp !== void 0 && typeof t.onFtyp != "function")
      throw new TypeError("options.onFtyp, when provided, must be a function.");
    if (t.onMoov !== void 0 && typeof t.onMoov != "function")
      throw new TypeError("options.onMoov, when provided, must be a function.");
    if (t.onMdat !== void 0 && typeof t.onMdat != "function")
      throw new TypeError("options.onMdat, when provided, must be a function.");
    if (t.onMoof !== void 0 && typeof t.onMoof != "function")
      throw new TypeError("options.onMoof, when provided, must be a function.");
    super(), this._options = t;
  }
  getSupportedTrackCounts() {
    return {
      video: { min: 0, max: 1 / 0 },
      audio: { min: 0, max: 1 / 0 },
      subtitle: { min: 0, max: 1 / 0 },
      total: { min: 1, max: 2 ** 32 - 1 }
      // Have fun reaching this one
    };
  }
  get supportsVideoRotationMetadata() {
    return !0;
  }
  /** @internal */
  _createMuxer(t) {
    return new xo(t, this);
  }
}, $r = class extends Hr {
  /** @internal */
  get _name() {
    return "MP4";
  }
  get fileExtension() {
    return ".mp4";
  }
  get mimeType() {
    return "video/mp4";
  }
  getSupportedCodecs() {
    return [
      ...re,
      ...Mi,
      ...ve
    ];
  }
  /** @internal */
  _codecUnsupportedHint(t) {
    return new jr().getSupportedCodecs().includes(t) ? " Switching to MOV will grant support for this codec." : "";
  }
}, jr = class extends Hr {
  /** @internal */
  get _name() {
    return "MOV";
  }
  get fileExtension() {
    return ".mov";
  }
  get mimeType() {
    return "video/quicktime";
  }
  getSupportedCodecs() {
    return [
      ...re,
      ...ne
    ];
  }
  /** @internal */
  _codecUnsupportedHint(t) {
    return new $r().getSupportedCodecs().includes(t) ? " Switching to MP4 will grant support for this codec." : "";
  }
}, bs = class extends Lt {
  constructor(t = {}) {
    if (!t || typeof t != "object")
      throw new TypeError("options must be an object.");
    if (t.streamable !== void 0 && typeof t.streamable != "boolean")
      throw new TypeError("options.streamable, when provided, must be a boolean.");
    if (t.minimumClusterDuration !== void 0 && (!Number.isFinite(t.minimumClusterDuration) || t.minimumClusterDuration < 0))
      throw new TypeError("options.minimumClusterDuration, when provided, must be a non-negative number.");
    if (t.onEbmlHeader !== void 0 && typeof t.onEbmlHeader != "function")
      throw new TypeError("options.onEbmlHeader, when provided, must be a function.");
    if (t.onSegmentHeader !== void 0 && typeof t.onSegmentHeader != "function")
      throw new TypeError("options.onHeader, when provided, must be a function.");
    if (t.onCluster !== void 0 && typeof t.onCluster != "function")
      throw new TypeError("options.onCluster, when provided, must be a function.");
    super(), this._options = t;
  }
  /** @internal */
  _createMuxer(t) {
    return new Bo(t, this);
  }
  /** @internal */
  get _name() {
    return "Matroska";
  }
  getSupportedTrackCounts() {
    return {
      video: { min: 0, max: 1 / 0 },
      audio: { min: 0, max: 1 / 0 },
      subtitle: { min: 0, max: 1 / 0 },
      total: { min: 1, max: 127 }
    };
  }
  get fileExtension() {
    return ".mkv";
  }
  get mimeType() {
    return "video/x-matroska";
  }
  getSupportedCodecs() {
    return [
      ...re,
      ...Mi,
      ...K.filter((t) => !["pcm-s8", "pcm-f32be", "ulaw", "alaw"].includes(t)),
      ...ve
    ];
  }
  get supportsVideoRotationMetadata() {
    return !1;
  }
}, qr = class extends bs {
  getSupportedCodecs() {
    return [
      ...re.filter((t) => ["vp8", "vp9", "av1"].includes(t)),
      ...ne.filter((t) => ["opus", "vorbis"].includes(t)),
      ...ve
    ];
  }
  /** @internal */
  get _name() {
    return "WebM";
  }
  get fileExtension() {
    return ".webm";
  }
  get mimeType() {
    return "video/webm";
  }
  /** @internal */
  _codecUnsupportedHint(t) {
    return new bs().getSupportedCodecs().includes(t) ? " Switching to MKV will grant support for this codec." : "";
  }
}, ac = class extends Lt {
  constructor(t = {}) {
    if (!t || typeof t != "object")
      throw new TypeError("options must be an object.");
    if (t.onPage !== void 0 && typeof t.onPage != "function")
      throw new TypeError("options.onPage, when provided, must be a function.");
    super(), this._options = t;
  }
  /** @internal */
  _createMuxer(t) {
    return new $o(t, this);
  }
  /** @internal */
  get _name() {
    return "Ogg";
  }
  getSupportedTrackCounts() {
    return {
      video: { min: 0, max: 0 },
      audio: { min: 0, max: 1 / 0 },
      subtitle: { min: 0, max: 0 },
      total: { min: 1, max: 2 ** 32 }
    };
  }
  get fileExtension() {
    return ".ogg";
  }
  get mimeType() {
    return "application/ogg";
  }
  getSupportedCodecs() {
    return [
      ...ne.filter((t) => ["vorbis", "opus"].includes(t))
    ];
  }
  get supportsVideoRotationMetadata() {
    return !1;
  }
}, $i = class {
  constructor() {
    this._connectedTrack = null, this._closingPromise = null, this._closed = !1, this._timestampOffset = 0;
  }
  /** @internal */
  _ensureValidAdd() {
    if (!this._connectedTrack)
      throw new Error("Source is not connected to an output track.");
    if (this._connectedTrack.output.state === "canceled")
      throw new Error("Output has been canceled.");
    if (this._connectedTrack.output.state === "finalizing" || this._connectedTrack.output.state === "finalized")
      throw new Error("Output has been finalized.");
    if (this._connectedTrack.output.state === "pending")
      throw new Error("Output has not started.");
    if (this._closed)
      throw new Error("Source is closed.");
  }
  /** @internal */
  _start() {
  }
  /** @internal */
  async _flushAndClose() {
  }
  /**
   * Closes this source. This prevents future samples from being added and signals to the output file that no further
   * samples will come in for this track. Calling `.close()` is optional but recommended after adding the
   * last sample - for improved performance and reduced memory usage.
   */
  close() {
    if (this._closingPromise)
      throw new Error("Source already closed.");
    const t = this._connectedTrack;
    if (!t)
      throw new Error("Cannot call close without connecting the source to an output track.");
    if (t.output.state === "pending")
      throw new Error("Cannot call close before output has been started.");
    return this._closingPromise = (async () => {
      await this._flushAndClose(), this._closed = !0, !(t.output.state === "finalizing" || t.output.state === "finalized") && t.output._muxer.onTrackClose(t);
    })();
  }
  /** @internal */
  async _flushOrWaitForClose() {
    return this._closingPromise ? this._closingPromise : this._flushAndClose();
  }
}, Qr = class extends $i {
  constructor(e) {
    if (super(), this._connectedTrack = null, !re.includes(e))
      throw new TypeError(`Invalid video codec '${e}'. Must be one of: ${re.join(", ")}.`);
    this._codec = e;
  }
}, nc = (t) => {
  if (!t || typeof t != "object")
    throw new TypeError("Encoding config must be an object.");
  if (!re.includes(t.codec))
    throw new TypeError(`Invalid video codec '${t.codec}'. Must be one of: ${re.join(", ")}.`);
  if (!(t.bitrate instanceof Nt) && (!Number.isInteger(t.bitrate) || t.bitrate <= 0))
    throw new TypeError("config.bitrate must be a positive integer or a quality.");
  if (t.latencyMode !== void 0 && !["quality", "realtime"].includes(t.latencyMode))
    throw new TypeError("config.latencyMode, when provided, must be 'quality' or 'realtime'.");
  if (t.keyFrameInterval !== void 0 && (!Number.isFinite(t.keyFrameInterval) || t.keyFrameInterval < 0))
    throw new TypeError("config.keyFrameInterval, when provided, must be a non-negative number.");
  if (t.fullCodecString !== void 0 && typeof t.fullCodecString != "string")
    throw new TypeError("config.fullCodecString, when provided, must be a string.");
  if (t.fullCodecString !== void 0 && hr(t.fullCodecString) !== t.codec)
    throw new TypeError(
      `config.fullCodecString, when provided, must be a string that matches the specified codec (${t.codec}).`
    );
  if (t.onEncodedPacket !== void 0 && typeof t.onEncodedPacket != "function")
    throw new TypeError("config.onEncodedChunk, when provided, must be a function.");
  if (t.onEncoderError !== void 0 && typeof t.onEncoderError != "function")
    throw new TypeError("config.onEncodingError, when provided, must be a function.");
  if (t.onEncoderConfig !== void 0 && typeof t.onEncoderConfig != "function")
    throw new TypeError("config.onEncoderConfig, when provided, must be a function.");
}, oc = class {
  constructor(t, e) {
    this.source = t, this.encodingConfig = e, this.ensureEncoderPromise = null, this.encoderInitialized = !1, this.encoder = null, this.muxer = null, this.lastMultipleOfKeyFrameInterval = -1, this.lastWidth = null, this.lastHeight = null, this.customEncoder = null, this.customEncoderCallSerializer = new Dt(), this.customEncoderQueueSize = 0;
  }
  async add(t, e, i) {
    if (this.source._ensureValidAdd(), this.lastWidth !== null && this.lastHeight !== null) {
      if (t.codedWidth !== this.lastWidth || t.codedHeight !== this.lastHeight)
        throw new Error(
          `Video sample size must remain constant. Expected ${this.lastWidth}x${this.lastHeight}, got ${t.codedWidth}x${t.codedHeight}.`
        );
    } else
      this.lastWidth = t.codedWidth, this.lastHeight = t.codedHeight;
    this.encoderInitialized || (this.ensureEncoderPromise || this.ensureEncoder(t), this.encoderInitialized || await this.ensureEncoderPromise), f(this.encoderInitialized);
    const s = this.encodingConfig.keyFrameInterval ?? 5, r = Math.floor(t.timestamp / s), a = {
      ...i,
      keyFrame: i?.keyFrame || s === 0 || r !== this.lastMultipleOfKeyFrameInterval
    };
    if (this.lastMultipleOfKeyFrameInterval = r, this.customEncoder) {
      this.customEncoderQueueSize++;
      const n = this.customEncoderCallSerializer.call(() => this.customEncoder.encode(t, a)).then(() => {
        this.customEncoderQueueSize--, e && t.close();
      });
      this.customEncoderQueueSize >= 4 && await n;
    } else {
      f(this.encoder);
      const n = t.toVideoFrame();
      this.encoder.encode(n, a), n.close(), e && t.close(), this.encoder.encodeQueueSize >= 4 && await new Promise((o) => this.encoder.addEventListener("dequeue", o, { once: !0 }));
    }
    await this.muxer.mutex.currentPromise;
  }
  async ensureEncoder(t) {
    if (this.encoder)
      return;
    const { promise: e, resolve: i } = Z();
    this.ensureEncoderPromise = e;
    const s = t.codedWidth, r = t.codedHeight, a = this.encodingConfig.bitrate instanceof Nt ? this.encodingConfig.bitrate._toVideoBitrate(this.encodingConfig.codec, s, r) : this.encodingConfig.bitrate, n = {
      codec: this.encodingConfig.fullCodecString ?? ci(
        this.encodingConfig.codec,
        s,
        r,
        a
      ),
      width: s,
      height: r,
      bitrate: a,
      framerate: this.source._connectedTrack?.metadata.frameRate,
      latencyMode: this.encodingConfig.latencyMode,
      ...di(this.encodingConfig.codec)
    };
    this.encodingConfig.onEncoderConfig?.(n);
    const o = It.find((c) => c.supports(
      this.encodingConfig.codec,
      n
    ));
    if (o)
      this.customEncoder = new o(), this.customEncoder.codec = this.encodingConfig.codec, this.customEncoder.config = n, this.customEncoder.onPacket = (c, l) => {
        if (!(c instanceof G))
          throw new TypeError("The first argument passed to onPacket must be an EncodedPacket.");
        if (l !== void 0 && (!l || typeof l != "object"))
          throw new TypeError("The second argument passed to onPacket must be an object or undefined.");
        this.encodingConfig.onEncodedPacket?.(c, l), this.muxer.addEncodedVideoPacket(this.source._connectedTrack, c, l);
      }, await this.customEncoder.init();
    else {
      if (typeof VideoEncoder > "u")
        throw new Error("VideoEncoder is not supported by this browser.");
      if (!(await VideoEncoder.isConfigSupported(n)).supported)
        throw new Error(
          "This specific encoder configuration is not supported by this browser. Consider using another codec or changing your video parameters."
        );
      this.encoder = new VideoEncoder({
        output: (l, d) => {
          const h = G.fromEncodedChunk(l);
          this.encodingConfig.onEncodedPacket?.(h, d), this.muxer.addEncodedVideoPacket(this.source._connectedTrack, h, d);
        },
        error: this.encodingConfig.onEncoderError ?? ((l) => console.error("VideoEncoder error:", l))
      }), this.encoder.configure(n);
    }
    f(this.source._connectedTrack), this.muxer = this.source._connectedTrack.output._muxer, this.encoderInitialized = !0, i();
  }
  async flushAndClose() {
    this.customEncoder ? (this.customEncoderCallSerializer.call(() => this.customEncoder.flush()), await this.customEncoderCallSerializer.call(() => this.customEncoder.close())) : this.encoder && (await this.encoder.flush(), this.encoder.close());
  }
  getQueueSize() {
    return this.customEncoder ? this.customEncoderQueueSize : this.encoder?.encodeQueueSize ?? 0;
  }
}, cc = class extends Qr {
  constructor(t, e) {
    if (!(typeof HTMLCanvasElement < "u" && t instanceof HTMLCanvasElement) && !(typeof OffscreenCanvas < "u" && t instanceof OffscreenCanvas))
      throw new TypeError("canvas must be an HTMLCanvasElement or OffscreenCanvas.");
    nc(e), super(e.codec), this._encoder = new oc(this, e), this._canvas = t;
  }
  /**
   * Captures the current canvas state as a video sample (frame), encodes it and adds it to the output.
   *
   * @param timestamp - The timestamp of the sample, in seconds.
   * @param duration - The duration of the sample, in seconds.
   *
   * @returns A Promise that resolves once the output is ready to receive more samples. You should await this Promise
   * to respect writer and encoder backpressure.
   */
  add(t, e = 0, i) {
    if (!Number.isFinite(t) || t < 0)
      throw new TypeError("timestamp must be a non-negative number.");
    if (!Number.isFinite(e) || e < 0)
      throw new TypeError("duration must be a non-negative number.");
    const s = new ki(this._canvas, { timestamp: t, duration: e });
    return this._encoder.add(s, !0, i);
  }
  /** @internal */
  _flushAndClose() {
    return this._encoder.flushAndClose();
  }
}, Xr = class extends $i {
  constructor(e) {
    if (super(), this._connectedTrack = null, !ne.includes(e))
      throw new TypeError(`Invalid audio codec '${e}'. Must be one of: ${ne.join(", ")}.`);
    this._codec = e;
  }
}, lc = (t) => {
  if (!t || typeof t != "object")
    throw new TypeError("Encoding config must be an object.");
  if (!ne.includes(t.codec))
    throw new TypeError(`Invalid audio codec '${t.codec}'. Must be one of: ${ne.join(", ")}.`);
  if (t.bitrate === void 0 && (!K.includes(t.codec) || t.codec === "flac"))
    throw new TypeError("config.bitrate must be provided for compressed audio codecs.");
  if (t.bitrate !== void 0 && !(t.bitrate instanceof Nt) && (!Number.isInteger(t.bitrate) || t.bitrate <= 0))
    throw new TypeError("config.bitrate, when provided, must be a positive integer or a quality.");
  if (t.fullCodecString !== void 0 && typeof t.fullCodecString != "string")
    throw new TypeError("config.fullCodecString, when provided, must be a string.");
  if (t.fullCodecString !== void 0 && hr(t.fullCodecString) !== t.codec)
    throw new TypeError(
      `config.fullCodecString, when provided, must be a string that matches the specified codec (${t.codec}).`
    );
  if (t.onEncodedPacket !== void 0 && typeof t.onEncodedPacket != "function")
    throw new TypeError("config.onEncodedChunk, when provided, must be a function.");
  if (t.onEncoderError !== void 0 && typeof t.onEncoderError != "function")
    throw new TypeError("config.onEncodingError, when provided, must be a function.");
  if (t.onEncoderConfig !== void 0 && typeof t.onEncoderConfig != "function")
    throw new TypeError("config.onEncoderConfig, when provided, must be a function.");
}, dc = class {
  constructor(t, e) {
    this.source = t, this.encodingConfig = e, this.ensureEncoderPromise = null, this.encoderInitialized = !1, this.encoder = null, this.muxer = null, this.lastNumberOfChannels = null, this.lastSampleRate = null, this.isPcmEncoder = !1, this.outputSampleSize = null, this.writeOutputValue = null, this.customEncoder = null, this.customEncoderCallSerializer = new Dt(), this.customEncoderQueueSize = 0;
  }
  async add(t, e) {
    if (this.source._ensureValidAdd(), this.lastNumberOfChannels !== null && this.lastSampleRate !== null) {
      if (t.numberOfChannels !== this.lastNumberOfChannels || t.sampleRate !== this.lastSampleRate)
        throw new Error(
          `Audio parameters must remain constant. Expected ${this.lastNumberOfChannels} channels at ${this.lastSampleRate} Hz, got ${t.numberOfChannels} channels at ${t.sampleRate} Hz.`
        );
    } else
      this.lastNumberOfChannels = t.numberOfChannels, this.lastSampleRate = t.sampleRate;
    if (this.encoderInitialized || (this.ensureEncoderPromise || this.ensureEncoder(t), this.encoderInitialized || await this.ensureEncoderPromise), f(this.encoderInitialized), this.customEncoder) {
      this.customEncoderQueueSize++;
      const i = this.customEncoderCallSerializer.call(() => this.customEncoder.encode(t)).then(() => {
        this.customEncoderQueueSize--, e && t.close();
      });
      this.customEncoderQueueSize >= 4 && await i, await this.muxer.mutex.currentPromise;
    } else if (this.isPcmEncoder)
      await this.doPcmEncoding(t, e);
    else {
      f(this.encoder);
      const i = t.toAudioData();
      this.encoder.encode(i), i.close(), e && t.close(), this.encoder.encodeQueueSize >= 4 && await new Promise((s) => this.encoder.addEventListener("dequeue", s, { once: !0 })), await this.muxer.mutex.currentPromise;
    }
  }
  async doPcmEncoding(t, e) {
    f(this.outputSampleSize), f(this.writeOutputValue);
    const { numberOfChannels: i, numberOfFrames: s, sampleRate: r, timestamp: a } = t, n = 2048, o = [];
    for (let h = 0; h < s; h += n) {
      const u = Math.min(n, t.numberOfFrames - h), m = u * i * this.outputSampleSize, p = new ArrayBuffer(m), g = new DataView(p);
      o.push({ frameCount: u, view: g });
    }
    const c = t.allocationSize({ planeIndex: 0, format: "f32-planar" }), l = new Float32Array(c / Float32Array.BYTES_PER_ELEMENT);
    for (let h = 0; h < i; h++) {
      t.copyTo(l, { planeIndex: h, format: "f32-planar" });
      for (let u = 0; u < o.length; u++) {
        const { frameCount: m, view: p } = o[u];
        for (let g = 0; g < m; g++)
          this.writeOutputValue(
            p,
            (g * i + h) * this.outputSampleSize,
            l[u * n + g]
          );
      }
    }
    e && t.close();
    const d = {
      decoderConfig: {
        codec: this.encodingConfig.codec,
        numberOfChannels: i,
        sampleRate: r
      }
    };
    for (let h = 0; h < o.length; h++) {
      const { frameCount: u, view: m } = o[h], p = m.buffer, g = h * n, w = new G(
        new Uint8Array(p),
        "key",
        a + g / r,
        u / r
      );
      this.encodingConfig.onEncodedPacket?.(w, d), await this.muxer.addEncodedAudioPacket(this.source._connectedTrack, w, d);
    }
  }
  async ensureEncoder(t) {
    if (this.encoderInitialized)
      return;
    const { promise: e, resolve: i } = Z();
    this.ensureEncoderPromise = e;
    const { numberOfChannels: s, sampleRate: r } = t, a = this.encodingConfig.bitrate instanceof Nt ? this.encodingConfig.bitrate._toAudioBitrate(this.encodingConfig.codec) : this.encodingConfig.bitrate, n = {
      codec: this.encodingConfig.fullCodecString ?? li(
        this.encodingConfig.codec,
        s,
        r
      ),
      numberOfChannels: s,
      sampleRate: r,
      bitrate: a,
      ...hi(this.encodingConfig.codec)
    };
    this.encodingConfig.onEncoderConfig?.(n);
    const o = Et.find((c) => c.supports(
      this.encodingConfig.codec,
      n
    ));
    if (o)
      this.customEncoder = new o(), this.customEncoder.codec = this.encodingConfig.codec, this.customEncoder.config = n, this.customEncoder.onPacket = (c, l) => {
        if (!(c instanceof G))
          throw new TypeError("The first argument passed to onPacket must be an EncodedPacket.");
        if (l !== void 0 && (!l || typeof l != "object"))
          throw new TypeError("The second argument passed to onPacket must be an object or undefined.");
        this.encodingConfig.onEncodedPacket?.(c, l), this.muxer.addEncodedAudioPacket(this.source._connectedTrack, c, l);
      }, await this.customEncoder.init();
    else if (K.includes(this.encodingConfig.codec))
      this.initPcmEncoder();
    else {
      if (typeof AudioEncoder > "u")
        throw new Error("AudioEncoder is not supported by this browser.");
      if (!(await AudioEncoder.isConfigSupported(n)).supported)
        throw new Error(
          "This specific encoder configuration not supported by this browser. Consider using another codec or changing your audio parameters."
        );
      this.encoder = new AudioEncoder({
        output: (l, d) => {
          const h = G.fromEncodedChunk(l);
          this.encodingConfig.onEncodedPacket?.(h, d), this.muxer.addEncodedAudioPacket(this.source._connectedTrack, h, d);
        },
        error: this.encodingConfig.onEncoderError ?? ((l) => console.error("AudioEncoder error:", l))
      }), this.encoder.configure(n);
    }
    f(this.source._connectedTrack), this.muxer = this.source._connectedTrack.output._muxer, this.encoderInitialized = !0, i();
  }
  initPcmEncoder() {
    this.isPcmEncoder = !0;
    const t = this.encodingConfig.codec, { dataType: e, sampleSize: i, littleEndian: s } = Be(t);
    switch (this.outputSampleSize = i, i) {
      case 1:
        e === "unsigned" ? this.writeOutputValue = (r, a, n) => r.setUint8(a, J((n + 1) * 127.5, 0, 255)) : e === "signed" ? this.writeOutputValue = (r, a, n) => {
          r.setInt8(a, J(Math.round(n * 128), -128, 127));
        } : e === "ulaw" ? this.writeOutputValue = (r, a, n) => {
          const o = J(Math.floor(n * 32767), -32768, 32767);
          r.setUint8(a, jo(o));
        } : e === "alaw" ? this.writeOutputValue = (r, a, n) => {
          const o = J(Math.floor(n * 32767), -32768, 32767);
          r.setUint8(a, Qo(o));
        } : f(!1);
        break;
      case 2:
        e === "unsigned" ? this.writeOutputValue = (r, a, n) => r.setUint16(a, J((n + 1) * 32767.5, 0, 65535), s) : e === "signed" ? this.writeOutputValue = (r, a, n) => r.setInt16(a, J(Math.round(n * 32767), -32768, 32767), s) : f(!1);
        break;
      case 3:
        e === "unsigned" ? this.writeOutputValue = (r, a, n) => ir(r, a, J((n + 1) * 83886075e-1, 0, 16777215), s) : e === "signed" ? this.writeOutputValue = (r, a, n) => za(
          r,
          a,
          J(Math.round(n * 8388607), -8388608, 8388607),
          s
        ) : f(!1);
        break;
      case 4:
        e === "unsigned" ? this.writeOutputValue = (r, a, n) => r.setUint32(a, J((n + 1) * 21474836475e-1, 0, 4294967295), s) : e === "signed" ? this.writeOutputValue = (r, a, n) => r.setInt32(
          a,
          J(Math.round(n * 2147483647), -2147483648, 2147483647),
          s
        ) : e === "float" ? this.writeOutputValue = (r, a, n) => r.setFloat32(a, n, s) : f(!1);
    }
  }
  async flushAndClose() {
    this.customEncoder ? (this.customEncoderCallSerializer.call(() => this.customEncoder.flush()), await this.customEncoderCallSerializer.call(() => this.customEncoder.close())) : this.encoder && (await this.encoder.flush(), this.encoder.close());
  }
  getQueueSize() {
    return this.customEncoder ? this.customEncoderQueueSize : this.isPcmEncoder ? 0 : this.encoder?.encodeQueueSize ?? 0;
  }
}, hc = class extends Xr {
  constructor(t) {
    lc(t), super(t.codec), this._accumulatedFrameCount = 0, this._encoder = new dc(this, t);
  }
  /**
   * Converts an AudioBuffer to audio samples, encodes them and adds them to the output. The first AudioBuffer will
   * be played at timestamp 0, and any subsequent AudioBuffer will have a timestamp equal to the total duration of
   * all previous AudioBuffers.
   *
   * @returns A Promise that resolves once the output is ready to receive more samples. You should await this Promise
   * to respect writer and encoder backpressure.
   */
  add(t) {
    if (!(t instanceof AudioBuffer))
      throw new TypeError("audioBuffer must be an AudioBuffer.");
    const e = 64 * 1024 * 1024, i = t.numberOfChannels, s = t.sampleRate, r = t.length, a = Math.floor(e / i);
    let n = 0, o = r;
    const c = [];
    for (; o > 0; ) {
      const l = Math.min(a, o), d = new Float32Array(i * l);
      for (let u = 0; u < i; u++)
        t.copyFromChannel(
          d.subarray(u * l, u * l + l),
          u,
          n
        );
      const h = new Ft({
        format: "f32-planar",
        sampleRate: s,
        numberOfFrames: l,
        numberOfChannels: i,
        timestamp: (this._accumulatedFrameCount + n) / s,
        data: d
      });
      c.push(this._encoder.add(h, !0)), n += l, o -= l;
    }
    return this._accumulatedFrameCount += r, Promise.all(c);
  }
  /** @internal */
  _flushAndClose() {
    return this._encoder.flushAndClose();
  }
}, uc = class extends $i {
  constructor(t) {
    if (super(), this._connectedTrack = null, !ve.includes(t))
      throw new TypeError(`Invalid subtitle codec '${t}'. Must be one of: ${ve.join(", ")}.`);
    this._codec = t;
  }
}, fc = ["video", "audio", "subtitle"], ii = (t) => {
  if (!t || typeof t != "object")
    throw new TypeError("metadata must be an object.");
  if (t.languageCode !== void 0 && !Oi(t.languageCode))
    throw new TypeError("metadata.languageCode must be a three-letter, ISO 639-2/T language code.");
}, mc = class {
  constructor(t) {
    if (this.state = "pending", this._tracks = [], this._startPromise = null, this._cancelPromise = null, this._finalizePromise = null, this._mutex = new ft(), !t || typeof t != "object")
      throw new TypeError("options must be an object.");
    if (!(t.format instanceof Lt))
      throw new TypeError("options.format must be an OutputFormat.");
    if (!(t.target instanceof Vi))
      throw new TypeError("options.target must be a Target.");
    if (t.target._output)
      throw new Error("Target is already used for another output.");
    t.target._output = this, this.format = t.format, this.target = t.target, this._writer = t.target._createWriter(), this._muxer = t.format._createMuxer(this);
  }
  /** Adds a video track to the output with the given source. Must be called before output is started. */
  addVideoTrack(t, e = {}) {
    if (!(t instanceof Qr))
      throw new TypeError("source must be a VideoSource.");
    if (ii(e), e.rotation !== void 0 && ![0, 90, 180, 270].includes(e.rotation))
      throw new TypeError(`Invalid video rotation: ${e.rotation}. Has to be 0, 90, 180 or 270.`);
    if (!this.format.supportsVideoRotationMetadata && e.rotation)
      throw new Error(`${this.format._name} does not support video rotation metadata.`);
    if (e.frameRate !== void 0 && (!Number.isFinite(e.frameRate) || e.frameRate <= 0))
      throw new TypeError(
        `Invalid video frame rate: ${e.frameRate}. Must be a positive number.`
      );
    this._addTrack("video", t, e);
  }
  /** Adds an audio track to the output with the given source. Must be called before output is started. */
  addAudioTrack(t, e = {}) {
    if (!(t instanceof Xr))
      throw new TypeError("source must be an AudioSource.");
    ii(e), this._addTrack("audio", t, e);
  }
  /** Adds a subtitle track to the output with the given source. Must be called before output is started. */
  addSubtitleTrack(t, e = {}) {
    if (!(t instanceof uc))
      throw new TypeError("source must be a SubtitleSource.");
    ii(e), this._addTrack("subtitle", t, e);
  }
  /** @internal */
  _addTrack(t, e, i) {
    if (this.state !== "pending")
      throw new Error("Cannot add track after output has been started or canceled.");
    if (e._connectedTrack)
      throw new Error("Source is already used for a track.");
    const s = this.format.getSupportedTrackCounts(), r = this._tracks.reduce(
      (c, l) => c + (l.type === t ? 1 : 0),
      0
    ), a = s[t].max;
    if (r === a)
      throw new Error(
        a === 0 ? `${this.format._name} does not support ${t} tracks.` : `${this.format._name} does not support more than ${a} ${t} track${a === 1 ? "" : "s"}.`
      );
    const n = s.total.max;
    if (this._tracks.length === n)
      throw new Error(
        `${this.format._name} does not support more than ${n} tracks${n === 1 ? "" : "s"} in total.`
      );
    const o = {
      id: this._tracks.length + 1,
      output: this,
      type: t,
      source: e,
      metadata: i
    };
    if (o.type === "video") {
      const c = this.format.getSupportedVideoCodecs();
      if (c.length === 0)
        throw new Error(
          `${this.format._name} does not support video tracks.` + this.format._codecUnsupportedHint(o.source._codec)
        );
      if (!c.includes(o.source._codec))
        throw new Error(
          `Codec '${o.source._codec}' cannot be contained within ${this.format._name}. Supported video codecs are: ${c.map((l) => `'${l}'`).join(", ")}.` + this.format._codecUnsupportedHint(o.source._codec)
        );
    } else if (o.type === "audio") {
      const c = this.format.getSupportedAudioCodecs();
      if (c.length === 0)
        throw new Error(
          `${this.format._name} does not support audio tracks.` + this.format._codecUnsupportedHint(o.source._codec)
        );
      if (!c.includes(o.source._codec))
        throw new Error(
          `Codec '${o.source._codec}' cannot be contained within ${this.format._name}. Supported audio codecs are: ${c.map((l) => `'${l}'`).join(", ")}.` + this.format._codecUnsupportedHint(o.source._codec)
        );
    } else if (o.type === "subtitle") {
      const c = this.format.getSupportedSubtitleCodecs();
      if (c.length === 0)
        throw new Error(
          `${this.format._name} does not support subtitle tracks.` + this.format._codecUnsupportedHint(o.source._codec)
        );
      if (!c.includes(o.source._codec))
        throw new Error(
          `Codec '${o.source._codec}' cannot be contained within ${this.format._name}. Supported subtitle codecs are: ${c.map((l) => `'${l}'`).join(", ")}.` + this.format._codecUnsupportedHint(o.source._codec)
        );
    }
    this._tracks.push(o), e._connectedTrack = o;
  }
  /**
   * Starts the creation of the output file. This method should be called after all tracks have been added. Only after
   * the output has started can media samples be added to the tracks.
   *
   * @returns A promise that resolves when the output has successfully started and is ready to receive media samples.
   */
  async start() {
    const t = this.format.getSupportedTrackCounts();
    for (const i of fc) {
      const s = this._tracks.reduce(
        (a, n) => a + (n.type === i ? 1 : 0),
        0
      ), r = t[i].min;
      if (s < r)
        throw new Error(
          r === t[i].max ? `${this.format._name} requires exactly ${r} ${i} track${r === 1 ? "" : "s"}.` : `${this.format._name} requires at least ${r} ${i} track${r === 1 ? "" : "s"}.`
        );
    }
    const e = t.total.min;
    if (this._tracks.length < e)
      throw new Error(
        e === t.total.max ? `${this.format._name} requires exactly ${e} track${e === 1 ? "" : "s"}.` : `${this.format._name} requires at least ${e} track${e === 1 ? "" : "s"}.`
      );
    if (this.state === "canceled")
      throw new Error("Output has been canceled.");
    return this._startPromise ? (console.warn("Output has already been started."), this._startPromise) : this._startPromise = (async () => {
      this.state = "started", this._writer.start();
      const i = await this._mutex.acquire();
      await this._muxer.start();
      for (const s of this._tracks)
        s.source._start();
      i();
    })();
  }
  /**
   * Cancels the creation of the output file, releasing internal resources like encoders and preventing further
   * samples from being added.
   *
   * @returns A promise that resolves once all internal resources have been released.
   */
  async cancel() {
    if (this._cancelPromise)
      return console.warn("Output has already been canceled."), this._cancelPromise;
    if (this.state === "finalizing" || this.state === "finalized") {
      console.warn("Output has already been finalized.");
      return;
    }
    return this._cancelPromise = (async () => {
      this.state = "canceled";
      const t = await this._mutex.acquire(), e = this._tracks.map((i) => i.source._flushOrWaitForClose());
      await Promise.all(e), await this._writer.close(), t();
    })();
  }
  /**
   * Finalizes the output file. This method must be called after all media samples across all tracks have been added.
   * Once the Promise returned by this method completes, the output file is ready.
   */
  async finalize() {
    if (this.state === "pending")
      throw new Error("Cannot finalize before starting.");
    if (this.state === "canceled")
      throw new Error("Cannot finalize after canceling.");
    return this._finalizePromise ? (console.warn("Output has already been finalized."), this._finalizePromise) : this._finalizePromise = (async () => {
      this.state = "finalizing";
      const t = await this._mutex.acquire(), e = this._tracks.map((i) => i.source._flushOrWaitForClose());
      await Promise.all(e), await this._muxer.finalize(), await this._writer.flush(), await this._writer.finalize(), this.state = "finalized", t();
    })();
  }
}, ji = class {
  constructor() {
    this._sizePromise = null, this.onread = null;
  }
  /**
   * Resolves with the total size of the file in bytes. This function is memoized, meaning only the first call
   * will retrieve the size.
   */
  getSize() {
    return this._sizePromise ??= this._retrieveSize();
  }
}, ks = class extends ji {
  constructor(t) {
    if (!(t instanceof Blob))
      throw new TypeError("blob must be a Blob.");
    super(), this._blob = t;
  }
  /** @internal */
  async _read(t, e) {
    const s = await this._blob.slice(t, e).arrayBuffer();
    return new Uint8Array(s);
  }
  /** @internal */
  async _retrieveSize() {
    return this._blob.size;
  }
}, pc = class extends ji {
  constructor(t, e = {}) {
    if (typeof t != "string" && !(t instanceof URL))
      throw new TypeError("url must be a string or URL.");
    if (!e || typeof e != "object")
      throw new TypeError("options must be an object.");
    if (e.requestInit !== void 0 && (!e.requestInit || typeof e.requestInit != "object"))
      throw new TypeError("options.requestInit, when provided, must be an object.");
    if (e.getRetryDelay !== void 0 && typeof e.getRetryDelay != "function")
      throw new TypeError("options.getRetryDelay, when provided, must be a function.");
    super(), this._fullData = null, this._url = t, this._options = e;
  }
  /** @internal */
  async _makeRequest(t) {
    const e = {};
    t && (e.Range = `bytes=${t.start}-${t.end - 1}`);
    const i = await ts(
      this._url,
      oi(this._options.requestInit ?? {}, {
        method: "GET",
        headers: e
      }),
      this._options.getRetryDelay ?? (() => null)
    );
    if (!i.ok)
      throw new Error(`Error fetching ${this._url}: ${i.status} ${i.statusText}`);
    const s = await i.arrayBuffer();
    return t || (this._fullData = s), {
      response: s,
      statusCode: i.status
    };
  }
  /** @internal */
  async _read(t, e) {
    if (this._fullData)
      return new Uint8Array(this._fullData, t, e - t);
    const { response: i, statusCode: s } = await this._makeRequest({ start: t, end: e });
    return s === 200 ? new Uint8Array(i).subarray(t, e) : new Uint8Array(i);
  }
  /** @internal */
  async _retrieveSize() {
    if (this._fullData)
      return this._fullData.byteLength;
    const t = await ts(
      this._url,
      oi(this._options.requestInit ?? {}, {
        method: "GET",
        headers: { Range: "bytes=0-0" }
      }),
      this._options.getRetryDelay ?? (() => null)
    );
    if (t.status === 206) {
      const i = t.headers.get("Content-Range");
      if (i) {
        const s = i.match(/bytes \d+-\d+\/(\d+)/);
        if (s && s[1])
          return parseInt(s[1], 10);
      }
    }
    const { response: e } = await this._makeRequest();
    return e.byteLength;
  }
}, xt = 16, Ti = class {
  constructor(t) {
    this.reader = t, this.pos = 0;
  }
  readBytes(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    return this.pos += t, new Uint8Array(e.buffer, i, t);
  }
  readU8() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 1);
    return this.pos++, t.getUint8(e);
  }
  readU16() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 2);
    return this.pos += 2, t.getUint16(e, !1);
  }
  readI16() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 2);
    return this.pos += 2, t.getInt16(e, !1);
  }
  readU24() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 3);
    this.pos += 3;
    const i = t.getUint16(e, !1), s = t.getUint8(e + 2);
    return i * 256 + s;
  }
  readU32() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 4);
    return this.pos += 4, t.getUint32(e, !1);
  }
  readI32() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 4);
    return this.pos += 4, t.getInt32(e, !1);
  }
  readU64() {
    const t = this.readU32(), e = this.readU32();
    return t * 4294967296 + e;
  }
  readI64() {
    const t = this.readI32(), e = this.readU32();
    return t * 4294967296 + e;
  }
  readF64() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 8);
    return this.pos += 8, t.getFloat64(e, !1);
  }
  readFixed_16_16() {
    return this.readI32() / 65536;
  }
  readFixed_2_30() {
    return this.readI32() / 1073741824;
  }
  readAscii(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    this.pos += t;
    let s = "";
    for (let r = 0; r < t; r++)
      s += String.fromCharCode(e.getUint8(i + r));
    return s;
  }
  readIsomVariableInteger() {
    let t = 0;
    for (let e = 0; e < 4; e++) {
      t <<= 7;
      const i = this.readU8();
      if (t |= i & 127, !(i & 128))
        break;
    }
    return t;
  }
  readBoxHeader() {
    let t = this.readU32();
    const e = this.readAscii(4);
    let i = 8;
    return t === 1 && (t = this.readU64(), i = 16), { name: e, totalSize: t, headerSize: i, contentSize: t - i };
  }
}, gc = class extends mt {
  constructor(t) {
    super(t), this.currentTrack = null, this.tracks = [], this.metadataPromise = null, this.movieTimescale = -1, this.movieDurationInTimescale = -1, this.isQuickTime = !1, this.isFragmented = !1, this.fragmentTrackDefaults = [], this.fragments = [], this.currentFragment = null, this.fragmentLookupMutex = new ft(), this.metadataReader = new Ti(t._mainReader), this.chunkReader = new Ti(new gt(t.source, 64 * 2 ** 20));
  }
  async computeDuration() {
    const t = await this.getTracks(), e = await Promise.all(t.map((i) => i.computeDuration()));
    return Math.max(0, ...e);
  }
  async getTracks() {
    return await this.readMetadata(), this.tracks.map((t) => t.inputTrack);
  }
  async getMimeType() {
    await this.readMetadata();
    let e = (this.tracks.some((i) => i.info?.type === "video") ? "video/" : this.tracks.some((i) => i.info?.type === "audio") ? "audio/" : "application/") + (this.isQuickTime ? "quicktime" : "mp4");
    if (this.tracks.length > 0) {
      const i = await Promise.all(this.tracks.map((r) => r.inputTrack.getCodecParameterString())), s = [...new Set(i.filter(Boolean))];
      e += `; codecs="${s.join(", ")}"`;
    }
    return e;
  }
  readMetadata() {
    return this.metadataPromise ??= (async () => {
      const t = await this.metadataReader.reader.source.getSize();
      for (; this.metadataReader.pos < t; ) {
        await this.metadataReader.reader.loadRange(
          this.metadataReader.pos,
          this.metadataReader.pos + xt
        );
        const e = this.metadataReader.pos, i = this.metadataReader.readBoxHeader();
        if (i.name === "ftyp") {
          const s = this.metadataReader.readAscii(4);
          this.isQuickTime = s === "qt  ";
        } else if (i.name === "moov") {
          await this.metadataReader.reader.loadRange(
            this.metadataReader.pos,
            this.metadataReader.pos + i.contentSize
          ), this.readContiguousBoxes(i.contentSize);
          for (const s of this.tracks) {
            const r = s.editListPreviousSegmentDurations / this.movieTimescale;
            s.editListOffset -= Math.round(r * s.timescale);
          }
          break;
        }
        this.metadataReader.pos = e + i.totalSize;
      }
      if (this.isFragmented) {
        await this.metadataReader.reader.loadRange(t - 4, t), this.metadataReader.pos = t - 4;
        const e = this.metadataReader.readU32(), i = t - e;
        if (i >= 0 && i < t) {
          await this.metadataReader.reader.loadRange(i, t), this.metadataReader.pos = i;
          const s = this.metadataReader.readBoxHeader();
          s.name === "mfra" && this.readContiguousBoxes(s.contentSize);
        }
      }
    })();
  }
  getSampleTableForTrack(t) {
    if (t.sampleTable)
      return t.sampleTable;
    const e = {
      sampleTimingEntries: [],
      sampleCompositionTimeOffsets: [],
      sampleSizes: [],
      keySampleIndices: null,
      chunkOffsets: [],
      sampleToChunk: [],
      presentationTimestamps: null,
      presentationTimestampIndexMap: null
    };
    if (t.sampleTable = e, this.metadataReader.pos = t.sampleTableByteOffset, this.currentTrack = t, this.traverseBox(), this.currentTrack = null, t.info?.type === "audio" && t.info.codec && K.includes(t.info.codec) && e.sampleCompositionTimeOffsets.length === 0) {
      const s = [], r = [];
      for (let a = 0; a < e.sampleToChunk.length; a++) {
        const n = e.sampleToChunk[a], o = e.sampleToChunk[a + 1], c = (o ? o.startChunkIndex : e.chunkOffsets.length) - n.startChunkIndex;
        for (let l = 0; l < c; l++) {
          const d = n.startSampleIndex + l * n.samplesPerChunk, h = d + n.samplesPerChunk, u = N(
            e.sampleTimingEntries,
            d,
            (y) => y.startIndex
          ), m = e.sampleTimingEntries[u], p = N(
            e.sampleTimingEntries,
            h,
            (y) => y.startIndex
          ), g = e.sampleTimingEntries[p], w = m.startDecodeTimestamp + (d - m.startIndex) * m.delta, x = g.startDecodeTimestamp + (h - g.startIndex) * g.delta - w, S = H(s);
          S && S.delta === x ? S.count++ : s.push({
            startIndex: n.startChunkIndex + l,
            startDecodeTimestamp: w,
            count: 1,
            delta: x
          });
          let k = 0;
          if (e.sampleSizes.length === 1)
            k = e.sampleSizes[0] * n.samplesPerChunk;
          else
            for (let y = d; y < h; y++)
              k += e.sampleSizes[y];
          r.push(k);
        }
        n.startSampleIndex = n.startChunkIndex, n.samplesPerChunk = 1;
      }
      e.sampleTimingEntries = s, e.sampleSizes = r;
    }
    if (e.sampleCompositionTimeOffsets.length > 0) {
      e.presentationTimestamps = [];
      for (const s of e.sampleTimingEntries)
        for (let r = 0; r < s.count; r++)
          e.presentationTimestamps.push({
            presentationTimestamp: s.startDecodeTimestamp + r * s.delta,
            sampleIndex: s.startIndex + r
          });
      for (const s of e.sampleCompositionTimeOffsets)
        for (let r = 0; r < s.count; r++) {
          const a = s.startIndex + r, n = e.presentationTimestamps[a];
          n && (n.presentationTimestamp += s.offset);
        }
      e.presentationTimestamps.sort((s, r) => s.presentationTimestamp - r.presentationTimestamp), e.presentationTimestampIndexMap = Array(e.presentationTimestamps.length).fill(-1);
      for (let s = 0; s < e.presentationTimestamps.length; s++)
        e.presentationTimestampIndexMap[e.presentationTimestamps[s].sampleIndex] = s;
    }
    return e;
  }
  async readFragment() {
    const t = this.metadataReader.pos;
    await this.metadataReader.reader.loadRange(
      this.metadataReader.pos,
      this.metadataReader.pos + xt
    );
    const e = this.metadataReader.readBoxHeader();
    f(e.name === "moof");
    const i = this.metadataReader.pos;
    await this.metadataReader.reader.loadRange(i, i + e.contentSize), this.metadataReader.pos = t, this.traverseBox();
    const s = X(this.fragments, t, (a) => a.moofOffset);
    f(s !== -1);
    const r = this.fragments[s];
    f(r.moofOffset === t), this.metadataReader.reader.forgetRange(i, i + e.contentSize);
    for (const [a, n] of r.trackData) {
      if (n.startTimestampIsFinal)
        continue;
      const o = this.tracks.find((u) => u.id === a);
      this.metadataReader.pos = 0;
      let c = null, l = null;
      const d = N(
        o.fragments,
        t - 1,
        (u) => u.moofOffset
      );
      d !== -1 && (c = o.fragments[d], l = c, this.metadataReader.pos = c.moofOffset + c.moofSize);
      let h = this.metadataReader.pos === 0;
      for (; this.metadataReader.pos < t; ) {
        if (c?.nextFragment)
          c = c.nextFragment, this.metadataReader.pos = c.moofOffset + c.moofSize;
        else {
          await this.metadataReader.reader.loadRange(
            this.metadataReader.pos,
            this.metadataReader.pos + xt
          );
          const u = this.metadataReader.pos, m = this.metadataReader.readBoxHeader();
          if (m.name === "moof") {
            const p = X(this.fragments, u, (w) => w.moofOffset);
            let g;
            p === -1 ? (this.metadataReader.pos = u, g = await this.readFragment()) : g = this.fragments[p], c && (c.nextFragment = g), c = g, h && (g.isKnownToBeFirstFragment = !0, h = !1);
          }
          this.metadataReader.pos = u + m.totalSize;
        }
        c && c.trackData.has(a) && (l = c);
      }
      if (l) {
        const u = l.trackData.get(a);
        f(u.startTimestampIsFinal), vs(n, u.endTimestamp);
      }
      n.startTimestampIsFinal = !0;
    }
    return r;
  }
  readContiguousBoxes(t) {
    const e = this.metadataReader.pos;
    for (; this.metadataReader.pos - e < t; )
      this.traverseBox();
  }
  traverseBox() {
    const t = this.metadataReader.pos, e = this.metadataReader.readBoxHeader(), i = t + e.totalSize;
    switch (e.name) {
      case "mdia":
      case "minf":
      case "dinf":
      case "mfra":
      case "edts":
        this.readContiguousBoxes(e.contentSize);
        break;
      case "mvhd":
        {
          const s = this.metadataReader.readU8();
          this.metadataReader.pos += 3, s === 1 ? (this.metadataReader.pos += 16, this.movieTimescale = this.metadataReader.readU32(), this.movieDurationInTimescale = this.metadataReader.readU64()) : (this.metadataReader.pos += 8, this.movieTimescale = this.metadataReader.readU32(), this.movieDurationInTimescale = this.metadataReader.readU32());
        }
        break;
      case "trak":
        {
          const s = {
            id: -1,
            demuxer: this,
            inputTrack: null,
            info: null,
            timescale: -1,
            durationInMovieTimescale: -1,
            durationInMediaTimescale: -1,
            rotation: 0,
            languageCode: fe,
            sampleTableByteOffset: -1,
            sampleTable: null,
            fragmentLookupTable: null,
            currentFragmentState: null,
            fragments: [],
            editListPreviousSegmentDurations: 0,
            editListOffset: 0
          };
          if (this.currentTrack = s, this.readContiguousBoxes(e.contentSize), s.id !== -1 && s.timescale !== -1 && s.info !== null) {
            if (s.info.type === "video" && s.info.width !== -1) {
              const r = s;
              s.inputTrack = new pt(new wc(r)), this.tracks.push(s);
            } else if (s.info.type === "audio" && s.info.numberOfChannels !== -1) {
              const r = s;
              s.inputTrack = new xe(new yc(r)), this.tracks.push(s);
            }
          }
          this.currentTrack = null;
        }
        break;
      case "tkhd":
        {
          const s = this.currentTrack;
          f(s);
          const r = this.metadataReader.readU8();
          if (!((this.metadataReader.readU24() & 1) !== 0))
            break;
          if (r === 0)
            this.metadataReader.pos += 8, s.id = this.metadataReader.readU32(), this.metadataReader.pos += 4, s.durationInMovieTimescale = this.metadataReader.readU32();
          else if (r === 1)
            this.metadataReader.pos += 16, s.id = this.metadataReader.readU32(), this.metadataReader.pos += 4, s.durationInMovieTimescale = this.metadataReader.readU64();
          else
            throw new Error(`Incorrect track header version ${r}.`);
          this.metadataReader.pos += 2 * 4 + 2 + 2 + 2 + 2;
          const o = [
            this.metadataReader.readFixed_16_16(),
            this.metadataReader.readFixed_16_16(),
            this.metadataReader.readFixed_2_30(),
            this.metadataReader.readFixed_16_16(),
            this.metadataReader.readFixed_16_16(),
            this.metadataReader.readFixed_2_30(),
            this.metadataReader.readFixed_16_16(),
            this.metadataReader.readFixed_16_16(),
            this.metadataReader.readFixed_2_30()
          ], c = Fi(ni(vc(o), 90));
          f(c === 0 || c === 90 || c === 180 || c === 270), s.rotation = c;
        }
        break;
      case "elst":
        {
          const s = this.currentTrack;
          f(s);
          const r = this.metadataReader.readU8();
          this.metadataReader.pos += 3;
          let a = !1, n = 0;
          const o = this.metadataReader.readU32();
          for (let c = 0; c < o; c++) {
            const l = r === 1 ? this.metadataReader.readU64() : this.metadataReader.readU32(), d = r === 1 ? this.metadataReader.readI64() : this.metadataReader.readI32(), h = this.metadataReader.readFixed_16_16();
            if (l !== 0) {
              if (a)
                throw new Error("Unsupported edit list: multiple edits are not supported.");
              if (d === -1) {
                n += l;
                continue;
              }
              if (h !== 1)
                throw new Error("Unsupported edit list: media rate must be 1.");
              s.editListPreviousSegmentDurations = n, s.editListOffset = d, a = !0;
            }
          }
        }
        break;
      case "mdhd":
        {
          const s = this.currentTrack;
          f(s);
          const r = this.metadataReader.readU8();
          this.metadataReader.pos += 3, r === 0 ? (this.metadataReader.pos += 8, s.timescale = this.metadataReader.readU32(), s.durationInMediaTimescale = this.metadataReader.readU32()) : r === 1 && (this.metadataReader.pos += 16, s.timescale = this.metadataReader.readU32(), s.durationInMediaTimescale = this.metadataReader.readU64());
          let a = this.metadataReader.readU16();
          if (a > 0) {
            s.languageCode = "";
            for (let n = 0; n < 3; n++)
              s.languageCode = String.fromCharCode(96 + (a & 31)) + s.languageCode, a >>= 5;
            Oi(s.languageCode) || (s.languageCode = fe);
          }
        }
        break;
      case "hdlr":
        {
          const s = this.currentTrack;
          f(s), this.metadataReader.pos += 8;
          const r = this.metadataReader.readAscii(4);
          r === "vide" ? s.info = {
            type: "video",
            width: -1,
            height: -1,
            codec: null,
            codecDescription: null,
            colorSpace: null,
            avcCodecInfo: null,
            hevcCodecInfo: null,
            vp9CodecInfo: null,
            av1CodecInfo: null
          } : r === "soun" && (s.info = {
            type: "audio",
            numberOfChannels: -1,
            sampleRate: -1,
            codec: null,
            codecDescription: null,
            aacCodecInfo: null
          });
        }
        break;
      case "stbl":
        {
          const s = this.currentTrack;
          f(s), s.sampleTableByteOffset = t, this.readContiguousBoxes(e.contentSize);
        }
        break;
      case "stsd":
        {
          const s = this.currentTrack;
          if (f(s), s.info === null || s.sampleTable)
            break;
          const r = this.metadataReader.readU8();
          this.metadataReader.pos += 3;
          const a = this.metadataReader.readU32();
          for (let n = 0; n < a; n++) {
            const o = this.metadataReader.readBoxHeader(), c = o.name.toLowerCase();
            if (s.info.type === "video")
              c === "avc1" ? s.info.codec = "avc" : c === "hvc1" || c === "hev1" ? s.info.codec = "hevc" : c === "vp08" ? s.info.codec = "vp8" : c === "vp09" ? s.info.codec = "vp9" : c === "av01" ? s.info.codec = "av1" : console.warn(`Unsupported video codec (sample entry type '${o.name}').`), this.metadataReader.pos += 6 * 1 + 2 + 2 + 2 + 3 * 4, s.info.width = this.metadataReader.readU16(), s.info.height = this.metadataReader.readU16(), this.metadataReader.pos += 50, this.readContiguousBoxes(t + o.totalSize - this.metadataReader.pos);
            else {
              c === "mp4a" || (c === "opus" ? s.info.codec = "opus" : c === "flac" ? s.info.codec = "flac" : c === "twos" || c === "sowt" || c === "raw " || c === "in24" || c === "in32" || c === "fl32" || c === "lpcm" || (c === "ulaw" ? s.info.codec = "ulaw" : c === "alaw" ? s.info.codec = "alaw" : console.warn(`Unsupported audio codec (sample entry type '${o.name}').`))), this.metadataReader.pos += 6 * 1 + 2;
              const l = this.metadataReader.readU16();
              this.metadataReader.pos += 3 * 2;
              let d = this.metadataReader.readU16(), h = this.metadataReader.readU16();
              this.metadataReader.pos += 2 * 2;
              let u = this.metadataReader.readU32() / 65536;
              if (r === 0 && l > 0) {
                if (l === 1)
                  this.metadataReader.pos += 4, h = 8 * this.metadataReader.readU32(), this.metadataReader.pos += 2 * 4;
                else if (l === 2) {
                  this.metadataReader.pos += 4, u = this.metadataReader.readF64(), d = this.metadataReader.readU32(), this.metadataReader.pos += 4, h = this.metadataReader.readU32();
                  const m = this.metadataReader.readU32();
                  if (this.metadataReader.pos += 2 * 4, c === "lpcm") {
                    const p = h + 7 >> 3, g = !!(m & 1), w = !!(m & 2), b = m & 4 ? -1 : 0;
                    h > 0 && h <= 64 && (g ? h === 32 && (s.info.codec = w ? "pcm-f32be" : "pcm-f32") : b & 1 << p - 1 ? p === 1 ? s.info.codec = "pcm-s8" : p === 2 ? s.info.codec = w ? "pcm-s16be" : "pcm-s16" : p === 3 ? s.info.codec = w ? "pcm-s24be" : "pcm-s24" : p === 4 && (s.info.codec = w ? "pcm-s32be" : "pcm-s32") : p === 1 && (s.info.codec = "pcm-u8")), s.info.codec === null && console.warn("Unsupported PCM format.");
                  }
                }
              }
              if (s.info.numberOfChannels = d, s.info.sampleRate = u, c === "twos")
                if (h === 8)
                  s.info.codec = "pcm-s8";
                else if (h === 16)
                  s.info.codec = "pcm-s16be";
                else
                  throw new Error(`Unsupported sample size ${h} for codec 'twos'.`);
              else if (c === "sowt")
                if (h === 8)
                  s.info.codec = "pcm-s8";
                else if (h === 16)
                  s.info.codec = "pcm-s16";
                else
                  throw new Error(`Unsupported sample size ${h} for codec 'sowt'.`);
              else c === "raw " ? s.info.codec = "pcm-u8" : c === "in24" ? s.info.codec = "pcm-s24be" : c === "in32" ? s.info.codec = "pcm-s32be" : c === "fl32" && (s.info.codec = "pcm-f32be");
              this.readContiguousBoxes(t + o.totalSize - this.metadataReader.pos);
            }
          }
        }
        break;
      case "avcC":
        {
          const s = this.currentTrack;
          f(s && s.info), s.info.codecDescription = this.metadataReader.readBytes(e.contentSize);
        }
        break;
      case "hvcC":
        {
          const s = this.currentTrack;
          f(s && s.info), s.info.codecDescription = this.metadataReader.readBytes(e.contentSize);
        }
        break;
      case "vpcC":
        {
          const s = this.currentTrack;
          f(s && s.info?.type === "video"), this.metadataReader.pos += 4;
          const r = this.metadataReader.readU8(), a = this.metadataReader.readU8(), n = this.metadataReader.readU8(), o = n >> 4, c = n >> 1 & 7, l = n & 1, d = this.metadataReader.readU8(), h = this.metadataReader.readU8(), u = this.metadataReader.readU8();
          s.info.vp9CodecInfo = {
            profile: r,
            level: a,
            bitDepth: o,
            chromaSubsampling: c,
            videoFullRangeFlag: l,
            colourPrimaries: d,
            transferCharacteristics: h,
            matrixCoefficients: u
          };
        }
        break;
      case "av1C":
        {
          const s = this.currentTrack;
          f(s && s.info?.type === "video"), this.metadataReader.pos += 1;
          const r = this.metadataReader.readU8(), a = r >> 5, n = r & 31, o = this.metadataReader.readU8(), c = o >> 7, l = o >> 6 & 1, d = o >> 5 & 1, h = o >> 4 & 1, u = o >> 3 & 1, m = o >> 2 & 1, p = o & 3, g = a == 2 && l ? d ? 12 : 10 : l ? 10 : 8;
          s.info.av1CodecInfo = {
            profile: a,
            level: n,
            tier: c,
            bitDepth: g,
            monochrome: h,
            chromaSubsamplingX: u,
            chromaSubsamplingY: m,
            chromaSamplePosition: p
          };
        }
        break;
      case "colr":
        {
          const s = this.currentTrack;
          if (f(s && s.info?.type === "video"), this.metadataReader.readAscii(4) !== "nclx")
            break;
          const a = this.metadataReader.readU16(), n = this.metadataReader.readU16(), o = this.metadataReader.readU16(), c = !!(this.metadataReader.readU8() & 128);
          s.info.colorSpace = {
            primaries: Ks[a],
            transfer: Gs[n],
            matrix: Ys[o],
            fullRange: c
          };
        }
        break;
      case "wave":
        e.totalSize > 8 && this.readContiguousBoxes(e.contentSize);
        break;
      case "esds":
        {
          const s = this.currentTrack;
          f(s && s.info?.type === "audio"), this.metadataReader.pos += 4;
          const r = this.metadataReader.readU8();
          f(r === 3), this.metadataReader.readIsomVariableInteger(), this.metadataReader.pos += 2;
          const a = this.metadataReader.readU8(), n = (a & 128) !== 0, o = (a & 64) !== 0, c = (a & 32) !== 0;
          if (n && (this.metadataReader.pos += 2), o) {
            const m = this.metadataReader.readU8();
            this.metadataReader.pos += m;
          }
          c && (this.metadataReader.pos += 2);
          const l = this.metadataReader.readU8();
          f(l === 4);
          const d = this.metadataReader.readIsomVariableInteger(), h = this.metadataReader.pos, u = this.metadataReader.readU8();
          if (u === 64 || u === 103 ? (s.info.codec = "aac", s.info.aacCodecInfo = { isMpeg2: u === 103 }) : u === 105 || u === 107 ? s.info.codec = "mp3" : u === 221 ? s.info.codec = "vorbis" : console.warn(
            `Unsupported audio codec (objectTypeIndication ${u}) - discarding track.`
          ), this.metadataReader.pos += 12, d > this.metadataReader.pos - h) {
            const m = this.metadataReader.readU8();
            f(m === 5);
            const p = this.metadataReader.readIsomVariableInteger();
            if (s.info.codecDescription = this.metadataReader.readBytes(p), s.info.codec === "aac") {
              const g = lr(s.info.codecDescription);
              g.numberOfChannels !== null && (s.info.numberOfChannels = g.numberOfChannels), g.sampleRate !== null && (s.info.sampleRate = g.sampleRate);
            }
          }
        }
        break;
      case "enda":
        {
          const s = this.currentTrack;
          f(s && s.info?.type === "audio"), this.metadataReader.readU16() & 255 && (s.info.codec === "pcm-s16be" ? s.info.codec = "pcm-s16" : s.info.codec === "pcm-s24be" ? s.info.codec = "pcm-s24" : s.info.codec === "pcm-s32be" ? s.info.codec = "pcm-s32" : s.info.codec === "pcm-f32be" && (s.info.codec = "pcm-f32"));
        }
        break;
      case "dOps":
        {
          const s = this.currentTrack;
          f(s && s.info?.type === "audio"), this.metadataReader.pos += 1;
          const r = this.metadataReader.readU8(), a = this.metadataReader.readU16(), n = this.metadataReader.readU32(), o = this.metadataReader.readI16(), c = this.metadataReader.readU8();
          let l;
          c !== 0 ? l = this.metadataReader.readBytes(2 + r) : l = new Uint8Array(0);
          const d = new Uint8Array(19 + l.byteLength), h = new DataView(d.buffer);
          h.setUint32(0, 1332770163, !1), h.setUint32(4, 1214603620, !1), h.setUint8(8, 1), h.setUint8(9, r), h.setUint16(10, a, !0), h.setUint32(12, n, !0), h.setInt16(16, o, !0), h.setUint8(18, c), d.set(l, 19), s.info.codecDescription = d, s.info.numberOfChannels = r, s.info.sampleRate = n;
        }
        break;
      case "dfLa":
        {
          const s = this.currentTrack;
          f(s && s.info?.type === "audio"), this.metadataReader.pos += 4;
          const r = 127, a = 128, n = this.metadataReader.pos;
          for (; this.metadataReader.pos < i; ) {
            const h = this.metadataReader.readU8(), u = this.metadataReader.readU24();
            if ((h & r) === 0) {
              this.metadataReader.pos += 10;
              const p = this.metadataReader.readU32(), g = p >>> 12, w = (p >> 9 & 7) + 1;
              s.info.sampleRate = g, s.info.numberOfChannels = w, this.metadataReader.pos += 20;
            } else
              this.metadataReader.pos += u;
            if (h & a)
              break;
          }
          const o = this.metadataReader.pos;
          this.metadataReader.pos = n;
          const c = this.metadataReader.readBytes(o - n), l = new Uint8Array(4 + c.byteLength);
          new DataView(l.buffer).setUint32(0, 1716281667, !1), l.set(c, 4), s.info.codecDescription = l;
        }
        break;
      case "stts":
        {
          const s = this.currentTrack;
          if (f(s), !s.sampleTable)
            break;
          this.metadataReader.pos += 4;
          const r = this.metadataReader.readU32();
          let a = 0, n = 0;
          for (let o = 0; o < r; o++) {
            const c = this.metadataReader.readU32(), l = this.metadataReader.readU32();
            s.sampleTable.sampleTimingEntries.push({
              startIndex: a,
              startDecodeTimestamp: n,
              count: c,
              delta: l
            }), a += c, n += c * l;
          }
        }
        break;
      case "ctts":
        {
          const s = this.currentTrack;
          if (f(s), !s.sampleTable)
            break;
          this.metadataReader.pos += 4;
          const r = this.metadataReader.readU32();
          let a = 0;
          for (let n = 0; n < r; n++) {
            const o = this.metadataReader.readU32(), c = this.metadataReader.readI32();
            s.sampleTable.sampleCompositionTimeOffsets.push({
              startIndex: a,
              count: o,
              offset: c
            }), a += o;
          }
        }
        break;
      case "stsz":
        {
          const s = this.currentTrack;
          if (f(s), !s.sampleTable)
            break;
          this.metadataReader.pos += 4;
          const r = this.metadataReader.readU32(), a = this.metadataReader.readU32();
          if (r === 0)
            for (let n = 0; n < a; n++) {
              const o = this.metadataReader.readU32();
              s.sampleTable.sampleSizes.push(o);
            }
          else
            s.sampleTable.sampleSizes.push(r);
        }
        break;
      case "stz2":
        throw new Error("Unsupported.");
      case "stss":
        {
          const s = this.currentTrack;
          if (f(s), !s.sampleTable)
            break;
          this.metadataReader.pos += 4, s.sampleTable.keySampleIndices = [];
          const r = this.metadataReader.readU32();
          for (let a = 0; a < r; a++) {
            const n = this.metadataReader.readU32() - 1;
            s.sampleTable.keySampleIndices.push(n);
          }
        }
        break;
      case "stsc":
        {
          const s = this.currentTrack;
          if (f(s), !s.sampleTable)
            break;
          this.metadataReader.pos += 4;
          const r = this.metadataReader.readU32();
          for (let n = 0; n < r; n++) {
            const o = this.metadataReader.readU32() - 1, c = this.metadataReader.readU32(), l = this.metadataReader.readU32();
            s.sampleTable.sampleToChunk.push({
              startSampleIndex: -1,
              startChunkIndex: o,
              samplesPerChunk: c,
              sampleDescriptionIndex: l
            });
          }
          let a = 0;
          for (let n = 0; n < s.sampleTable.sampleToChunk.length; n++)
            if (s.sampleTable.sampleToChunk[n].startSampleIndex = a, n < s.sampleTable.sampleToChunk.length - 1) {
              const c = s.sampleTable.sampleToChunk[n + 1].startChunkIndex - s.sampleTable.sampleToChunk[n].startChunkIndex;
              a += c * s.sampleTable.sampleToChunk[n].samplesPerChunk;
            }
        }
        break;
      case "stco":
        {
          const s = this.currentTrack;
          if (f(s), !s.sampleTable)
            break;
          this.metadataReader.pos += 4;
          const r = this.metadataReader.readU32();
          for (let a = 0; a < r; a++) {
            const n = this.metadataReader.readU32();
            s.sampleTable.chunkOffsets.push(n);
          }
        }
        break;
      case "co64":
        {
          const s = this.currentTrack;
          if (f(s), !s.sampleTable)
            break;
          this.metadataReader.pos += 4;
          const r = this.metadataReader.readU32();
          for (let a = 0; a < r; a++) {
            const n = this.metadataReader.readU64();
            s.sampleTable.chunkOffsets.push(n);
          }
        }
        break;
      case "mvex":
        this.isFragmented = !0, this.readContiguousBoxes(e.contentSize);
        break;
      case "mehd":
        {
          const s = this.metadataReader.readU8();
          this.metadataReader.pos += 3;
          const r = s === 1 ? this.metadataReader.readU64() : this.metadataReader.readU32();
          this.movieDurationInTimescale = r;
        }
        break;
      case "trex":
        {
          this.metadataReader.pos += 4;
          const s = this.metadataReader.readU32(), r = this.metadataReader.readU32(), a = this.metadataReader.readU32(), n = this.metadataReader.readU32(), o = this.metadataReader.readU32();
          this.fragmentTrackDefaults.push({
            trackId: s,
            defaultSampleDescriptionIndex: r,
            defaultSampleDuration: a,
            defaultSampleSize: n,
            defaultSampleFlags: o
          });
        }
        break;
      case "tfra":
        {
          const s = this.metadataReader.readU8();
          this.metadataReader.pos += 3;
          const r = this.metadataReader.readU32(), a = this.tracks.find((w) => w.id === r);
          if (!a)
            break;
          a.fragmentLookupTable = [];
          const n = this.metadataReader.readU32(), o = (n & 48) >> 4, c = (n & 12) >> 2, l = n & 3, d = this.metadataReader, h = [d.readU8.bind(d), d.readU16.bind(d), d.readU24.bind(d), d.readU32.bind(d)], u = h[o], m = h[c], p = h[l], g = this.metadataReader.readU32();
          for (let w = 0; w < g; w++) {
            const b = s === 1 ? this.metadataReader.readU64() : this.metadataReader.readU32(), x = s === 1 ? this.metadataReader.readU64() : this.metadataReader.readU32();
            u(), m(), p(), a.fragmentLookupTable.push({
              timestamp: b,
              moofOffset: x
            });
          }
        }
        break;
      case "moof":
        {
          this.currentFragment = {
            moofOffset: t,
            moofSize: e.totalSize,
            implicitBaseDataOffset: t,
            trackData: /* @__PURE__ */ new Map(),
            dataStart: 1 / 0,
            dataEnd: 0,
            nextFragment: null,
            isKnownToBeFirstFragment: !1
          }, this.readContiguousBoxes(e.contentSize);
          const s = N(
            this.fragments,
            this.currentFragment.moofOffset,
            (r) => r.moofOffset
          );
          this.fragments.splice(s + 1, 0, this.currentFragment);
          for (const [, r] of this.currentFragment.trackData) {
            const a = r.samples[0], n = H(r.samples);
            this.currentFragment.dataStart = Math.min(
              this.currentFragment.dataStart,
              a.byteOffset
            ), this.currentFragment.dataEnd = Math.max(
              this.currentFragment.dataEnd,
              n.byteOffset + n.byteSize
            );
          }
          this.currentFragment = null;
        }
        break;
      case "traf":
        if (f(this.currentFragment), this.readContiguousBoxes(e.contentSize), this.currentTrack) {
          const s = this.currentFragment.trackData.get(this.currentTrack.id);
          if (s) {
            const r = N(
              this.currentTrack.fragments,
              this.currentFragment.moofOffset,
              (n) => n.moofOffset
            );
            this.currentTrack.fragments.splice(r + 1, 0, this.currentFragment);
            const { currentFragmentState: a } = this.currentTrack;
            f(a), a.startTimestamp !== null && (vs(s, a.startTimestamp), s.startTimestampIsFinal = !0);
          }
          this.currentTrack.currentFragmentState = null, this.currentTrack = null;
        }
        break;
      case "tfhd":
        {
          f(this.currentFragment), this.metadataReader.pos += 1;
          const s = this.metadataReader.readU24(), r = !!(s & 1), a = !!(s & 2), n = !!(s & 8), o = !!(s & 16), c = !!(s & 32), l = !!(s & 65536), d = !!(s & 131072), h = this.metadataReader.readU32(), u = this.tracks.find((p) => p.id === h);
          if (!u)
            break;
          const m = this.fragmentTrackDefaults.find((p) => p.trackId === h);
          this.currentTrack = u, u.currentFragmentState = {
            baseDataOffset: this.currentFragment.implicitBaseDataOffset,
            sampleDescriptionIndex: m?.defaultSampleDescriptionIndex ?? null,
            defaultSampleDuration: m?.defaultSampleDuration ?? null,
            defaultSampleSize: m?.defaultSampleSize ?? null,
            defaultSampleFlags: m?.defaultSampleFlags ?? null,
            startTimestamp: null
          }, r ? u.currentFragmentState.baseDataOffset = this.metadataReader.readU64() : d && (u.currentFragmentState.baseDataOffset = this.currentFragment.moofOffset), a && (u.currentFragmentState.sampleDescriptionIndex = this.metadataReader.readU32()), n && (u.currentFragmentState.defaultSampleDuration = this.metadataReader.readU32()), o && (u.currentFragmentState.defaultSampleSize = this.metadataReader.readU32()), c && (u.currentFragmentState.defaultSampleFlags = this.metadataReader.readU32()), l && (u.currentFragmentState.defaultSampleDuration = 0);
        }
        break;
      case "tfdt":
        {
          const s = this.currentTrack;
          if (!s)
            break;
          f(s.currentFragmentState);
          const r = this.metadataReader.readU8();
          this.metadataReader.pos += 3;
          const a = r === 0 ? this.metadataReader.readU32() : this.metadataReader.readU64();
          s.currentFragmentState.startTimestamp = a;
        }
        break;
      case "trun":
        {
          const s = this.currentTrack;
          if (!s)
            break;
          if (f(this.currentFragment), f(s.currentFragmentState), this.currentFragment.trackData.has(s.id))
            throw new Error("Can't have two trun boxes for the same track in one fragment.");
          const r = this.metadataReader.readU8(), a = this.metadataReader.readU24(), n = !!(a & 1), o = !!(a & 4), c = !!(a & 256), l = !!(a & 512), d = !!(a & 1024), h = !!(a & 2048), u = this.metadataReader.readU32();
          let m = s.currentFragmentState.baseDataOffset;
          n && (m += this.metadataReader.readI32());
          let p = null;
          o && (p = this.metadataReader.readU32());
          let g = m;
          if (u === 0) {
            this.currentFragment.implicitBaseDataOffset = g;
            break;
          }
          let w = 0;
          const b = {
            startTimestamp: 0,
            endTimestamp: 0,
            samples: [],
            presentationTimestamps: [],
            startTimestampIsFinal: !1
          };
          this.currentFragment.trackData.set(s.id, b);
          for (let k = 0; k < u; k++) {
            let y;
            c ? y = this.metadataReader.readU32() : (f(s.currentFragmentState.defaultSampleDuration !== null), y = s.currentFragmentState.defaultSampleDuration);
            let T;
            l ? T = this.metadataReader.readU32() : (f(s.currentFragmentState.defaultSampleSize !== null), T = s.currentFragmentState.defaultSampleSize);
            let P;
            d ? P = this.metadataReader.readU32() : (f(s.currentFragmentState.defaultSampleFlags !== null), P = s.currentFragmentState.defaultSampleFlags), k === 0 && p !== null && (P = p);
            let F = 0;
            h && (r === 0 ? F = this.metadataReader.readU32() : F = this.metadataReader.readI32());
            const z = !(P & 65536);
            b.samples.push({
              presentationTimestamp: w + F,
              duration: y,
              byteOffset: g,
              byteSize: T,
              isKeyFrame: z
            }), g += T, w += y;
          }
          b.presentationTimestamps = b.samples.map((k, y) => ({ presentationTimestamp: k.presentationTimestamp, sampleIndex: y })).sort((k, y) => k.presentationTimestamp - y.presentationTimestamp);
          for (let k = 0; k < b.presentationTimestamps.length - 1; k++) {
            const y = b.presentationTimestamps[k], P = b.presentationTimestamps[k + 1].presentationTimestamp - y.presentationTimestamp;
            b.samples[y.sampleIndex].duration = P;
          }
          const x = b.samples[b.presentationTimestamps[0].sampleIndex], S = b.samples[H(b.presentationTimestamps).sampleIndex];
          b.startTimestamp = x.presentationTimestamp, b.endTimestamp = S.presentationTimestamp + S.duration, this.currentFragment.implicitBaseDataOffset = g;
        }
        break;
    }
    this.metadataReader.pos = i;
  }
}, Kr = class {
  constructor(t) {
    this.internalTrack = t, this.packetToSampleIndex = /* @__PURE__ */ new WeakMap(), this.packetToFragmentLocation = /* @__PURE__ */ new WeakMap();
  }
  getId() {
    return this.internalTrack.id;
  }
  getCodec() {
    throw new Error("Not implemented on base class.");
  }
  getLanguageCode() {
    return this.internalTrack.languageCode;
  }
  getTimeResolution() {
    return this.internalTrack.timescale;
  }
  async computeDuration() {
    const t = await this.getPacket(1 / 0, { metadataOnly: !0 });
    return (t?.timestamp ?? 0) + (t?.duration ?? 0);
  }
  async getFirstTimestamp() {
    return (await this.getFirstPacket({ metadataOnly: !0 }))?.timestamp ?? 0;
  }
  async getFirstPacket(t) {
    return this.internalTrack.demuxer.isFragmented ? this.performFragmentedLookup(
      () => {
        const e = this.internalTrack.demuxer.fragments[0] ?? null;
        if (e?.isKnownToBeFirstFragment) {
          let i = e;
          for (; i; ) {
            if (i.trackData.get(this.internalTrack.id))
              return {
                fragmentIndex: X(
                  this.internalTrack.fragments,
                  i.moofOffset,
                  (r) => r.moofOffset
                ),
                sampleIndex: 0,
                correctSampleFound: !0
              };
            i = i.nextFragment;
          }
        }
        return {
          fragmentIndex: -1,
          sampleIndex: -1,
          correctSampleFound: !1
        };
      },
      -1 / 0,
      // Use -Infinity as a search timestamp to avoid using the lookup entries
      1 / 0,
      t
    ) : this.fetchPacketForSampleIndex(0, t);
  }
  mapTimestampIntoTimescale(t) {
    return Bi(t * this.internalTrack.timescale, 14) + this.internalTrack.editListOffset;
  }
  async getPacket(t, e) {
    const i = this.mapTimestampIntoTimescale(t);
    if (this.internalTrack.demuxer.isFragmented)
      return this.performFragmentedLookup(
        () => this.findSampleInFragmentsForTimestamp(i),
        i,
        i,
        e
      );
    {
      const s = this.internalTrack.demuxer.getSampleTableForTrack(this.internalTrack), r = Ss(s, i);
      return this.fetchPacketForSampleIndex(r, e);
    }
  }
  async getNextPacket(t, e) {
    if (this.internalTrack.demuxer.isFragmented) {
      const s = this.packetToFragmentLocation.get(t);
      if (s === void 0)
        throw new Error("Packet was not created from this track.");
      const r = s.fragment.trackData.get(this.internalTrack.id), a = r.samples[s.sampleIndex], n = X(
        this.internalTrack.fragments,
        s.fragment.moofOffset,
        (o) => o.moofOffset
      );
      return f(n !== -1), this.performFragmentedLookup(
        () => {
          if (s.sampleIndex + 1 < r.samples.length)
            return {
              fragmentIndex: n,
              sampleIndex: s.sampleIndex + 1,
              correctSampleFound: !0
            };
          {
            let o = s.fragment;
            for (; o.nextFragment; )
              if (o = o.nextFragment, o.trackData.get(this.internalTrack.id)) {
                const l = X(
                  this.internalTrack.fragments,
                  o.moofOffset,
                  (d) => d.moofOffset
                );
                return f(l !== -1), {
                  fragmentIndex: l,
                  sampleIndex: 0,
                  correctSampleFound: !0
                };
              }
            return {
              fragmentIndex: n,
              sampleIndex: -1,
              correctSampleFound: !1
            };
          }
        },
        a.presentationTimestamp,
        1 / 0,
        e
      );
    }
    const i = this.packetToSampleIndex.get(t);
    if (i === void 0)
      throw new Error("Packet was not created from this track.");
    return this.fetchPacketForSampleIndex(i + 1, e);
  }
  async getKeyPacket(t, e) {
    const i = this.mapTimestampIntoTimescale(t);
    if (this.internalTrack.demuxer.isFragmented)
      return this.performFragmentedLookup(
        () => this.findKeySampleInFragmentsForTimestamp(i),
        i,
        i,
        e
      );
    const s = this.internalTrack.demuxer.getSampleTableForTrack(this.internalTrack), r = Ss(s, i), a = r === -1 ? -1 : kc(s, r);
    return this.fetchPacketForSampleIndex(a, e);
  }
  async getNextKeyPacket(t, e) {
    if (this.internalTrack.demuxer.isFragmented) {
      const a = this.packetToFragmentLocation.get(t);
      if (a === void 0)
        throw new Error("Packet was not created from this track.");
      const n = a.fragment.trackData.get(this.internalTrack.id), o = n.samples[a.sampleIndex], c = X(
        this.internalTrack.fragments,
        a.fragment.moofOffset,
        (l) => l.moofOffset
      );
      return f(c !== -1), this.performFragmentedLookup(
        () => {
          const l = n.samples.findIndex(
            (d, h) => d.isKeyFrame && h > a.sampleIndex
          );
          if (l !== -1)
            return {
              fragmentIndex: c,
              sampleIndex: l,
              correctSampleFound: !0
            };
          {
            let d = a.fragment;
            for (; d.nextFragment; ) {
              d = d.nextFragment;
              const h = d.trackData.get(this.internalTrack.id);
              if (h) {
                const u = X(
                  this.internalTrack.fragments,
                  d.moofOffset,
                  (p) => p.moofOffset
                );
                f(u !== -1);
                const m = h.samples.findIndex((p) => p.isKeyFrame);
                if (m === -1)
                  throw new Error("Not supported: Fragment does not contain key sample.");
                return {
                  fragmentIndex: u,
                  sampleIndex: m,
                  correctSampleFound: !0
                };
              }
            }
            return {
              fragmentIndex: c,
              sampleIndex: -1,
              correctSampleFound: !1
            };
          }
        },
        o.presentationTimestamp,
        1 / 0,
        e
      );
    }
    const i = this.packetToSampleIndex.get(t);
    if (i === void 0)
      throw new Error("Packet was not created from this track.");
    const s = this.internalTrack.demuxer.getSampleTableForTrack(this.internalTrack), r = Sc(s, i);
    return this.fetchPacketForSampleIndex(r, e);
  }
  async fetchPacketForSampleIndex(t, e) {
    if (t === -1)
      return null;
    const i = this.internalTrack.demuxer.getSampleTableForTrack(this.internalTrack), s = bc(i, t);
    if (!s)
      return null;
    let r;
    e.metadataOnly ? r = me : (await this.internalTrack.demuxer.chunkReader.reader.loadRange(
      s.chunkOffset,
      s.chunkOffset + s.chunkSize
    ), this.internalTrack.demuxer.chunkReader.pos = s.sampleOffset, r = this.internalTrack.demuxer.chunkReader.readBytes(s.sampleSize));
    const a = (s.presentationTimestamp - this.internalTrack.editListOffset) / this.internalTrack.timescale, n = s.duration / this.internalTrack.timescale, o = new G(
      r,
      s.isKeyFrame ? "key" : "delta",
      a,
      n,
      t,
      s.sampleSize
    );
    return this.packetToSampleIndex.set(o, t), o;
  }
  async fetchPacketInFragment(t, e, i) {
    if (e === -1)
      return null;
    const r = t.trackData.get(this.internalTrack.id).samples[e];
    f(r);
    let a;
    i.metadataOnly ? a = me : (await this.internalTrack.demuxer.chunkReader.reader.loadRange(t.dataStart, t.dataEnd), this.internalTrack.demuxer.chunkReader.pos = r.byteOffset, a = this.internalTrack.demuxer.chunkReader.readBytes(r.byteSize));
    const n = (r.presentationTimestamp - this.internalTrack.editListOffset) / this.internalTrack.timescale, o = r.duration / this.internalTrack.timescale, c = new G(
      a,
      r.isKeyFrame ? "key" : "delta",
      n,
      o,
      t.moofOffset + e,
      r.byteSize
    );
    return this.packetToFragmentLocation.set(c, { fragment: t, sampleIndex: e }), c;
  }
  findSampleInFragmentsForTimestamp(t) {
    const e = N(
      this.internalTrack.fragments,
      t,
      (r) => r.trackData.get(this.internalTrack.id).startTimestamp
    );
    let i = -1, s = !1;
    if (e !== -1) {
      const a = this.internalTrack.fragments[e].trackData.get(this.internalTrack.id), n = N(
        a.presentationTimestamps,
        t,
        (o) => o.presentationTimestamp
      );
      f(n !== -1), i = a.presentationTimestamps[n].sampleIndex, s = t < a.endTimestamp;
    }
    return { fragmentIndex: e, sampleIndex: i, correctSampleFound: s };
  }
  findKeySampleInFragmentsForTimestamp(t) {
    const e = N(
      this.internalTrack.fragments,
      t,
      (r) => r.trackData.get(this.internalTrack.id).startTimestamp
    );
    let i = -1, s = !1;
    if (e !== -1) {
      const a = this.internalTrack.fragments[e].trackData.get(this.internalTrack.id), n = er(a.presentationTimestamps, (c) => a.samples[c.sampleIndex].isKeyFrame && c.presentationTimestamp <= t);
      if (n === -1)
        throw new Error("Not supported: Fragment does not begin with a key sample.");
      i = a.presentationTimestamps[n].sampleIndex, s = t < a.endTimestamp;
    }
    return { fragmentIndex: e, sampleIndex: i, correctSampleFound: s };
  }
  /** Looks for a packet in the fragments while trying to load as few fragments as possible to retrieve it. */
  async performFragmentedLookup(t, e, i, s) {
    const r = this.internalTrack.demuxer, a = await r.fragmentLookupMutex.acquire();
    try {
      const { fragmentIndex: n, sampleIndex: o, correctSampleFound: c } = t();
      if (c) {
        const S = this.internalTrack.fragments[n];
        return this.fetchPacketInFragment(S, o, s);
      }
      const l = r.metadataReader, d = await l.reader.source.getSize();
      let h = null, u = n, m = o;
      const p = this.internalTrack.fragmentLookupTable ? N(
        this.internalTrack.fragmentLookupTable,
        e,
        (S) => S.timestamp
      ) : -1, g = p !== -1 ? this.internalTrack.fragmentLookupTable[p] : null;
      let w = !1;
      if (n === -1)
        l.pos = g?.moofOffset ?? 0, w = l.pos === 0;
      else {
        const S = this.internalTrack.fragments[n];
        !g || S.moofOffset >= g.moofOffset ? (l.pos = S.moofOffset + S.moofSize, h = S) : l.pos = g.moofOffset;
      }
      for (; l.pos < d; ) {
        if (h) {
          const y = h.trackData.get(this.internalTrack.id);
          if (y && y.startTimestamp > i)
            break;
          if (h.nextFragment) {
            l.pos = h.nextFragment.moofOffset + h.nextFragment.moofSize, h = h.nextFragment;
            continue;
          }
        }
        await l.reader.loadRange(l.pos, l.pos + xt);
        const S = l.pos, k = l.readBoxHeader();
        if (k.name === "moof") {
          const y = X(r.fragments, S, (M) => M.moofOffset);
          let T;
          y === -1 ? (l.pos = S, T = await r.readFragment()) : T = r.fragments[y], h && (h.nextFragment = T), h = T, w && (T.isKnownToBeFirstFragment = !0, w = !1);
          const { fragmentIndex: P, sampleIndex: F, correctSampleFound: z } = t();
          if (z) {
            const M = this.internalTrack.fragments[P];
            return this.fetchPacketInFragment(M, F, s);
          }
          P !== -1 && (u = P, m = F);
        }
        l.pos = S + k.totalSize;
      }
      let b = null;
      const x = u !== -1 ? this.internalTrack.fragments[u] : null;
      if (x && (b = await this.fetchPacketInFragment(x, m, s)), !b && g && (!x || x.moofOffset < g.moofOffset)) {
        const k = this.internalTrack.fragmentLookupTable[p - 1]?.timestamp ?? -1 / 0;
        return this.performFragmentedLookup(t, k, i, s);
      }
      return b;
    } finally {
      a();
    }
  }
}, wc = class extends Kr {
  constructor(t) {
    super(t), this.decoderConfigPromise = null, this.internalTrack = t;
  }
  getCodec() {
    return this.internalTrack.info.codec;
  }
  getCodedWidth() {
    return this.internalTrack.info.width;
  }
  getCodedHeight() {
    return this.internalTrack.info.height;
  }
  getRotation() {
    return this.internalTrack.rotation;
  }
  async getColorSpace() {
    return {
      primaries: this.internalTrack.info.colorSpace?.primaries,
      transfer: this.internalTrack.info.colorSpace?.transfer,
      matrix: this.internalTrack.info.colorSpace?.matrix,
      fullRange: this.internalTrack.info.colorSpace?.fullRange
    };
  }
  async getDecoderConfig() {
    return this.internalTrack.info.codec ? this.decoderConfigPromise ??= (async () => {
      if (this.internalTrack.info.codec === "vp9" && !this.internalTrack.info.vp9CodecInfo) {
        const t = await this.getFirstPacket({});
        this.internalTrack.info.vp9CodecInfo = t && br(t.data);
      } else if (this.internalTrack.info.codec === "av1" && !this.internalTrack.info.av1CodecInfo) {
        const t = await this.getFirstPacket({});
        this.internalTrack.info.av1CodecInfo = t && kr(t.data);
      }
      return {
        codec: or(this.internalTrack.info),
        codedWidth: this.internalTrack.info.width,
        codedHeight: this.internalTrack.info.height,
        description: this.internalTrack.info.codecDescription ?? void 0,
        colorSpace: this.internalTrack.info.colorSpace ?? void 0
      };
    })() : null;
  }
}, yc = class extends Kr {
  constructor(t) {
    super(t), this.decoderConfig = null, this.internalTrack = t;
  }
  getCodec() {
    return this.internalTrack.info.codec;
  }
  getNumberOfChannels() {
    return this.internalTrack.info.numberOfChannels;
  }
  getSampleRate() {
    return this.internalTrack.info.sampleRate;
  }
  async getDecoderConfig() {
    return this.internalTrack.info.codec ? this.decoderConfig ??= {
      codec: cr(this.internalTrack.info),
      numberOfChannels: this.internalTrack.info.numberOfChannels,
      sampleRate: this.internalTrack.info.sampleRate,
      description: this.internalTrack.info.codecDescription ?? void 0
    } : null;
  }
}, Ss = (t, e) => {
  if (t.presentationTimestamps) {
    const i = N(
      t.presentationTimestamps,
      e,
      (s) => s.presentationTimestamp
    );
    return i === -1 ? -1 : t.presentationTimestamps[i].sampleIndex;
  } else {
    const i = N(
      t.sampleTimingEntries,
      e,
      (r) => r.startDecodeTimestamp
    );
    if (i === -1)
      return -1;
    const s = t.sampleTimingEntries[i];
    return s.startIndex + Math.min(Math.floor((e - s.startDecodeTimestamp) / s.delta), s.count - 1);
  }
}, bc = (t, e) => {
  const i = N(t.sampleTimingEntries, e, (b) => b.startIndex), s = t.sampleTimingEntries[i];
  if (!s || s.startIndex + s.count <= e)
    return null;
  let a = s.startDecodeTimestamp + (e - s.startIndex) * s.delta;
  const n = N(
    t.sampleCompositionTimeOffsets,
    e,
    (b) => b.startIndex
  ), o = t.sampleCompositionTimeOffsets[n];
  o && e - o.startIndex < o.count && (a += o.offset);
  const c = t.sampleSizes[Math.min(e, t.sampleSizes.length - 1)], l = N(t.sampleToChunk, e, (b) => b.startSampleIndex), d = t.sampleToChunk[l];
  f(d);
  const h = d.startChunkIndex + Math.floor((e - d.startSampleIndex) / d.samplesPerChunk), u = t.chunkOffsets[h], m = d.startSampleIndex + (h - d.startChunkIndex) * d.samplesPerChunk;
  let p = 0, g = u;
  if (t.sampleSizes.length === 1)
    g += c * (e - m), p += c * d.samplesPerChunk;
  else
    for (let b = m; b < m + d.samplesPerChunk; b++) {
      const x = t.sampleSizes[b];
      b < e && (g += x), p += x;
    }
  let w = s.delta;
  if (t.presentationTimestamps) {
    const b = t.presentationTimestampIndexMap[e];
    f(b !== void 0), b < t.presentationTimestamps.length - 1 && (w = t.presentationTimestamps[b + 1].presentationTimestamp - a);
  }
  return {
    presentationTimestamp: a,
    duration: w,
    sampleOffset: g,
    sampleSize: c,
    chunkOffset: u,
    chunkSize: p,
    isKeyFrame: t.keySampleIndices ? X(t.keySampleIndices, e, (b) => b) !== -1 : !0
  };
}, kc = (t, e) => {
  if (!t.keySampleIndices)
    return e;
  const i = N(t.keySampleIndices, e, (s) => s);
  return t.keySampleIndices[i] ?? -1;
}, Sc = (t, e) => {
  if (!t.keySampleIndices)
    return e + 1;
  const i = N(t.keySampleIndices, e, (s) => s);
  return t.keySampleIndices[i + 1] ?? -1;
}, vs = (t, e) => {
  t.startTimestamp += e, t.endTimestamp += e;
  for (const i of t.samples)
    i.presentationTimestamp += e;
  for (const i of t.presentationTimestamps)
    i.presentationTimestamp += e;
}, vc = (t) => {
  const [e, , , i] = t, s = Math.hypot(e, i), r = e / s, a = i / s;
  return -Math.atan2(a, r) * (180 / Math.PI);
}, si = [
  { id: 290298740, flag: "seekHeadSeen" },
  { id: 357149030, flag: "infoSeen" },
  { id: 374648427, flag: "tracksSeen" },
  { id: 475249515, flag: "cuesSeen" }
], Tc = class extends mt {
  constructor(t) {
    super(t), this.readMetadataPromise = null, this.segments = [], this.currentSegment = null, this.currentTrack = null, this.currentCluster = null, this.currentBlock = null, this.currentCueTime = null, this.isWebM = !1, this.metadataReader = new wi(t._mainReader), this.clusterReader = new wi(new gt(t.source, 64 * 2 ** 20));
  }
  async computeDuration() {
    const t = await this.getTracks(), e = await Promise.all(t.map((i) => i.computeDuration()));
    return Math.max(0, ...e);
  }
  async getTracks() {
    return await this.readMetadata(), this.segments.flatMap((t) => t.tracks.map((e) => e.inputTrack));
  }
  async getMimeType() {
    await this.readMetadata();
    let e = (this.segments.some((s) => s.tracks.some((r) => r.info?.type === "video")) ? "video/" : this.segments.some((s) => s.tracks.some((r) => r.info?.type === "audio")) ? "audio/" : "application/") + (this.isWebM ? "webm" : "x-matroska");
    const i = await this.getTracks();
    if (i.length > 0) {
      const s = await Promise.all(i.map((a) => a.getCodecParameterString())), r = [...new Set(s.filter(Boolean))];
      e += `; codecs="${r.join(", ")}"`;
    }
    return e;
  }
  readMetadata() {
    return this.readMetadataPromise ??= (async () => {
      this.metadataReader.pos = 0;
      const t = await this.input.source.getSize();
      for (; this.metadataReader.pos < t - Ar; ) {
        await this.metadataReader.reader.loadRange(
          this.metadataReader.pos,
          this.metadataReader.pos + je
        );
        const { id: e, size: i } = this.metadataReader.readElementHeader(), s = this.metadataReader.pos;
        if (e === 440786851)
          We(i), await this.metadataReader.reader.loadRange(this.metadataReader.pos, this.metadataReader.pos + i), this.readContiguousElements(this.metadataReader, i);
        else if (e === 408125543 && (await this.readSegment(i), i === null))
          break;
        We(i), this.metadataReader.pos = s + i;
      }
    })();
  }
  async readSegment(t) {
    const e = this.metadataReader.pos;
    this.currentSegment = {
      seekHeadSeen: !1,
      infoSeen: !1,
      tracksSeen: !1,
      cuesSeen: !1,
      timestampScale: -1,
      timestampFactor: -1,
      duration: -1,
      seekEntries: [],
      tracks: [],
      cuePoints: [],
      dataStartPos: e,
      elementEndPos: t === null ? await this.input.source.getSize() : e + t,
      clusterSeekStartPos: e,
      clusters: [],
      clusterLookupMutex: new ft()
    }, this.segments.push(this.currentSegment), await this.metadataReader.reader.loadRange(
      this.metadataReader.pos,
      this.metadataReader.pos + 2 ** 14
    );
    let i = !1;
    for (; this.metadataReader.pos < this.currentSegment.elementEndPos; ) {
      await this.metadataReader.reader.loadRange(
        this.metadataReader.pos,
        this.metadataReader.pos + je
      );
      const o = this.metadataReader.pos, { id: c, size: l } = this.metadataReader.readElementHeader(), d = this.metadataReader.pos, h = si.findIndex((u) => u.id === c);
      if (h !== -1) {
        const u = si[h].flag;
        this.currentSegment[u] = !0, We(l), await this.metadataReader.reader.loadRange(this.metadataReader.pos, this.metadataReader.pos + l), this.readContiguousElements(this.metadataReader, l);
      } else c === 524531317 && (i || (i = !0, this.currentSegment.clusterSeekStartPos = o));
      if (this.currentSegment.infoSeen && this.currentSegment.tracksSeen && this.currentSegment.cuesSeen)
        break;
      if (this.currentSegment.seekHeadSeen) {
        let u = this.currentSegment.infoSeen, m = this.currentSegment.tracksSeen, p = this.currentSegment.cuesSeen;
        for (const g of this.currentSegment.seekEntries)
          g.id === 357149030 ? u = !0 : g.id === 374648427 ? m = !0 : g.id === 475249515 && (p = !0);
        if (u && m && p)
          break;
      }
      if (l === null)
        break;
      this.metadataReader.pos = d + l, i || (this.currentSegment.clusterSeekStartPos = this.metadataReader.pos);
    }
    for (const o of si) {
      if (this.currentSegment[o.flag]) continue;
      const c = this.currentSegment.seekEntries.find((h) => h.id === o.id);
      if (!c) continue;
      this.metadataReader.pos = e + c.segmentPosition, await this.metadataReader.reader.loadRange(
        this.metadataReader.pos,
        this.metadataReader.pos + 2 ** 12
        // Load a larger range, assuming the correct element will be there
      );
      const { id: l, size: d } = this.metadataReader.readElementHeader();
      l === o.id && (We(d), this.currentSegment[o.flag] = !0, await this.metadataReader.reader.loadRange(this.metadataReader.pos, this.metadataReader.pos + d), this.readContiguousElements(this.metadataReader, d));
    }
    this.currentSegment.tracks.sort((o, c) => Number(c.isDefault) - Number(o.isDefault)), this.currentSegment.cuePoints.sort((o, c) => o.clusterPosition - c.clusterPosition);
    const s = this.currentSegment.tracks.map((o) => o.id), r = /* @__PURE__ */ new Set();
    let a = null, n = null;
    for (const o of this.currentSegment.cuePoints) {
      if (o.clusterPosition !== a) {
        for (const l of r)
          f(n), this.currentSegment.tracks.find((h) => h.id === l).cuePoints.push(n);
        for (const l of s)
          r.add(l);
      }
      if (n = o, !r.has(o.trackId))
        continue;
      this.currentSegment.tracks.find((l) => l.id === o.trackId).cuePoints.push(o), r.delete(o.trackId), a = o.clusterPosition;
    }
    for (const o of r)
      f(n), this.currentSegment.tracks.find((l) => l.id === o).cuePoints.push(n);
    for (const o of this.currentSegment.tracks)
      o.cuePoints.sort((c, l) => c.time - l.time);
    this.currentSegment = null;
  }
  async readCluster(t) {
    await this.metadataReader.reader.loadRange(this.metadataReader.pos, this.metadataReader.pos + je);
    const e = this.metadataReader.pos, i = this.metadataReader.readElementHeader(), s = i.id;
    let r = i.size;
    const a = this.metadataReader.pos;
    r === null && (this.clusterReader.pos = a, r = (await this.clusterReader.searchForNextElementId(
      Or,
      t.elementEndPos
    ) ?? t.elementEndPos) - a), f(
      s === 524531317
      /* Cluster */
    ), this.clusterReader.pos = a, await this.clusterReader.reader.loadRange(this.clusterReader.pos, this.clusterReader.pos + r);
    const n = {
      elementStartPos: e,
      elementEndPos: a + r,
      dataStartPos: a,
      timestamp: -1,
      trackData: /* @__PURE__ */ new Map(),
      nextCluster: null,
      isKnownToBeFirstCluster: !1
    };
    this.currentCluster = n, this.readContiguousElements(this.clusterReader, r);
    for (const [c, l] of n.trackData) {
      let d = !1;
      f(l.blocks.length > 0);
      for (let g = 0; g < l.blocks.length; g++) {
        const w = l.blocks[g];
        w.timestamp += n.timestamp, d ||= w.referencedTimestamps.length > 0;
      }
      d && (l.blocks = _c(l.blocks)), l.presentationTimestamps = l.blocks.map((g, w) => ({ timestamp: g.timestamp, blockIndex: w })).sort((g, w) => g.timestamp - w.timestamp);
      let h = !1;
      for (let g = 0; g < l.presentationTimestamps.length; g++) {
        const w = l.presentationTimestamps[g], b = l.blocks[w.blockIndex];
        if (b.isKeyFrame && (h = !0, l.firstKeyFrameTimestamp === null && b.isKeyFrame && (l.firstKeyFrameTimestamp = b.timestamp)), g < l.presentationTimestamps.length - 1) {
          const x = l.presentationTimestamps[g + 1], S = l.blocks[x.blockIndex];
          b.duration = S.timestamp - b.timestamp;
        }
      }
      const u = l.blocks[l.presentationTimestamps[0].blockIndex], m = l.blocks[H(l.presentationTimestamps).blockIndex];
      l.startTimestamp = u.timestamp, l.endTimestamp = m.timestamp + m.duration;
      const p = t.tracks.find((g) => g.id === c);
      if (p) {
        const g = N(
          p.clusters,
          n.timestamp,
          (w) => w.timestamp
        );
        if (p.clusters.splice(g + 1, 0, n), h) {
          const w = N(
            p.clustersWithKeyFrame,
            n.timestamp,
            (b) => b.timestamp
          );
          p.clustersWithKeyFrame.splice(w + 1, 0, n);
        }
      }
    }
    const o = N(
      t.clusters,
      e,
      (c) => c.elementStartPos
    );
    return t.clusters.splice(o + 1, 0, n), this.currentCluster = null, n;
  }
  getTrackDataInCluster(t, e) {
    let i = t.trackData.get(e);
    return i || (i = {
      startTimestamp: 0,
      endTimestamp: 0,
      firstKeyFrameTimestamp: null,
      blocks: [],
      presentationTimestamps: []
    }, t.trackData.set(e, i)), i;
  }
  readContiguousElements(t, e) {
    const i = t.pos;
    for (; t.pos - i < e; )
      this.traverseElement(t);
  }
  traverseElement(t) {
    const { id: e, size: i } = t.readElementHeader(), s = t.pos;
    switch (We(i), e) {
      case 17026:
        this.isWebM = t.readString(i) === "webm";
        break;
      case 19899:
        {
          if (!this.currentSegment) break;
          const r = { id: -1, segmentPosition: -1 };
          this.currentSegment.seekEntries.push(r), this.readContiguousElements(t, i), (r.id === -1 || r.segmentPosition === -1) && this.currentSegment.seekEntries.pop();
        }
        break;
      case 21419:
        {
          const r = this.currentSegment?.seekEntries[this.currentSegment.seekEntries.length - 1];
          if (!r) break;
          r.id = t.readUnsignedInt(i);
        }
        break;
      case 21420:
        {
          const r = this.currentSegment?.seekEntries[this.currentSegment.seekEntries.length - 1];
          if (!r) break;
          r.segmentPosition = t.readUnsignedInt(i);
        }
        break;
      case 2807729:
        {
          if (!this.currentSegment) break;
          this.currentSegment.timestampScale = t.readUnsignedInt(i), this.currentSegment.timestampFactor = 1e9 / this.currentSegment.timestampScale;
        }
        break;
      case 17545:
        {
          if (!this.currentSegment) break;
          this.currentSegment.duration = t.readFloat(i);
        }
        break;
      case 174:
        {
          if (!this.currentSegment) break;
          if (this.currentTrack = {
            id: -1,
            segment: this.currentSegment,
            demuxer: this,
            clusters: [],
            clustersWithKeyFrame: [],
            cuePoints: [],
            isDefault: !1,
            inputTrack: null,
            codecId: null,
            codecPrivate: null,
            languageCode: fe,
            info: null
          }, this.readContiguousElements(t, i), this.currentTrack && this.currentTrack.id !== -1 && this.currentTrack.codecId && this.currentTrack.info) {
            const r = this.currentTrack.codecId.indexOf("/"), a = r === -1 ? this.currentTrack.codecId : this.currentTrack.codecId.slice(0, r);
            if (this.currentTrack.info.type === "video" && this.currentTrack.info.width !== -1 && this.currentTrack.info.height !== -1) {
              this.currentTrack.codecId === ae.avc ? (this.currentTrack.info.codec = "avc", this.currentTrack.info.codecDescription = this.currentTrack.codecPrivate) : this.currentTrack.codecId === ae.hevc ? (this.currentTrack.info.codec = "hevc", this.currentTrack.info.codecDescription = this.currentTrack.codecPrivate) : a === ae.vp8 ? this.currentTrack.info.codec = "vp8" : a === ae.vp9 ? this.currentTrack.info.codec = "vp9" : a === ae.av1 && (this.currentTrack.info.codec = "av1");
              const n = this.currentTrack, o = new pt(new xc(n));
              this.currentTrack.inputTrack = o, this.currentSegment.tracks.push(this.currentTrack);
            } else if (this.currentTrack.info.type === "audio" && this.currentTrack.info.numberOfChannels !== -1 && this.currentTrack.info.sampleRate !== -1) {
              a === ae.aac ? (this.currentTrack.info.codec = "aac", this.currentTrack.info.aacCodecInfo = {
                isMpeg2: this.currentTrack.codecId.includes("MPEG2")
              }, this.currentTrack.info.codecDescription = this.currentTrack.codecPrivate) : this.currentTrack.codecId === ae.mp3 ? this.currentTrack.info.codec = "mp3" : a === ae.opus ? (this.currentTrack.info.codec = "opus", this.currentTrack.info.codecDescription = this.currentTrack.codecPrivate) : a === ae.vorbis ? (this.currentTrack.info.codec = "vorbis", this.currentTrack.info.codecDescription = this.currentTrack.codecPrivate) : a === ae.flac ? (this.currentTrack.info.codec = "flac", this.currentTrack.info.codecDescription = this.currentTrack.codecPrivate) : this.currentTrack.codecId === "A_PCM/INT/LIT" ? this.currentTrack.info.bitDepth === 8 ? this.currentTrack.info.codec = "pcm-u8" : this.currentTrack.info.bitDepth === 16 ? this.currentTrack.info.codec = "pcm-s16" : this.currentTrack.info.bitDepth === 24 ? this.currentTrack.info.codec = "pcm-s24" : this.currentTrack.info.bitDepth === 32 && (this.currentTrack.info.codec = "pcm-s32") : this.currentTrack.codecId === "A_PCM/INT/BIG" ? this.currentTrack.info.bitDepth === 8 ? this.currentTrack.info.codec = "pcm-u8" : this.currentTrack.info.bitDepth === 16 ? this.currentTrack.info.codec = "pcm-s16be" : this.currentTrack.info.bitDepth === 24 ? this.currentTrack.info.codec = "pcm-s24be" : this.currentTrack.info.bitDepth === 32 && (this.currentTrack.info.codec = "pcm-s32be") : this.currentTrack.codecId === "A_PCM/FLOAT/IEEE" && this.currentTrack.info.bitDepth === 32 && (this.currentTrack.info.codec = "pcm-f32");
              const n = this.currentTrack, o = new xe(new Cc(n));
              this.currentTrack.inputTrack = o, this.currentSegment.tracks.push(this.currentTrack);
            }
          }
          this.currentTrack = null;
        }
        break;
      case 215:
        {
          if (!this.currentTrack) break;
          this.currentTrack.id = t.readUnsignedInt(i);
        }
        break;
      case 131:
        {
          if (!this.currentTrack) break;
          const r = t.readUnsignedInt(i);
          r === 1 ? this.currentTrack.info = {
            type: "video",
            width: -1,
            height: -1,
            rotation: 0,
            codec: null,
            codecDescription: null,
            colorSpace: null
          } : r === 2 && (this.currentTrack.info = {
            type: "audio",
            numberOfChannels: -1,
            sampleRate: -1,
            bitDepth: -1,
            codec: null,
            codecDescription: null,
            aacCodecInfo: null
          });
        }
        break;
      case 185:
        {
          if (!this.currentTrack) break;
          t.readUnsignedInt(i) || (this.currentSegment.tracks.pop(), this.currentTrack = null);
        }
        break;
      case 136:
        {
          if (!this.currentTrack) break;
          this.currentTrack.isDefault = !!t.readUnsignedInt(i);
        }
        break;
      case 134:
        {
          if (!this.currentTrack) break;
          this.currentTrack.codecId = t.readString(i);
        }
        break;
      case 25506:
        {
          if (!this.currentTrack) break;
          this.currentTrack.codecPrivate = t.readBytes(i);
        }
        break;
      case 2274716:
        {
          if (!this.currentTrack) break;
          this.currentTrack.languageCode = t.readString(i), Oi(this.currentTrack.languageCode) || (this.currentTrack.languageCode = fe);
        }
        break;
      case 224:
        {
          if (this.currentTrack?.info?.type !== "video") break;
          this.readContiguousElements(t, i);
        }
        break;
      case 176:
        {
          if (this.currentTrack?.info?.type !== "video") break;
          this.currentTrack.info.width = t.readUnsignedInt(i);
        }
        break;
      case 186:
        {
          if (this.currentTrack?.info?.type !== "video") break;
          this.currentTrack.info.height = t.readUnsignedInt(i);
        }
        break;
      case 21936:
        {
          if (this.currentTrack?.info?.type !== "video") break;
          this.currentTrack.info.colorSpace = {}, this.readContiguousElements(t, i);
        }
        break;
      case 21937:
        {
          if (this.currentTrack?.info?.type !== "video" || !this.currentTrack.info.colorSpace) break;
          const r = t.readUnsignedInt(i), a = Ys[r] ?? null;
          this.currentTrack.info.colorSpace.matrix = a;
        }
        break;
      case 21945:
        {
          if (this.currentTrack?.info?.type !== "video" || !this.currentTrack.info.colorSpace) break;
          this.currentTrack.info.colorSpace.fullRange = t.readUnsignedInt(i) === 2;
        }
        break;
      case 21946:
        {
          if (this.currentTrack?.info?.type !== "video" || !this.currentTrack.info.colorSpace) break;
          const r = t.readUnsignedInt(i), a = Gs[r] ?? null;
          this.currentTrack.info.colorSpace.transfer = a;
        }
        break;
      case 21947:
        {
          if (this.currentTrack?.info?.type !== "video" || !this.currentTrack.info.colorSpace) break;
          const r = t.readUnsignedInt(i), a = Ks[r] ?? null;
          this.currentTrack.info.colorSpace.primaries = a;
        }
        break;
      case 30320:
        {
          if (this.currentTrack?.info?.type !== "video") break;
          this.readContiguousElements(t, i);
        }
        break;
      case 30325:
        {
          if (this.currentTrack?.info?.type !== "video") break;
          const a = -t.readFloat(i);
          try {
            this.currentTrack.info.rotation = Fi(a);
          } catch {
          }
        }
        break;
      case 225:
        {
          if (this.currentTrack?.info?.type !== "audio") break;
          this.readContiguousElements(t, i);
        }
        break;
      case 181:
        {
          if (this.currentTrack?.info?.type !== "audio") break;
          this.currentTrack.info.sampleRate = t.readFloat(i);
        }
        break;
      case 159:
        {
          if (this.currentTrack?.info?.type !== "audio") break;
          this.currentTrack.info.numberOfChannels = t.readUnsignedInt(i);
        }
        break;
      case 25188:
        {
          if (this.currentTrack?.info?.type !== "audio") break;
          this.currentTrack.info.bitDepth = t.readUnsignedInt(i);
        }
        break;
      case 187:
        {
          if (!this.currentSegment) break;
          this.readContiguousElements(t, i), this.currentCueTime = null;
        }
        break;
      case 179:
        this.currentCueTime = t.readUnsignedInt(i);
        break;
      case 183:
        {
          if (this.currentCueTime === null) break;
          f(this.currentSegment);
          const r = { time: this.currentCueTime, trackId: -1, clusterPosition: -1 };
          this.currentSegment.cuePoints.push(r), this.readContiguousElements(t, i), (r.trackId === -1 || r.clusterPosition === -1) && this.currentSegment.cuePoints.pop();
        }
        break;
      case 247:
        {
          const r = this.currentSegment?.cuePoints[this.currentSegment.cuePoints.length - 1];
          if (!r) break;
          r.trackId = t.readUnsignedInt(i);
        }
        break;
      case 241:
        {
          const r = this.currentSegment?.cuePoints[this.currentSegment.cuePoints.length - 1];
          if (!r) break;
          f(this.currentSegment), r.clusterPosition = this.currentSegment.dataStartPos + t.readUnsignedInt(i);
        }
        break;
      case 231:
        {
          if (!this.currentCluster) break;
          this.currentCluster.timestamp = t.readUnsignedInt(i);
        }
        break;
      case 163:
        {
          if (!this.currentCluster) break;
          const r = t.readVarInt(), a = t.readS16(), o = !!(t.readU8() & 128);
          this.getTrackDataInCluster(this.currentCluster, r).blocks.push({
            timestamp: a,
            // We'll add the cluster's timestamp to this later
            duration: 0,
            isKeyFrame: o,
            referencedTimestamps: [],
            data: t.readBytes(i - (t.pos - s))
          });
        }
        break;
      case 160:
        {
          if (!this.currentCluster) break;
          if (this.readContiguousElements(t, i), this.currentBlock) {
            for (let r = 0; r < this.currentBlock.referencedTimestamps.length; r++)
              this.currentBlock.referencedTimestamps[r] += this.currentBlock.timestamp;
            this.currentBlock = null;
          }
        }
        break;
      case 161:
        {
          if (!this.currentCluster) break;
          const r = t.readVarInt(), a = t.readS16();
          t.readU8();
          const n = this.getTrackDataInCluster(this.currentCluster, r);
          this.currentBlock = {
            timestamp: a,
            // We'll add the cluster's timestamp to this later
            duration: 0,
            isKeyFrame: !0,
            referencedTimestamps: [],
            data: t.readBytes(i - (t.pos - s))
          }, n.blocks.push(this.currentBlock);
        }
        break;
      case 155:
        {
          if (!this.currentBlock) break;
          this.currentBlock.duration = t.readUnsignedInt(i);
        }
        break;
      case 251:
        {
          if (!this.currentBlock) break;
          this.currentBlock.isKeyFrame = !1;
          const r = t.readSignedInt(i);
          this.currentBlock.referencedTimestamps.push(r);
        }
        break;
    }
    t.pos = s + i;
  }
}, Gr = class {
  constructor(t) {
    this.internalTrack = t, this.packetToClusterLocation = /* @__PURE__ */ new WeakMap();
  }
  getId() {
    return this.internalTrack.id;
  }
  getCodec() {
    throw new Error("Not implemented on base class.");
  }
  async computeDuration() {
    const t = await this.getPacket(1 / 0, { metadataOnly: !0 });
    return (t?.timestamp ?? 0) + (t?.duration ?? 0);
  }
  getLanguageCode() {
    return this.internalTrack.languageCode;
  }
  async getFirstTimestamp() {
    return (await this.getFirstPacket({ metadataOnly: !0 }))?.timestamp ?? 0;
  }
  getTimeResolution() {
    return this.internalTrack.segment.timestampFactor;
  }
  async getFirstPacket(t) {
    return this.performClusterLookup(
      () => {
        const e = this.internalTrack.segment.clusters[0] ?? null;
        if (e?.isKnownToBeFirstCluster) {
          let i = e;
          for (; i; ) {
            if (i.trackData.get(this.internalTrack.id))
              return {
                clusterIndex: X(
                  this.internalTrack.clusters,
                  i.elementStartPos,
                  (r) => r.elementStartPos
                ),
                blockIndex: 0,
                correctBlockFound: !0
              };
            i = i.nextCluster;
          }
        }
        return {
          clusterIndex: -1,
          blockIndex: -1,
          correctBlockFound: !1
        };
      },
      -1 / 0,
      // Use -Infinity as a search timestamp to avoid using the cues
      1 / 0,
      t
    );
  }
  intoTimescale(t) {
    return Bi(t * this.internalTrack.segment.timestampFactor, 14);
  }
  async getPacket(t, e) {
    const i = this.intoTimescale(t);
    return this.performClusterLookup(
      () => this.findBlockInClustersForTimestamp(i),
      i,
      i,
      e
    );
  }
  async getNextPacket(t, e) {
    const i = this.packetToClusterLocation.get(t);
    if (i === void 0)
      throw new Error("Packet was not created from this track.");
    const s = i.cluster.trackData.get(this.internalTrack.id), r = s.blocks[i.blockIndex], a = X(
      this.internalTrack.clusters,
      i.cluster.elementStartPos,
      (n) => n.elementStartPos
    );
    return f(a !== -1), this.performClusterLookup(
      () => {
        if (i.blockIndex + 1 < s.blocks.length)
          return {
            clusterIndex: a,
            blockIndex: i.blockIndex + 1,
            correctBlockFound: !0
          };
        {
          let n = i.cluster;
          for (; n.nextCluster; )
            if (n = n.nextCluster, n.trackData.get(this.internalTrack.id)) {
              const c = X(
                this.internalTrack.clusters,
                n.elementStartPos,
                (l) => l.elementStartPos
              );
              return f(c !== -1), {
                clusterIndex: c,
                blockIndex: 0,
                correctBlockFound: !0
              };
            }
          return {
            clusterIndex: a,
            blockIndex: -1,
            correctBlockFound: !1
          };
        }
      },
      r.timestamp,
      1 / 0,
      e
    );
  }
  async getKeyPacket(t, e) {
    const i = this.intoTimescale(t);
    return this.performClusterLookup(
      () => this.findKeyBlockInClustersForTimestamp(i),
      i,
      i,
      e
    );
  }
  async getNextKeyPacket(t, e) {
    const i = this.packetToClusterLocation.get(t);
    if (i === void 0)
      throw new Error("Packet was not created from this track.");
    const s = i.cluster.trackData.get(this.internalTrack.id), r = s.blocks[i.blockIndex], a = X(
      this.internalTrack.clusters,
      i.cluster.elementStartPos,
      (n) => n.elementStartPos
    );
    return f(a !== -1), this.performClusterLookup(
      () => {
        const n = s.blocks.findIndex(
          (o, c) => o.isKeyFrame && c > i.blockIndex
        );
        if (n !== -1)
          return {
            clusterIndex: a,
            blockIndex: n,
            correctBlockFound: !0
          };
        {
          let o = i.cluster;
          for (; o.nextCluster; ) {
            o = o.nextCluster;
            const c = o.trackData.get(this.internalTrack.id);
            if (c && c.firstKeyFrameTimestamp !== null) {
              const l = X(
                this.internalTrack.clusters,
                o.elementStartPos,
                (h) => h.elementStartPos
              );
              f(l !== -1);
              const d = c.blocks.findIndex((h) => h.isKeyFrame);
              return f(d !== -1), {
                clusterIndex: l,
                blockIndex: d,
                correctBlockFound: !0
              };
            }
          }
          return {
            clusterIndex: a,
            blockIndex: -1,
            correctBlockFound: !1
          };
        }
      },
      r.timestamp,
      1 / 0,
      e
    );
  }
  async fetchPacketInCluster(t, e, i) {
    if (e === -1)
      return null;
    const r = t.trackData.get(this.internalTrack.id).blocks[e];
    f(r);
    const a = i.metadataOnly ? me : r.data, n = r.timestamp / this.internalTrack.segment.timestampFactor, o = r.duration / this.internalTrack.segment.timestampFactor, c = new G(
      a,
      r.isKeyFrame ? "key" : "delta",
      n,
      o,
      t.dataStartPos + e,
      r.data.byteLength
    );
    return this.packetToClusterLocation.set(c, { cluster: t, blockIndex: e }), c;
  }
  findBlockInClustersForTimestamp(t) {
    const e = N(
      this.internalTrack.clusters,
      t,
      (r) => r.trackData.get(this.internalTrack.id).startTimestamp
    );
    let i = -1, s = !1;
    if (e !== -1) {
      const a = this.internalTrack.clusters[e].trackData.get(this.internalTrack.id), n = N(
        a.presentationTimestamps,
        t,
        (o) => o.timestamp
      );
      f(n !== -1), i = a.presentationTimestamps[n].blockIndex, s = t < a.endTimestamp;
    }
    return { clusterIndex: e, blockIndex: i, correctBlockFound: s };
  }
  findKeyBlockInClustersForTimestamp(t) {
    const e = N(
      this.internalTrack.clustersWithKeyFrame,
      t,
      (a) => a.trackData.get(this.internalTrack.id).firstKeyFrameTimestamp
    );
    let i = -1, s = -1, r = !1;
    if (e !== -1) {
      const a = this.internalTrack.clustersWithKeyFrame[e];
      i = X(
        this.internalTrack.clusters,
        a.elementStartPos,
        (l) => l.elementStartPos
      ), f(i !== -1);
      const n = a.trackData.get(this.internalTrack.id), o = er(n.presentationTimestamps, (l) => n.blocks[l.blockIndex].isKeyFrame && l.timestamp <= t);
      f(o !== -1), s = n.presentationTimestamps[o].blockIndex, r = t < n.endTimestamp;
    }
    return { clusterIndex: i, blockIndex: s, correctBlockFound: r };
  }
  /** Looks for a packet in the clusters while trying to load as few clusters as possible to retrieve it. */
  async performClusterLookup(t, e, i, s) {
    const { demuxer: r, segment: a } = this.internalTrack, n = await a.clusterLookupMutex.acquire();
    try {
      const { clusterIndex: o, blockIndex: c, correctBlockFound: l } = t();
      if (l) {
        const k = this.internalTrack.clusters[o];
        return this.fetchPacketInCluster(k, c, s);
      }
      const d = r.metadataReader, h = r.clusterReader;
      let u = null, m = o, p = c;
      const g = N(
        this.internalTrack.cuePoints,
        e,
        (k) => k.time
      ), w = g !== -1 ? this.internalTrack.cuePoints[g] : null;
      let b = !1;
      if (o === -1)
        d.pos = w?.clusterPosition ?? a.clusterSeekStartPos, b = d.pos === a.clusterSeekStartPos;
      else {
        const k = this.internalTrack.clusters[o];
        !w || k.elementStartPos >= w.clusterPosition ? (d.pos = k.elementEndPos, u = k) : d.pos = w.clusterPosition;
      }
      for (; d.pos < a.elementEndPos; ) {
        if (u) {
          const z = u.trackData.get(this.internalTrack.id);
          if (z && z.startTimestamp > i)
            break;
          if (u.nextCluster) {
            d.pos = u.nextCluster.elementEndPos, u = u.nextCluster;
            continue;
          }
        }
        await d.reader.loadRange(d.pos, d.pos + je);
        const k = d.pos, y = d.readElementHeader(), T = y.id;
        let P = y.size;
        const F = d.pos;
        if (T === 524531317) {
          const z = X(a.clusters, k, ($t) => $t.elementStartPos);
          let M;
          z === -1 ? (d.pos = k, M = await r.readCluster(a)) : M = a.clusters[z], u && (u.nextCluster = M), u = M, b && (M.isKnownToBeFirstCluster = !0, b = !1);
          const { clusterIndex: A, blockIndex: j, correctBlockFound: Ie } = t();
          if (Ie) {
            const $t = this.internalTrack.clusters[A];
            return this.fetchPacketInCluster($t, j, s);
          }
          A !== -1 && (m = A, p = j);
        }
        if (P === null) {
          T === 524531317 ? (f(u), P = u.elementEndPos - F) : (h.pos = F, P = (await h.searchForNextElementId(
            Or,
            a.elementEndPos
          ) ?? a.elementEndPos) - F);
          const z = F + P;
          if (z >= a.elementEndPos - Ar)
            break;
          if (h.pos = z, h.readElementId() === 408125543) {
            a.elementEndPos = z;
            break;
          }
        }
        d.pos = F + P;
      }
      let x = null;
      const S = m !== -1 ? this.internalTrack.clusters[m] : null;
      if (S && (x = await this.fetchPacketInCluster(S, p, s)), !x && w && (!S || S.elementStartPos < w.clusterPosition)) {
        const y = this.internalTrack.cuePoints[g - 1]?.time ?? -1 / 0;
        return this.performClusterLookup(t, y, i, s);
      }
      return x;
    } finally {
      n();
    }
  }
}, xc = class extends Gr {
  constructor(t) {
    super(t), this.decoderConfigPromise = null, this.internalTrack = t;
  }
  getCodec() {
    return this.internalTrack.info.codec;
  }
  getCodedWidth() {
    return this.internalTrack.info.width;
  }
  getCodedHeight() {
    return this.internalTrack.info.height;
  }
  getRotation() {
    return this.internalTrack.info.rotation;
  }
  async getColorSpace() {
    return {
      primaries: this.internalTrack.info.colorSpace?.primaries,
      transfer: this.internalTrack.info.colorSpace?.transfer,
      matrix: this.internalTrack.info.colorSpace?.matrix,
      fullRange: this.internalTrack.info.colorSpace?.fullRange
    };
  }
  async getDecoderConfig() {
    return this.internalTrack.info.codec ? this.decoderConfigPromise ??= (async () => {
      let t = null;
      return (this.internalTrack.info.codec === "vp9" || this.internalTrack.info.codec === "av1" || this.internalTrack.info.codec === "avc" && !this.internalTrack.info.codecDescription || this.internalTrack.info.codec === "hevc" && !this.internalTrack.info.codecDescription) && (t = await this.getFirstPacket({})), {
        codec: or({
          width: this.internalTrack.info.width,
          height: this.internalTrack.info.height,
          codec: this.internalTrack.info.codec,
          codecDescription: this.internalTrack.info.codecDescription,
          colorSpace: this.internalTrack.info.colorSpace,
          avcCodecInfo: this.internalTrack.info.codec === "avc" && t ? wr(t.data) : null,
          hevcCodecInfo: this.internalTrack.info.codec === "hevc" && t ? yr(t.data) : null,
          vp9CodecInfo: this.internalTrack.info.codec === "vp9" && t ? br(t.data) : null,
          av1CodecInfo: this.internalTrack.info.codec === "av1" && t ? kr(t.data) : null
        }),
        codedWidth: this.internalTrack.info.width,
        codedHeight: this.internalTrack.info.height,
        description: this.internalTrack.info.codecDescription ?? void 0,
        colorSpace: this.internalTrack.info.colorSpace ?? void 0
      };
    })() : null;
  }
}, Cc = class extends Gr {
  constructor(t) {
    super(t), this.decoderConfig = null, this.internalTrack = t;
  }
  getCodec() {
    return this.internalTrack.info.codec;
  }
  getNumberOfChannels() {
    return this.internalTrack.info.numberOfChannels;
  }
  getSampleRate() {
    return this.internalTrack.info.sampleRate;
  }
  async getDecoderConfig() {
    return this.internalTrack.info.codec ? this.decoderConfig ??= {
      codec: cr({
        codec: this.internalTrack.info.codec,
        codecDescription: this.internalTrack.info.codecDescription,
        aacCodecInfo: this.internalTrack.info.aacCodecInfo
      }),
      numberOfChannels: this.internalTrack.info.numberOfChannels,
      sampleRate: this.internalTrack.info.sampleRate,
      description: this.internalTrack.info.codecDescription ?? void 0
    } : null;
  }
}, _c = (t) => {
  const e = /* @__PURE__ */ new Map();
  for (let a = 0; a < t.length; a++) {
    const n = t[a];
    e.set(n.timestamp, n);
  }
  const i = /* @__PURE__ */ new Set(), s = [], r = (a) => {
    if (!i.has(a)) {
      i.add(a);
      for (let n = 0; n < a.referencedTimestamps.length; n++) {
        const o = a.referencedTimestamps[n], c = e.get(o);
        c && r(c);
      }
      s.push(a);
    }
  };
  for (let a = 0; a < t.length; a++)
    r(t[a]);
  return s;
}, Yr = class {
  constructor(t) {
    this.reader = t, this.pos = 0, this.fileSize = null;
  }
  readBytes(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    return this.pos += t, new Uint8Array(e.buffer, i, t);
  }
  readU16() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 2);
    return this.pos += 2, t.getUint16(e, !1);
  }
  readU32() {
    const { view: t, offset: e } = this.reader.getViewAndOffset(this.pos, this.pos + 4);
    return this.pos += 4, t.getUint32(e, !1);
  }
  readAscii(t) {
    const { view: e, offset: i } = this.reader.getViewAndOffset(this.pos, this.pos + t);
    this.pos += t;
    let s = "";
    for (let r = 0; r < t; r++)
      s += String.fromCharCode(e.getUint8(i + r));
    return s;
  }
  readId3() {
    return this.readAscii(3) !== "ID3" ? (this.pos -= 3, null) : (this.pos += 3, { size: Ic(this.readU32()) });
  }
  readNextFrameHeader(t) {
    for (f(this.fileSize), t ??= this.fileSize; this.pos < t - Oo; ) {
      const e = this.readU32();
      this.pos -= 4;
      const i = Lo(e, this);
      if (i)
        return i;
    }
    return null;
  }
}, Ic = (t) => {
  let e = 2130706432, i = 0;
  for (; e !== 0; )
    i >>= 1, i |= t & e, e >>= 8;
  return i;
}, Ec = class extends mt {
  constructor(t) {
    super(t), this.metadataPromise = null, this.firstFrameHeader = null, this.allSamples = [], this.tracks = [], this.reader = new Yr(t._mainReader);
  }
  async readMetadata() {
    return this.metadataPromise ??= (async () => {
      const t = await this.input.source.getSize();
      this.reader.fileSize = t, await this.reader.reader.loadRange(0, t);
      const e = this.reader.readId3();
      e && (this.reader.pos += e.size);
      let i = 0;
      for (; ; ) {
        const s = this.reader.readNextFrameHeader();
        if (!s)
          break;
        const r = Vo(s.mpegVersionId, s.channel);
        this.reader.pos = s.startPos + r;
        const a = this.reader.readU32(), n = a === Uo || a === Do;
        if (this.reader.pos = s.startPos + s.totalSize - 1, n)
          continue;
        this.firstFrameHeader || (this.firstFrameHeader = s);
        const o = s.audioSamplesInFrame / s.sampleRate, c = {
          timestamp: i / s.sampleRate,
          duration: o,
          dataStart: s.startPos,
          dataSize: s.totalSize
        };
        this.allSamples.push(c), i += s.audioSamplesInFrame;
      }
      if (!this.firstFrameHeader)
        throw new Error("No MP3 frames found.");
      this.tracks = [new xe(new Pc(this))];
    })();
  }
  async getMimeType() {
    return "audio/mpeg";
  }
  async getTracks() {
    return await this.readMetadata(), this.tracks;
  }
  async computeDuration() {
    await this.readMetadata();
    const t = H(this.allSamples);
    return f(t), t.timestamp + t.duration;
  }
}, Pc = class {
  constructor(t) {
    this.demuxer = t;
  }
  getId() {
    return 1;
  }
  async getFirstTimestamp() {
    return 0;
  }
  getTimeResolution() {
    return f(this.demuxer.firstFrameHeader), this.demuxer.firstFrameHeader.sampleRate / this.demuxer.firstFrameHeader.audioSamplesInFrame;
  }
  computeDuration() {
    return this.demuxer.computeDuration();
  }
  getLanguageCode() {
    return fe;
  }
  getCodec() {
    return "mp3";
  }
  getNumberOfChannels() {
    return f(this.demuxer.firstFrameHeader), this.demuxer.firstFrameHeader.channel === 3 ? 1 : 2;
  }
  getSampleRate() {
    return f(this.demuxer.firstFrameHeader), this.demuxer.firstFrameHeader.sampleRate;
  }
  async getDecoderConfig() {
    return f(this.demuxer.firstFrameHeader), {
      codec: "mp3",
      numberOfChannels: this.demuxer.firstFrameHeader.channel === 3 ? 1 : 2,
      sampleRate: this.demuxer.firstFrameHeader.sampleRate
    };
  }
  getPacketAtIndex(t, e) {
    if (t === -1)
      return null;
    const i = this.demuxer.allSamples[t];
    if (!i)
      return null;
    let s;
    return e.metadataOnly ? s = me : (this.demuxer.reader.pos = i.dataStart, s = this.demuxer.reader.readBytes(i.dataSize)), new G(
      s,
      "key",
      i.timestamp,
      i.duration,
      t,
      i.dataSize
    );
  }
  async getFirstPacket(t) {
    return this.getPacketAtIndex(0, t);
  }
  async getNextPacket(t, e) {
    const i = X(
      this.demuxer.allSamples,
      t.timestamp,
      (s) => s.timestamp
    );
    if (i === -1)
      throw new Error("Packet was not created from this track.");
    return this.getPacketAtIndex(i + 1, e);
  }
  async getPacket(t, e) {
    const i = N(
      this.demuxer.allSamples,
      t,
      (s) => s.timestamp
    );
    return this.getPacketAtIndex(i, e);
  }
  getKeyPacket(t, e) {
    return this.getPacket(t, e);
  }
  getNextKeyPacket(t, e) {
    return this.getNextPacket(t, e);
  }
}, Fc = class extends mt {
  constructor(t) {
    super(t), this.readingMutex = new ft(), this.metadataPromise = null, this.fileSize = null, this.bitstreams = [], this.tracks = [], this.reader = new Vr(new gt(t.source, 64 * 2 ** 20));
  }
  async readMetadata() {
    return this.metadataPromise ??= (async () => {
      for (this.fileSize = await this.input.source.getSize(); this.reader.pos < this.fileSize - ct; ) {
        await this.reader.reader.loadRange(
          this.reader.pos,
          this.reader.pos + qe
        );
        const t = this.reader.readPageHeader();
        if (!t || !!!(t.headerType & 2))
          break;
        this.bitstreams.push({
          serialNumber: t.serialNumber,
          bosPage: t,
          description: null,
          numberOfChannels: -1,
          sampleRate: -1,
          codecInfo: {
            codec: null,
            vorbisInfo: null,
            opusInfo: null
          },
          lastMetadataPacket: null
        }), this.reader.pos = t.headerStartPos + t.totalSize;
      }
      for (const t of this.bitstreams) {
        const e = await this.readPacket(this.reader, t.bosPage, 0);
        e && (// Check for Vorbis
        e.data.byteLength >= 7 && e.data[0] === 1 && e.data[1] === 118 && e.data[2] === 111 && e.data[3] === 114 && e.data[4] === 98 && e.data[5] === 105 && e.data[6] === 115 ? await this.readVorbisMetadata(e, t) : (
          // Check for Opus
          e.data.byteLength >= 8 && e.data[0] === 79 && e.data[1] === 112 && e.data[2] === 117 && e.data[3] === 115 && e.data[4] === 72 && e.data[5] === 101 && e.data[6] === 97 && e.data[7] === 100 && await this.readOpusMetadata(e, t)
        ), t.codecInfo.codec !== null && this.tracks.push(new xe(new Rc(t, this))));
      }
    })();
  }
  async readVorbisMetadata(t, e) {
    let i = await this.findNextPacketStart(this.reader, t);
    if (!i)
      return;
    const s = await this.readPacket(
      this.reader,
      i.startPage,
      i.startSegmentIndex
    );
    if (!s || (i = await this.findNextPacketStart(this.reader, s), !i))
      return;
    const r = await this.readPacket(
      this.reader,
      i.startPage,
      i.startSegmentIndex
    );
    if (!r || s.data[0] !== 3 || r.data[0] !== 5)
      return;
    const a = [], n = (d) => {
      for (; a.push(Math.min(255, d)), !(d < 255); )
        d -= 255;
    };
    n(t.data.length), n(s.data.length);
    const o = new Uint8Array(
      1 + a.length + t.data.length + s.data.length + r.data.length
    );
    o[0] = a.length, o.set(
      a,
      1
    ), o.set(
      t.data,
      1 + a.length
    ), o.set(
      s.data,
      1 + a.length + t.data.length
    ), o.set(
      r.data,
      1 + a.length + t.data.length + s.data.length
    ), e.codecInfo.codec = "vorbis", e.description = o, e.lastMetadataPacket = r;
    const c = Re(t.data);
    e.numberOfChannels = c.getUint8(11), e.sampleRate = c.getUint32(12, !0);
    const l = c.getUint8(28);
    e.codecInfo.vorbisInfo = {
      blocksizes: [
        1 << (l & 15),
        1 << (l >> 4)
      ],
      modeBlockflags: Sr(r.data).modeBlockflags
    };
  }
  async readOpusMetadata(t, e) {
    const i = await this.findNextPacketStart(this.reader, t);
    if (!i)
      return;
    const s = await this.readPacket(
      this.reader,
      i.startPage,
      i.startSegmentIndex
    );
    if (!s)
      return;
    e.codecInfo.codec = "opus", e.description = t.data, e.lastMetadataPacket = s;
    const r = Vt(t.data);
    e.numberOfChannels = r.outputChannelCount, e.sampleRate = r.inputSampleRate, e.codecInfo.opusInfo = {
      preSkip: r.preSkip
    };
  }
  async readPacket(t, e, i) {
    f(i < e.lacingValues.length), f(this.fileSize);
    let s = 0;
    for (let h = 0; h < i; h++)
      s += e.lacingValues[h];
    let r = e, a = s, n = i;
    const o = [];
    e:
      for (; ; ) {
        await t.reader.loadRange(
          r.dataStartPos,
          r.dataStartPos + r.dataSize
        ), t.pos = r.dataStartPos;
        const h = t.readBytes(r.dataSize);
        for (; ; ) {
          if (n === r.lacingValues.length) {
            o.push(h.subarray(s, a));
            break;
          }
          const u = r.lacingValues[n];
          if (a += u, u < 255) {
            o.push(h.subarray(s, a));
            break e;
          }
          n++;
        }
        for (; ; ) {
          if (t.pos = r.headerStartPos + r.totalSize, t.pos >= this.fileSize - ct)
            return null;
          await t.reader.loadRange(t.pos, t.pos + qe);
          const u = t.readPageHeader();
          if (!u)
            return null;
          if (r = u, r.serialNumber === e.serialNumber)
            break;
        }
        s = 0, a = 0, n = 0;
      }
    const c = o.reduce((h, u) => h + u.length, 0), l = new Uint8Array(c);
    let d = 0;
    for (let h = 0; h < o.length; h++) {
      const u = o[h];
      l.set(u, d), d += u.length;
    }
    return {
      data: l,
      endPage: r,
      endSegmentIndex: n
    };
  }
  async findNextPacketStart(t, e) {
    if (f(this.fileSize !== null), e.endSegmentIndex < e.endPage.lacingValues.length - 1)
      return { startPage: e.endPage, startSegmentIndex: e.endSegmentIndex + 1 };
    if (!!(e.endPage.headerType & 4))
      return null;
    for (t.pos = e.endPage.headerStartPos + e.endPage.totalSize; ; ) {
      if (t.pos >= this.fileSize - ct)
        return null;
      await t.reader.loadRange(t.pos, t.pos + qe);
      const s = t.readPageHeader();
      if (!s)
        return null;
      if (s.serialNumber === e.endPage.serialNumber)
        return { startPage: s, startSegmentIndex: 0 };
      t.pos = s.headerStartPos + s.totalSize;
    }
  }
  async getMimeType() {
    await this.readMetadata();
    let t = "audio/ogg";
    if (this.tracks.length > 0) {
      const e = await Promise.all(this.tracks.map((s) => s.getCodecParameterString())), i = [...new Set(e.filter(Boolean))];
      t += `; codecs="${i.join(", ")}"`;
    }
    return t;
  }
  async getTracks() {
    return await this.readMetadata(), this.tracks;
  }
  async computeDuration() {
    const t = await this.getTracks(), e = await Promise.all(t.map((i) => i.computeDuration()));
    return Math.max(0, ...e);
  }
}, Rc = class {
  constructor(t, e) {
    this.bitstream = t, this.demuxer = e, this.encodedPacketToMetadata = /* @__PURE__ */ new WeakMap(), this.internalSampleRate = t.codecInfo.codec === "opus" ? Ai : t.sampleRate;
  }
  getId() {
    return this.bitstream.serialNumber;
  }
  getNumberOfChannels() {
    return this.bitstream.numberOfChannels;
  }
  getSampleRate() {
    return this.bitstream.sampleRate;
  }
  getTimeResolution() {
    return this.bitstream.sampleRate;
  }
  getCodec() {
    return this.bitstream.codecInfo.codec;
  }
  async getDecoderConfig() {
    return f(this.bitstream.codecInfo.codec), {
      codec: this.bitstream.codecInfo.codec,
      numberOfChannels: this.bitstream.numberOfChannels,
      sampleRate: this.bitstream.sampleRate,
      description: this.bitstream.description ?? void 0
    };
  }
  getLanguageCode() {
    return fe;
  }
  async getFirstTimestamp() {
    return 0;
  }
  async computeDuration() {
    const t = await this.getPacket(1 / 0, { metadataOnly: !0 });
    return (t?.timestamp ?? 0) + (t?.duration ?? 0);
  }
  granulePositionToTimestampInSamples(t) {
    return this.bitstream.codecInfo.codec === "opus" ? (f(this.bitstream.codecInfo.opusInfo), t - this.bitstream.codecInfo.opusInfo.preSkip) : t;
  }
  createEncodedPacketFromOggPacket(t, e, i) {
    if (!t)
      return null;
    const { durationInSamples: s, vorbisBlockSize: r } = Dr(
      t.data,
      this.bitstream.codecInfo,
      e.vorbisLastBlocksize
    ), a = new G(
      i.metadataOnly ? me : t.data,
      "key",
      Math.max(0, e.timestampInSamples) / this.internalSampleRate,
      s / this.internalSampleRate,
      t.endPage.headerStartPos + t.endSegmentIndex,
      t.data.byteLength
    );
    return this.encodedPacketToMetadata.set(a, {
      packet: t,
      timestampInSamples: e.timestampInSamples,
      durationInSamples: s,
      vorbisBlockSize: r
    }), a;
  }
  async getFirstPacket(t, e = !0) {
    const i = e ? await this.demuxer.readingMutex.acquire() : null;
    try {
      f(this.bitstream.lastMetadataPacket);
      const s = await this.demuxer.findNextPacketStart(
        this.demuxer.reader,
        this.bitstream.lastMetadataPacket
      );
      if (!s)
        return null;
      let r = 0;
      this.bitstream.codecInfo.codec === "opus" && (f(this.bitstream.codecInfo.opusInfo), r -= this.bitstream.codecInfo.opusInfo.preSkip);
      const a = await this.demuxer.readPacket(
        this.demuxer.reader,
        s.startPage,
        s.startSegmentIndex
      );
      return this.createEncodedPacketFromOggPacket(
        a,
        {
          timestampInSamples: r,
          vorbisLastBlocksize: null
        },
        t
      );
    } finally {
      i?.();
    }
  }
  async getNextPacket(t, e) {
    const i = await this.demuxer.readingMutex.acquire();
    try {
      const s = this.encodedPacketToMetadata.get(t);
      if (!s)
        throw new Error("Packet was not created from this track.");
      const r = await this.demuxer.findNextPacketStart(this.demuxer.reader, s.packet);
      if (!r)
        return null;
      const a = s.timestampInSamples + s.durationInSamples, n = await this.demuxer.readPacket(
        this.demuxer.reader,
        r.startPage,
        r.startSegmentIndex
      );
      return this.createEncodedPacketFromOggPacket(
        n,
        {
          timestampInSamples: a,
          vorbisLastBlocksize: s.vorbisBlockSize
        },
        e
      );
    } finally {
      i();
    }
  }
  async getPacket(t, e) {
    const i = await this.demuxer.readingMutex.acquire();
    try {
      f(this.demuxer.fileSize !== null);
      const s = Bi(t * this.internalSampleRate, 14);
      if (s === 0)
        return this.getFirstPacket(e, !1);
      if (s < 0)
        return null;
      const r = this.demuxer.reader;
      f(this.bitstream.lastMetadataPacket);
      const a = await this.demuxer.findNextPacketStart(
        r,
        this.bitstream.lastMetadataPacket
      );
      if (!a)
        return null;
      let n = a.startPage, o = this.demuxer.fileSize;
      const c = [n];
      e:
        for (; n.headerStartPos + n.totalSize < o; ) {
          const S = n.headerStartPos, k = Math.floor((S + o) / 2);
          let y = k;
          for (; ; ) {
            const T = Math.min(
              y + Nr,
              o - ct
            );
            if (await r.reader.loadRange(y, T), r.pos = y, !r.findNextPageHeader(T)) {
              o = k + ct;
              continue e;
            }
            await r.reader.loadRange(r.pos, r.pos + qe);
            const F = r.readPageHeader();
            f(F);
            let z = !1;
            if (F.serialNumber === this.bitstream.serialNumber)
              z = !0;
            else {
              await r.reader.loadRange(F.headerStartPos, F.headerStartPos + F.totalSize), r.pos = F.headerStartPos;
              const A = r.readBytes(F.totalSize);
              z = Ur(A) === F.checksum;
            }
            if (!z) {
              y = F.headerStartPos + 4;
              continue;
            }
            if (z && F.serialNumber !== this.bitstream.serialNumber) {
              y = F.headerStartPos + F.totalSize;
              continue;
            }
            if (F.granulePosition === -1) {
              y = F.headerStartPos + F.totalSize;
              continue;
            }
            this.granulePositionToTimestampInSamples(F.granulePosition) > s ? o = F.headerStartPos : (n = F, c.push(F));
            continue e;
          }
        }
      let l = a.startPage;
      for (const S of c) {
        if (S.granulePosition === n.granulePosition)
          break;
        (!l || S.headerStartPos > l.headerStartPos) && (l = S);
      }
      let d = l;
      const h = [d];
      for (; !(d.serialNumber === this.bitstream.serialNumber && d.granulePosition === n.granulePosition); ) {
        r.pos = d.headerStartPos + d.totalSize, await r.reader.loadRange(r.pos, r.pos + qe);
        const S = r.readPageHeader();
        f(S), d = S, d.serialNumber === this.bitstream.serialNumber && h.push(d);
      }
      f(d.granulePosition !== -1);
      let u = null, m, p, g = d, w = 0;
      if (d.headerStartPos === a.startPage.headerStartPos)
        m = this.granulePositionToTimestampInSamples(0), p = !0, u = 0;
      else {
        m = 0, p = !1;
        for (let y = d.lacingValues.length - 1; y >= 0; y--)
          if (d.lacingValues[y] < 255) {
            u = y + 1;
            break;
          }
        if (u === null)
          throw new Error("Invalid page with granule position: no packets end on this page.");
        w = u - 1;
        const S = {
          data: me,
          endPage: g,
          endSegmentIndex: w
        };
        if (await this.demuxer.findNextPacketStart(r, S)) {
          const y = xs(h, d, u);
          f(y);
          const T = Ts(
            h,
            y.page,
            y.segmentIndex
          );
          T && (d = T.page, u = T.segmentIndex);
        } else
          for (; ; ) {
            const y = xs(
              h,
              d,
              u
            );
            if (!y)
              break;
            const T = Ts(
              h,
              y.page,
              y.segmentIndex
            );
            if (!T)
              break;
            if (d = T.page, u = T.segmentIndex, y.page.headerStartPos !== g.headerStartPos) {
              g = y.page, w = y.segmentIndex;
              break;
            }
          }
      }
      let b = null, x = null;
      for (; d !== null; ) {
        f(u !== null);
        const S = await this.demuxer.readPacket(r, d, u);
        if (!S)
          break;
        if (!(d.headerStartPos === a.startPage.headerStartPos && u < a.startSegmentIndex)) {
          let T = this.createEncodedPacketFromOggPacket(
            S,
            {
              timestampInSamples: m,
              vorbisLastBlocksize: x?.vorbisBlockSize ?? null
            },
            e
          );
          f(T);
          let P = this.encodedPacketToMetadata.get(T);
          if (f(P), !p && S.endPage.headerStartPos === g.headerStartPos && S.endSegmentIndex === w ? (m = this.granulePositionToTimestampInSamples(
            d.granulePosition
          ), p = !0, T = this.createEncodedPacketFromOggPacket(
            S,
            {
              timestampInSamples: m - P.durationInSamples,
              vorbisLastBlocksize: x?.vorbisBlockSize ?? null
            },
            e
          ), f(T), P = this.encodedPacketToMetadata.get(T), f(P)) : m += P.durationInSamples, b = T, x = P, p && // Next timestamp will be too late
          (Math.max(m, 0) > s || Math.max(P.timestampInSamples, 0) === s))
            break;
        }
        const y = await this.demuxer.findNextPacketStart(r, S);
        if (!y)
          break;
        d = y.startPage, u = y.startSegmentIndex;
      }
      return b;
    } finally {
      i();
    }
  }
  getKeyPacket(t, e) {
    return this.getPacket(t, e);
  }
  getNextKeyPacket(t, e) {
    return this.getNextPacket(t, e);
  }
}, Ts = (t, e, i) => {
  let s = e, r = i;
  e:
    for (; ; ) {
      for (r--, r; r >= 0; r--)
        if (s.lacingValues[r] < 255) {
          r++;
          break e;
        }
      if (f(r === -1), !(s.headerType & 1)) {
        r = 0;
        break;
      }
      const n = Zs(
        t,
        (o) => o.headerStartPos < s.headerStartPos
      );
      if (!n)
        return null;
      s = n, r = s.lacingValues.length;
    }
  if (f(r !== -1), r === s.lacingValues.length) {
    const a = t[t.indexOf(s) + 1];
    f(a), s = a, r = 0;
  }
  return { page: s, segmentIndex: r };
}, xs = (t, e, i) => {
  if (i > 0)
    return { page: e, segmentIndex: i - 1 };
  const s = Zs(
    t,
    (r) => r.headerStartPos < e.headerStartPos
  );
  return s ? { page: s, segmentIndex: s.lacingValues.length - 1 } : null;
}, Je = class {
}, Jr = class extends Je {
  /** @internal */
  async _getMajorBrand(t) {
    if (await t._mainReader.source.getSize() < 12)
      return null;
    const i = new Ti(t._mainReader);
    return i.pos = 4, i.readAscii(4) !== "ftyp" ? null : i.readAscii(4);
  }
  /** @internal */
  _createDemuxer(t) {
    return new gc(t);
  }
}, Bc = class extends Jr {
  /** @internal */
  async _canReadInput(t) {
    const e = await this._getMajorBrand(t);
    return !!e && e !== "qt  ";
  }
  getName() {
    return "MP4";
  }
  getMimeType() {
    return "video/mp4";
  }
}, Oc = class extends Jr {
  /** @internal */
  async _canReadInput(t) {
    return await this._getMajorBrand(t) === "qt  ";
  }
  getName() {
    return "QuickTime File Format";
  }
  getMimeType() {
    return "video/quicktime";
  }
}, Zr = class extends Je {
  /** @internal */
  async isSupportedEBMLOfDocType(t, e) {
    if (await t._mainReader.source.getSize() < 8)
      return !1;
    const s = new wi(t._mainReader), r = s.readVarIntSize();
    if (r < 1 || r > 8 || s.readUnsignedInt(r) !== 440786851)
      return !1;
    const n = s.readElementSize();
    if (n === null)
      return !1;
    const o = s.pos;
    for (; s.pos < o + n; ) {
      const { id: c, size: l } = s.readElementHeader(), d = s.pos;
      if (l === null) return !1;
      switch (c) {
        case 17030:
          if (s.readUnsignedInt(l) !== 1)
            return !1;
          break;
        case 17143:
          if (s.readUnsignedInt(l) !== 1)
            return !1;
          break;
        case 17026:
          if (s.readString(l) !== e)
            return !1;
          break;
        case 17031:
          if (s.readUnsignedInt(l) > 4)
            return !1;
          break;
      }
      s.pos = d + l;
    }
    return !0;
  }
  /** @internal */
  _canReadInput(t) {
    return this.isSupportedEBMLOfDocType(t, "matroska");
  }
  /** @internal */
  _createDemuxer(t) {
    return new Tc(t);
  }
  getName() {
    return "Matroska";
  }
  getMimeType() {
    return "video/x-matroska";
  }
}, Mc = class extends Zr {
  /** @internal */
  _canReadInput(t) {
    return this.isSupportedEBMLOfDocType(t, "webm");
  }
  getName() {
    return "WebM";
  }
  getMimeType() {
    return "video/webm";
  }
}, Ac = class extends Je {
  /** @internal */
  async _canReadInput(t) {
    const e = await t._mainReader.source.getSize();
    if (e < 4)
      return !1;
    const i = new Yr(t._mainReader);
    i.fileSize = e;
    const s = i.readId3();
    s && (i.pos += s.size);
    const r = i.pos;
    await i.reader.loadRange(i.pos, i.pos + 4096);
    const a = i.readNextFrameHeader(r + 4096);
    if (!a)
      return !1;
    if (s)
      return !0;
    i.pos = a.startPos + a.totalSize;
    const n = i.readNextFrameHeader(r + 4096);
    return !(!n || a.channel !== n.channel || a.sampleRate !== n.sampleRate);
  }
  /** @internal */
  _createDemuxer(t) {
    return new Ec(t);
  }
  getName() {
    return "MP3";
  }
  getMimeType() {
    return "audio/mpeg";
  }
}, zc = class extends Je {
  /** @internal */
  async _canReadInput(t) {
    if (await t._mainReader.source.getSize() < 12)
      return !1;
    const i = new vi(t._mainReader), s = i.readAscii(4);
    return s !== "RIFF" && s !== "RIFX" ? !1 : (i.pos = 8, i.readAscii(4) === "WAVE");
  }
  /** @internal */
  _createDemuxer(t) {
    return new sc(t);
  }
  getName() {
    return "WAVE";
  }
  getMimeType() {
    return "audio/wav";
  }
}, Uc = class extends Je {
  /** @internal */
  async _canReadInput(t) {
    return await t._mainReader.source.getSize() < 4 ? !1 : new Vr(t._mainReader).readAscii(4) === "OggS";
  }
  /** @internal */
  _createDemuxer(t) {
    return new Fc(t);
  }
  getName() {
    return "Ogg";
  }
  getMimeType() {
    return "application/ogg";
  }
}, Dc = new Bc(), Nc = new Oc(), Vc = new Zr(), Lc = new Mc(), Wc = new Ac(), Hc = new zc(), $c = new Uc(), Cs = [Dc, Nc, Vc, Lc, Hc, $c, Wc], _s = class {
  constructor(t) {
    if (this._demuxerPromise = null, this._format = null, !t || typeof t != "object")
      throw new TypeError("options must be an object.");
    if (!Array.isArray(t.formats) || t.formats.some((e) => !(e instanceof Je)))
      throw new TypeError("options.formats must be an array of InputFormat.");
    if (!(t.source instanceof ji))
      throw new TypeError("options.source must be a Source.");
    this._formats = t.formats, this._source = t.source, this._mainReader = new gt(t.source);
  }
  /** @internal */
  _getDemuxer() {
    return this._demuxerPromise ??= (async () => {
      await this._mainReader.loadRange(0, 4096);
      for (const t of this._formats)
        if (await t._canReadInput(this))
          return this._format = t, t._createDemuxer(this);
      throw new Error("Input has an unsupported or unrecognizable format.");
    })();
  }
  /**
   * Returns the source from which this input file reads its data. This is the same source that was passed to the
   * constructor.
   */
  get source() {
    return this._source;
  }
  /**
   * Returns the format of the input file. You can compare this result directly to the InputFormat singletons or use
   * `instanceof` checks for subset-aware logic (for example, `format instanceof MatroskaInputFormat` is true for
   * both MKV and WebM).
   */
  async getFormat() {
    return await this._getDemuxer(), f(this._format), this._format;
  }
  /**
   * Computes the duration of the longest track in this input file, in seconds. More precisely, returns the largest
   * end timestamp among all tracks.
   */
  async computeDuration() {
    return (await this._getDemuxer()).computeDuration();
  }
  /** Returns the list of all tracks of this input file. */
  async getTracks() {
    return (await this._getDemuxer()).getTracks();
  }
  /** Returns the list of all video tracks of this input file. */
  async getVideoTracks() {
    return (await this.getTracks()).filter((e) => e.isVideoTrack());
  }
  /** Returns the primary video track of this input file, or null if there are no video tracks. */
  async getPrimaryVideoTrack() {
    return (await this.getTracks()).find((e) => e.isVideoTrack()) ?? null;
  }
  /** Returns the list of all audio tracks of this input file. */
  async getAudioTracks() {
    return (await this.getTracks()).filter((e) => e.isAudioTrack());
  }
  /** Returns the primary audio track of this input file, or null if there are no audio tracks. */
  async getPrimaryAudioTrack() {
    return (await this.getTracks()).find((e) => e.isAudioTrack()) ?? null;
  }
  /** Returns the full MIME type of this input file, including track codecs. */
  async getMimeType() {
    return (await this._getDemuxer()).getMimeType();
  }
};
const ri = 3e3;
class jc extends js() {
  input;
  buffers = /* @__PURE__ */ new Map();
  constructor(e) {
    super(), this.input = e;
  }
  /**
   * Decodes an audio file or URL and returns a resampled AudioBuffer.
   * @param input - Either a File, Blob, or URL string.
   * @returns Promise<AudioBuffer>
   */
  async decode(e, i, s = !1) {
    let r = this.buffers.get(`${i}-${e}`);
    if (r) return r;
    let a;
    if (e || i ? a = new OfflineAudioContext(e ?? 2, 1, i ?? 44100) : a = new AudioContext(), typeof this.input == "string") {
      const o = await (await fetch(this.input)).arrayBuffer();
      r = await a.decodeAudioData(o);
    } else if (this.input instanceof Blob)
      r = await a.decodeAudioData(await this.input.arrayBuffer());
    else {
      const n = await this.input.getFile();
      r = await a.decodeAudioData(await n.arrayBuffer());
    }
    return s && this.buffers.set(`${i}-${e}`, r), r;
  }
  async dispose() {
    this.buffers.clear();
  }
}
function qc(t, e = {}) {
  const { threshold: i = 0.02, hopSize: s = 1024, minDuration: r = 500 } = e, a = [], n = t.getChannelData(0), o = t.sampleRate, c = Math.floor(r / 1e3 * o);
  let l = null, d = 0;
  for (let h = 0; h < n.length; h += s) {
    let u = 0;
    const m = Math.min(h + s, n.length);
    for (let p = h; p < m; p++)
      u += n[p] * n[p];
    u = Math.sqrt(u / (m - h)), u < i ? (d += s, l === null && (l = h)) : (l !== null && d >= c && a.push({
      start: new E(0, l / o),
      stop: new E(0, h / o)
    }), l = null, d = 0);
  }
  return l !== null && d >= c && a.push({
    start: new E(0, l / o),
    stop: new E(0, n.length / o)
  }), a;
}
var Qc = Object.defineProperty, Xc = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && Qc(e, i, r), r;
};
class Ze extends le {
  type = "audio";
  element = new Audio();
  decoder;
  duration;
  demuxer;
  transcript;
  constructor(e) {
    super(e), this.decoder = new jc(e.input);
  }
  async init(e = {}) {
    if (this.input instanceof Blob)
      this.element.src = URL.createObjectURL(this.input);
    else if (this.input instanceof FileSystemFileHandle) {
      const i = await this.input.getFile();
      this.element.src = URL.createObjectURL(i);
    } else
      this.element.src = this.input;
    await new Promise((i, s) => {
      this.element.readyState >= 3 && i(!0), this.element.addEventListener("loadedmetadata", () => {
        i(!0);
      }), this.element.addEventListener("error", () => {
        s(new ee({
          message: "Failed to load audio",
          code: "failed_to_load_audio"
        }));
      });
    }), this.duration = new E(0, this.element.duration), e.prefetch && typeof this.input == "string" ? this.demuxer = fetch(this.input).then((i) => i.blob()).then((i) => new _s({
      formats: Cs,
      source: new ks(i)
    })) : this.demuxer = Promise.resolve(new _s({
      formats: Cs,
      source: typeof this.input == "string" ? new pc(this.input) : new ks(
        this.input instanceof Blob ? this.input : await this.input.getFile()
      )
    }));
  }
  async decode(e = 2, i = 48e3, s = !1) {
    return this.decoder.decode(e, i, s);
  }
  /**
   * Find silences in the audio clip. Results are cached.
   * 
   * uses default sample rate of 3000
   * @param options - Silences options.
   * @returns An array of the silences (in ms) in the clip.
   */
  async silences(e = {}) {
    const i = await this.decode(1, 24e3);
    return qc(i, e);
  }
  /**
   * Sampler that uses a window size to calculate the max value of the samples in the window.
   * @param options - Sampling options.
   * @returns An array of the max values of the samples in the window.
   */
  async sample({
    length: e = 60,
    start: i = 0,
    stop: s,
    logarithmic: r = !1
  } = {}) {
    typeof i == "object" && (i = i.millis), typeof s == "object" && (s = s.millis);
    const a = await this.decode(1, ri, !0), n = a.getChannelData(0), o = Math.floor(Math.max(i * ri / 1e3, 0)), c = s ? Math.floor(Math.min(s * ri / 1e3, a.length)) : a.length, l = Math.floor((c - o) / e), d = new Float32Array(e);
    for (let h = 0; h < e; h++) {
      const u = o + h * l, m = u + l;
      let p = Number.NEGATIVE_INFINITY;
      for (let g = u; g < m; g++) {
        const w = n[g];
        w > p && (p = w);
      }
      d[h] = r ? Math.log2(1 + p) : p;
    }
    return d;
  }
  async thumbnail(e = {}) {
    const i = await this.sample(e), s = document.createElement("div");
    s.className = "flex flex-row absolute inset-0 audio-samples";
    for (const r of i) {
      const a = document.createElement("div");
      a.className = "audio-sample-item", a.style.height = `${r * 100}%`, s.appendChild(a);
    }
    return s;
  }
}
Xc([
  v(se)
], Ze.prototype, "transcript");
class qi extends Pi(Ze) {
  type = "video";
  element = document.createElement("video");
  async init(e = {}) {
    await super.init(e), this.height = this.element.videoHeight, this.width = this.element.videoWidth;
  }
  async thumbnail() {
    return this.element.className = "object-cover w-full aspect-video h-auto", this.element.controls = !1, this.element.addEventListener("mousemove", (e) => {
      const i = this.element.getBoundingClientRect(), s = e.clientX - (i?.left ?? 0), r = this.element.duration;
      r && i && i.width > 0 && (this.element.currentTime = Math.round(r * (s / i.width)));
    }), this.element;
  }
  async *thumbnailsInRange(e) {
    const s = await (await this.demuxer)?.getPrimaryVideoTrack();
    if (!s)
      return;
    const r = O(e.startTime).seconds, a = O(e.endTime).seconds, n = Array.from({ length: e.count }, (c, l) => r + l * (a - r) / e.count);
    yield* new Jo(s, {
      ...e,
      fit: "cover"
    }).canvasesAtTimestamps(n);
  }
}
class Qi extends Pi(le) {
  type = "image";
  element = new Image();
  async init() {
    if (typeof this.input == "string") {
      const e = await fetch(this.input);
      if (!e.ok) throw new Ee({
        message: "Failed to load image",
        code: "failed_to_load_image"
      });
      this.element.src = URL.createObjectURL(await e.blob());
    } else this.input instanceof Blob ? this.element.src = URL.createObjectURL(this.input) : this.element.src = URL.createObjectURL(await this.input.getFile());
    await new Promise((e, i) => {
      this.element.onload = () => {
        this.height = this.element.naturalHeight, this.width = this.element.naturalWidth, e();
      }, this.element.onerror = () => i(new Ee({
        message: "Failed to load image",
        code: "failed_to_load_image"
      }));
    });
  }
  async thumbnail() {
    return this.element.className = "object-cover w-full aspect-video h-auto", this.element;
  }
}
const Is = "data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg'%3E%3C/svg%3E";
function Kc(t) {
  const e = new TextEncoder().encode(t);
  let i = "";
  const s = e.byteLength;
  for (let r = 0; r < s; r++)
    i += String.fromCharCode(e[r]);
  return btoa(i);
}
function Gc(t) {
  if (!t || !t.body) return Is;
  const e = t.body.scrollWidth, i = t.body.scrollHeight, s = t.cloneNode(!0), r = s.getElementsByTagName("style").item(0), a = s.getElementsByTagName("body").item(0);
  if (a?.setAttribute("xmlns", "http://www.w3.org/1999/xhtml"), !a) return Is;
  const n = new XMLSerializer(), o = r ? n.serializeToString(r) : "", c = n.serializeToString(a), l = `
  <svg xmlns="http://www.w3.org/2000/svg" width="${e}" height="${i}">
		body { padding: 0; }
    ${o}
    <foreignObject width="100%" height="100%">
      ${c}
    </foreignObject>
  </svg>`;
  return "data:image/svg+xml;base64," + Kc(l);
}
class Xi extends Pi(le) {
  type = "html";
  element = document.createElement("iframe");
  constructor(e) {
    super(e), this.element.style.position = "absolute", this.element.style.width = "0", this.element.style.height = "0", this.element.style.border = "0", this.element.style.visibility = "hidden", document.body.appendChild(this.element);
  }
  async init() {
    if (typeof this.input == "string" && this.input.startsWith("<html>"))
      this.element.srcdoc = this.input;
    else if (typeof this.input == "string") {
      const e = await fetch(this.input);
      if (!e.ok) throw new Ee({
        message: "Failed to load html",
        code: "failed_to_load_html"
      });
      this.element.src = URL.createObjectURL(await e.blob());
    } else this.input instanceof Blob ? this.element.src = URL.createObjectURL(this.input) : this.element.src = URL.createObjectURL(await this.input.getFile());
    await new Promise((e, i) => {
      this.element.onload = () => {
        this.height = this.element.contentWindow?.document.body.scrollHeight ?? 1, this.width = this.element.contentWindow?.document.body.scrollWidth ?? 1, e();
      }, this.element.onerror = () => i(new Ee({
        message: "Failed to load html",
        code: "failed_to_load_html"
      }));
    });
  }
  /**
   * Access to the html document as loaded
   * within the iframe. Can be manipulated with
   * javascript
   */
  get document() {
    return this.element.contentWindow?.document;
  }
  get imageUrl() {
    return this.height = this.element.contentWindow?.document.body.scrollHeight ?? 1, this.width = this.element.contentWindow?.document.body.scrollWidth ?? 1, Gc(this.document);
  }
  async thumbnail() {
    const e = new Image();
    return e.className = "object-contain w-full aspect-video h-auto", e.src = this.imageUrl, e;
  }
}
class Oe {
  static async from(e, i = {}) {
    const s = await Fa(e);
    let r;
    if (s.startsWith("audio/"))
      r = new Ze({ input: e, mimeType: s, name: i.name });
    else if (s.startsWith("video/"))
      r = new qi({ input: e, mimeType: s, name: i.name });
    else if (s.startsWith("image/"))
      r = new Qi({ input: e, mimeType: s, name: i.name });
    else if (s.startsWith("text/html"))
      r = new Xi({ input: e, mimeType: s, name: i.name });
    else
      throw new ee({
        message: "Unsupported file type",
        code: "unsupported_file_type"
      });
    return await r.init(i), r;
  }
  /**
   * Restore a checkpoint of the source.
   * @param checkpoint The checkpoint to restore
   * @param sources The sources to use for the restoration
   * @returns The restored source
   */
  static async restoreCheckpoint(e, i) {
    B(typeof e == "object"), B(e != null), B("input" in e), B("id" in e), B(
      typeof e.input == "string" || e.input instanceof Blob || e.input instanceof FileSystemFileHandle,
      "Source input must be a string or Blob or FileSystemFileHandle"
    );
    const s = await Oe.from(e.input, i);
    return s.fromJSON(e), s;
  }
}
var Yc = Object.defineProperty, oe = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && Yc(e, i, r), r;
};
class $ extends dt(Te) {
  id = crypto.randomUUID();
  _name;
  _delay = new E();
  _duration = new E(0, 16);
  data = {};
  /**
   * Flag to check if the clip has been initialized
   */
  initialized = !1;
  type = "base";
  source;
  createdAt = /* @__PURE__ */ new Date();
  disabled = !1;
  animations = [];
  /**
   * Access the parent layer
   */
  layer;
  /**
   * The input that was used to create the clip
   */
  input;
  transition;
  /**
   * Human readable identifier of the clip
   */
  get name() {
    return this._name ?? this.source?.name;
  }
  set name(e) {
    this._name = e;
  }
  /**
   * Get the first visible frame
   */
  get start() {
    return this._delay;
  }
  /**
   * Get the last visible frame
   */
  get stop() {
    return this._delay.add(this._duration);
  }
  /**
   * Get the delay of the clip
   */
  get delay() {
    return this._delay;
  }
  /**
   * Get the duration of the clip
   */
  get duration() {
    return this._duration;
  }
  constructor(e = {}) {
    super(), Object.assign(this, e);
  }
  /**
   * Set the animation time of the clip
   * and interpolate the values
   * @param time the current absolute time to render
   */
  animate(e) {
    return this;
  }
  /**
   * Method for connecting the layer with the clip
   */
  async connect(e) {
    this.layer = e, this.emit("attach", void 0);
  }
  /**
   * Change clip's offset to zero in seconds. Can be negative
   */
  set delay(e) {
    this._delay = O(e), this.emit("frame", this._delay.frames);
  }
  /**
   * Set the duration of the clip, needs to be positive
   */
  set duration(e) {
    this._duration = O(e), this._duration.millis <= 0 && (this._duration.frames = 1), this.emit("frame", this._duration.frames);
  }
  /**
   * Offsets the clip by a given frame number
   */
  offset(e) {
    return e = O(e), this.delay.addMillis(e.millis), this.emit("offset", e), this.emit("frame", void 0), this;
  }
  /**
   * Triggered when the clip is
   * added to the composition
   */
  async init(e) {
  }
  /**
   * Triggered when the clip enters the scene
   */
  async enter(e) {
  }
  /**
   * Triggered for each redraw of the scene.
   */
  async update(e) {
  }
  /**
   * Triggered after the clip was updated
   */
  render(e) {
  }
  /**
   * Triggered when the clip exits the scene
   */
  async exit(e) {
  }
  /**
   * Seek the clip to a specific absolute time
   */
  async seek(e) {
  }
  /**
   * Play the clip
   */
  async play(e) {
  }
  /**
   * Pause the clip
   */
  async pause(e) {
  }
  /**
   * Remove the clip from the layer
   */
  detach() {
    return this.layer?.remove(this), this;
  }
  /**
   * Cleanup the clip after it has been removed from the layer
   */
  cleanup() {
    this.off("*");
  }
  /**
   * Trim the clip to the specified start and stop
   */
  trim(e = this.start, i = this.stop) {
    return e = O(e), i = O(i), this.delay = e, this.duration = i.subtract(e), this;
  }
  /**
   * Split the clip into two clips at the specified time
   * @param time split, will use the current frame of the composition 
   * a fallback
   * @returns The clip that was created by performing this action
   */
  async split(e) {
    if (e = O(e ?? this.layer?.composition?.audio.playbackFrame), !e || e.millis <= this.start.millis || e.millis >= this.stop.millis)
      throw new ee({
        code: "split_out_of_range",
        message: "Cannot split clip at the specified time"
      });
    if (!this.layer)
      throw new ee({
        code: "layer_not_attached",
        message: "Clip must be attached to a layer"
      });
    const i = this.animate(e).copy();
    this.duration = e.subtract(this.delay), i.trim(e.addMillis(1), i.stop), i.animations = [];
    const s = this.layer.clips.findIndex((r) => r.id == this.id);
    return await this.layer.add(i, s + 1), i;
  }
  /**
   * Create a copy of the clip. Will have receive a new id
   * but share the same source
   */
  copy() {
    const e = this.constructor.fromJSON(this.toJSON());
    return e.source = this.source, e.id = crypto.randomUUID(), e;
  }
  /**
   * Create a checkpoint of the clip. May include Blob or FileSystemFileHandle.
   * @param middleware A function to modify the checkpoint data
   * @returns A serialized representation of the clip
   */
  async createCheckpoint() {
    return this.toJSON();
  }
  /**
   * Restore a checkpoint of the clip.
   * @param checkpoint The checkpoint to restore
   * @param sources The sources to use for the restoration
   * @returns The restored clip
   */
  async restoreCheckpoint(e, i) {
    B(typeof e == "object"), B(e != null);
    let s = e, r;
    if ("source" in e && (B(e.source != null), B(typeof e.source == "object"), B("id" in e.source), { source: r, ...s } = e), r && r.id !== this.source?.id) {
      const a = i?.find((n) => n.id === r?.id);
      a ? this.source = a : (this.source = await Oe.restoreCheckpoint(r), i?.push(this.source));
    }
    return this.fromJSON(s), this;
  }
}
oe([
  v()
], $.prototype, "id");
oe([
  v(void 0, "name")
], $.prototype, "_name");
oe([
  v(E, "delay")
], $.prototype, "_delay");
oe([
  v(E, "duration")
], $.prototype, "_duration");
oe([
  v()
], $.prototype, "data");
oe([
  v()
], $.prototype, "type");
oe([
  v()
], $.prototype, "source");
oe([
  v(Ot)
], $.prototype, "createdAt");
oe([
  v()
], $.prototype, "disabled");
oe([
  v()
], $.prototype, "animations");
oe([
  v()
], $.prototype, "transition");
class Jc {
  static fromType(e) {
    switch (e.type) {
      case "video":
        return new zs();
      case "audio":
        return new pe();
      case "html":
        return new Us();
      case "image":
        return new Es();
      case "text":
        return new L();
      case "circle":
        return new xl();
      case "rectangle":
        return new ta();
      case "waveform":
        return new Il();
      default:
        return new $();
    }
  }
  static fromSource(e) {
    if (e.type == "video" && e instanceof qi)
      return new zs(e);
    if (e.type == "audio" && e instanceof Ze)
      return new pe(e);
    if (e.type == "image" && e instanceof Qi)
      return new Es(e);
    if (e.type == "html" && e instanceof Xi)
      return new Us(e);
  }
}
var Zc = Object.defineProperty, q = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && Zc(e, i, r), r;
};
function et(t) {
  class e extends t {
    _height;
    _width;
    _aspectRatio;
    _keepAspectRatio = !1;
    _mask;
    anchorX = 0;
    anchorY = 0;
    scaleX = 1;
    scaleY = 1;
    translateX = 0;
    translateY = 0;
    x = 0;
    y = 0;
    rotation = 0;
    opacity = 100;
    filter;
    blendMode;
    /**
     * 2D position offset of the clip.
     * @default { x: 0, y: 0 }
     */
    get translate() {
      return { x: this.translateX, y: this.translateY };
    }
    set translate(s) {
      typeof s == "number" ? (this.translateX = s, this.translateY = s) : (this.translateX = s.x, this.translateY = s.y);
    }
    /**
     * The anchor sets the origin point of the clip. Setting the anchor to (0.5,0.5) 
     * means the clips' origin is centered. Setting the anchor to (1,1) would mean 
     * the clips' origin point will be the bottom right corner. If you pass only 
     * single parameter, it will set both x and y to the same value.
     */
    get anchor() {
      return { x: this.anchorX, y: this.anchorY };
    }
    set anchor(s) {
      typeof s == "number" ? (this.anchorX = s, this.anchorY = s) : (this.anchorX = s.x, this.anchorY = s.y);
    }
    /**
     * The scale factors of this object along the local coordinate axes.
     * Will be added to the scale applied by setting height and/or width
     * @default { x: 1, y: 1 }
     */
    get scale() {
      return { x: this.scaleX, y: this.scaleY };
    }
    set scale(s) {
      typeof s == "number" ? (this.scaleX = s, this.scaleY = s) : (this.scaleX = s.x, this.scaleY = s.y);
    }
    /**
     * Defines a region of the clip to be masked.
     */
    get mask() {
      return this._mask;
    }
    set mask(s) {
      this._mask = s, this._mask?.connect(this);
    }
    /**
     * If true, the clip will keep the aspect ratio. 
     * Will be set to false if both height and width are set.
     * @default true
     */
    get keepAspectRatio() {
      return this._keepAspectRatio;
    }
    set keepAspectRatio(s) {
      if ((!s && this._keepAspectRatio && !this._height || !this._width) && (this._height = this.height, this._width = this.width), s && !this._keepAspectRatio && this._height && this._width) {
        const { width: r, height: a } = this.size;
        this._aspectRatio = r / a;
      }
      this._keepAspectRatio = s;
    }
    /**
     * Gets the effective aspect ratio, using custom ratio if set, otherwise falling back to source
     */
    get aspectRatio() {
      return this._aspectRatio ?? this.source?.aspectRatio ?? 1;
    }
    /**
     * Sets a custom aspect ratio that overrides the source aspect ratio when free transform is enabled
     */
    set aspectRatio(s) {
      this._aspectRatio = s;
    }
    /**
     * Gets the current height of the clip
     */
    get height() {
      if (!this._height && this._width && this.keepAspectRatio) {
        const s = this.layer?.composition?.width ?? Number.POSITIVE_INFINITY, r = this.layer?.composition?.height ?? Number.POSITIVE_INFINITY, a = _(this._width, s) / this.aspectRatio;
        return typeof this._width == "string" && this._width.endsWith("%") ? `${a / r * 100}%` : a;
      }
      return this._height ?? this.source?.height ?? 0;
    }
    set height(s) {
      s != null && this._width && this.keepAspectRatio && (this._width = void 0), this._height = s;
    }
    /**
     * Gets the current width of the clip
     */
    get width() {
      if (!this._width && this._height && this.keepAspectRatio) {
        const s = this.layer?.composition?.height ?? Number.POSITIVE_INFINITY, r = this.layer?.composition?.width ?? Number.POSITIVE_INFINITY, a = _(this._height, s) * this.aspectRatio;
        return typeof this._height == "string" && this._height.endsWith("%") ? `${a / r * 100}%` : a;
      }
      return this._width ?? this.source?.width ?? 0;
    }
    set width(s) {
      s != null && this._height && this.keepAspectRatio && (this._height = void 0), this._width = s;
    }
    /**
     * The coordinate of the object relative to the local coordinates of the parent.
     * @default { x: 0, y: 0 }
     */
    get position() {
      return { x: this.x, y: this.y };
    }
    set position(s) {
      typeof s == "string" ? (this.x = "50%", this.y = "50%", this.anchor = 0.5) : (this.x = s.x, this.y = s.y);
    }
    animate(s) {
      for (const r of this.animations) {
        const a = r?.frames[0].value;
        typeof a == "number" || typeof a == "string" && a.match(/^[0-9]+(\.[0-9]+)?%$/) ? this[r.key] = qs(r, s.subtract(this.start)) : typeof a == "string" && a.match(/^#[0-9abcdef]{3,8}$/i) ? this[r.key] = ba(r, s.subtract(this.start)) : typeof a == "string" && (this[r.key] = ka(r, s.subtract(this.start)));
      }
      return this.mask?.animate(s), this;
    }
    /**
     * Gets the size of the clip in absolute units
     */
    get size() {
      const s = this.layer?.composition?.width ?? Number.POSITIVE_INFINITY, r = this.layer?.composition?.height ?? Number.POSITIVE_INFINITY;
      return {
        width: _(this.width, s),
        height: _(this.height, r)
      };
    }
    /**
     * Calculates the absolute coordinates of the shape's corners, taking into account
     * position, anchor point, translation, scale, and rotation. Assumes scale of 1.
     * @returns Object containing the coordinates of all four corners
     */
    get bounds() {
      let { width: s, height: r } = this.size;
      s *= this.scaleX, r *= this.scaleY;
      const a = this.layer?.composition?.width ?? Number.POSITIVE_INFINITY, n = this.layer?.composition?.height ?? Number.POSITIVE_INFINITY, o = _(this.x, a), c = _(this.y, n), l = o + this.translateX, d = c + this.translateY, h = s * this.anchorX, u = r * this.anchorY, m = [
        { x: -h, y: -u },
        // Top-left
        { x: s - h, y: -u },
        // Top-right
        { x: s - h, y: r - u },
        // Bottom-right
        { x: -h, y: r - u }
        // Bottom-left
      ];
      if (this.rotation !== 0) {
        const p = this.rotation * Math.PI / 180, g = Math.cos(p), w = Math.sin(p);
        for (const b of m) {
          const x = b.x, S = b.y;
          b.x = x * g - S * w, b.y = x * w + S * g;
        }
      }
      for (const p of m)
        p.x += l, p.y += d;
      return m;
    }
    /**
     * @deprecated Use keepAspectRatio instead
     */
    get freeTransform() {
      return !this.keepAspectRatio;
    }
    set freeTransform(s) {
      this.keepAspectRatio = !s;
    }
  }
  return q([
    v(void 0, "height")
  ], e.prototype, "_height"), q([
    v(void 0, "width")
  ], e.prototype, "_width"), q([
    v(void 0, "aspectRatio")
  ], e.prototype, "_aspectRatio"), q([
    v(void 0, "keepAspectRatio")
  ], e.prototype, "_keepAspectRatio"), q([
    v(Ta, "mask")
  ], e.prototype, "_mask"), q([
    v()
  ], e.prototype, "anchorX"), q([
    v()
  ], e.prototype, "anchorY"), q([
    v()
  ], e.prototype, "scaleX"), q([
    v()
  ], e.prototype, "scaleY"), q([
    v()
  ], e.prototype, "translateX"), q([
    v()
  ], e.prototype, "translateY"), q([
    v()
  ], e.prototype, "x"), q([
    v()
  ], e.prototype, "y"), q([
    v()
  ], e.prototype, "rotation"), q([
    v()
  ], e.prototype, "opacity"), q([
    v()
  ], e.prototype, "filter"), q([
    v()
  ], e.prototype, "blendMode"), e;
}
class Es extends et($) {
  _keepAspectRatio = !0;
  type = "image";
  animations = [];
  constructor(e, i = {}) {
    super(), e instanceof Qi ? this.source = e : this.input = e, Object.assign(this, i);
  }
  async init() {
    this.input && !this.source && (this.source = await Oe.from(this.input));
  }
  render(e) {
    if (!this.source) return;
    const { width: i, height: s } = this.size;
    e.save().blendMode(this.blendMode).clip(this.mask).filter(this.filter).opacity(this.opacity).transform({ translate: this }).transform({
      translate: this.translate,
      rotate: this.rotation,
      scale: this.scale
    }).transform({
      translate: {
        x: -this.anchor.x * i,
        y: -this.anchor.y * s
      }
    }).image(this.source.element, { width: i, height: s }).restore();
  }
}
function el(t, e) {
  return e == "lower" ? t.toLocaleLowerCase() : e == "upper" ? t.toUpperCase() : t;
}
function tl(t, e = []) {
  if (e.length === 0)
    return [{ words: kt(t) }];
  e.sort((r, a) => r.start - a.start);
  const i = [];
  let s = 0;
  for (const r of e) {
    if (r.start > s) {
      const o = t.slice(s, r.start);
      o && i.push({ words: kt(o) });
    }
    const a = r.stop ?? t.length, n = t.slice(r.start, a);
    n && i.push({
      words: kt(n),
      style: r.style
    }), s = a;
  }
  if (s < t.length) {
    const r = t.slice(s);
    r && i.push({ words: kt(r) });
  }
  return i;
}
function kt(t) {
  return t.indexOf(" ") === -1 ? [t] : t.match(/[^]*? |[^]+$/g) || [t];
}
class il {
  offset;
  chars;
  metrics;
  style;
  padding = { x: 0, y: 0 };
  line = {
    offsetX: 0,
    offsetY: 0,
    baseline: 0,
    height: 0
  };
  constructor(e) {
    this.chars = e.chars, this.offset = e.offset, this.metrics = e.metrics, this.style = e.style ?? {};
  }
  get width() {
    return this.metrics.width | 0;
  }
  get height() {
    return this.metrics.fontBoundingBoxAscent + this.metrics.fontBoundingBoxDescent | 0;
  }
  get x() {
    return this.offset + this.padding.x + this.line.offsetX | 0;
  }
  get y() {
    return this.line.offsetY + this.line.baseline + this.padding.y | 0;
  }
  get bounds() {
    return {
      top: this.line.offsetY + this.padding.y + this.line.baseline - this.metrics.actualBoundingBoxAscent | 0,
      right: this.offset + this.line.offsetX + this.padding.x + this.metrics.actualBoundingBoxRight | 0,
      bottom: this.line.offsetY + this.padding.y + this.line.baseline + this.metrics.actualBoundingBoxDescent | 0,
      left: this.offset + this.line.offsetX + this.padding.x - this.metrics.actualBoundingBoxLeft | 0
    };
  }
}
const Ps = 1, Fs = 3, sl = 3, Rs = 8, Bs = 2, Os = 10, rl = 16, al = "sans-serif", nl = "normal", ol = "400";
class Ms {
  text = "";
  font = {
    family: al,
    size: rl
  };
  color = "#FFFFFF";
  maxWidth = Number.MAX_SAFE_INTEGER;
  resolution = 1;
  leading = 1;
  opacity = 100;
  casing;
  align = "left";
  baseline = "top";
  shadows = [];
  strokes = [];
  styles = [];
  glow;
}
class cl extends Ms {
  isDirty = !0;
  canvas = document.createElement("canvas");
  ctx;
  padding = {
    x: 0,
    y: 0
  };
  lines = [];
  /**
   * Create a new Text instance
   * @param options Configuration options
   */
  constructor({ alpha: e = !0, willReadFrequently: i = !1, ...s } = {}) {
    super();
    const r = this.canvas.getContext("2d", {
      alpha: e,
      willReadFrequently: i,
      ...s
    });
    if (!r)
      throw new Error("Could not get 2D context for shared canvas");
    this.ctx = r, this.ctx.imageSmoothingEnabled = !1, this.ctx.textAlign = "left";
    const a = Object.keys(new Ms());
    return new Proxy(this, {
      set: (n, o, c) => {
        if (a.includes(o.toString())) {
          const l = n[o];
          typeof c == "object" && c !== null ? (!l || JSON.stringify(l) !== JSON.stringify(c)) && (n.isDirty = !0) : l !== c && (n.isDirty = !0);
        }
        return n[o] = c, !0;
      }
    });
  }
  get width() {
    return this.canvas.width / this.resolution;
  }
  get height() {
    return this.canvas.height / this.resolution;
  }
  get fontSize() {
    return this.font.size;
  }
  set fontSize(e) {
    this.font.size = e;
  }
  /**
   * Draw/update the text on the shared canvas if needed
   * @param options Optional parameters to update text, fontSize, and color
   */
  render() {
    this.isDirty && (this.lines = this.getLines(), this.alignLines(this.lines), this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height), this.renderLines(this.lines), this.isDirty = !1);
  }
  alignLines(e) {
    const i = e.map((p) => p.reduce((g, w) => g + w.width, 0)), s = e.map((p) => Math.max(...p.map((g) => g.height))), r = Math.max(...i), a = s.reduce((p, g, w) => p + g * (w < s.length - 1 ? this.leading : 1), 0);
    let n = 0, o = 0, c = 0, l = 0, d = 0;
    this.shadows.length && (n = Math.max(...this.shadows.map((p) => (p?.blur ?? Rs) * this.resolution), 0), o = Math.max(...this.shadows.map((p) => Math.abs((p?.offsetX ?? 0) * this.resolution)), 0) + n, c = Math.max(...this.shadows.map((p) => Math.abs((p?.offsetY ?? 0) * this.resolution)), 0) + n), this.strokes.length && (l = Math.max(...this.strokes.map((p) => (p.width ?? Bs) * this.resolution * Fs * 0.5), 0)), this.glow && (d = (this.glow?.radius ?? Os) * this.resolution * 2), this.padding = {
      x: Ps * 2 + Math.max(o + l, d) | 0,
      y: Ps * 2 + Math.max(c + l, d) | 0
    };
    const h = r + this.padding.x * 2 | 0, u = a + this.padding.y * 2 | 0;
    (this.canvas.width !== h || this.canvas.height !== u) && (this.canvas.width = h, this.canvas.height = u, this.ctx.imageSmoothingEnabled = !1);
    const m = {
      offsetX: 0,
      offsetY: 0,
      baseline: 0,
      height: 0
    };
    for (let p = 0; p < e.length; p++) {
      if (m.height = s[p], this.align === "left" ? m.offsetX = 0 : this.align === "center" ? m.offsetX = (r - i[p]) / 2 : this.align === "right" && (m.offsetX = r - i[p]), this.baseline === "top")
        m.baseline = 0;
      else if (this.baseline === "middle")
        m.baseline = s[p] / 2;
      else if (this.baseline === "bottom")
        m.baseline = s[p];
      else if (this.baseline === "alphabetic") {
        const g = Math.max(...e[p].map((w) => w.metrics.fontBoundingBoxAscent));
        m.baseline = g || s[p] * 0.75;
      }
      for (const g of e[p])
        g.line = { ...m }, g.padding = { ...this.padding };
      m.offsetY += s[p] * this.leading;
    }
  }
  renderLines(e) {
    const i = e.flat();
    this.ctx.save();
    for (const s of i) {
      this.applyFont({
        ...this.font,
        ...s.style?.font,
        fontSize: s.style?.fontSize,
        color: s.style?.color ?? this.color,
        opacity: s.style?.opacity ?? this.opacity
      });
      const r = (s.style?.strokes ?? this.strokes).at(0);
      r && this.applyStroke(r);
      for (const a of s.style?.shadows ?? this.shadows)
        this.applyShadow(a), (s.style?.strokes ?? this.strokes).length ? this.ctx.strokeText(s.chars, s.x, s.y) : this.ctx.fillText(s.chars, s.x, s.y);
    }
    this.ctx.restore(), this.ctx.save();
    for (const s of i) {
      this.applyFont({
        ...this.font,
        ...s.style?.font,
        fontSize: s.style?.fontSize
      });
      for (const r of s.style?.strokes ?? this.strokes)
        this.applyStroke(r), this.ctx.strokeText(s.chars, s.x, s.y);
    }
    this.ctx.restore(), this.ctx.save();
    for (const s of i)
      this.applyFont({
        ...this.font,
        ...s.style?.font,
        fontSize: s.style?.fontSize,
        color: s.style?.color ?? this.color,
        opacity: s.style?.opacity ?? this.opacity
      }), this.ctx.fillText(s.chars, s.x, s.y);
    this.ctx.restore(), this.ctx.save();
    for (const s of i) {
      this.applyFont({
        ...this.font,
        ...s.style?.font,
        fontSize: s.style?.fontSize,
        color: s.style?.color ?? this.color,
        opacity: s.style?.opacity ?? this.opacity
      });
      const r = s.style?.glow ?? this.glow;
      if (r) {
        this.applyGlow(r);
        for (let a = 0; a < (r.intensity ?? 1); a++)
          this.ctx.fillText(s.chars, s.x, s.y);
      }
    }
    this.ctx.restore();
  }
  /**
   * Split text into lines based on maxWidth
   */
  getLines() {
    const e = [[]];
    let i = 0;
    for (const { words: s, style: r } of tl(this.text, this.styles)) {
      this.applyFont({
        ...this.font,
        ...r?.font,
        fontSize: r?.fontSize
      });
      for (const a of s) {
        const n = a.split(`
`);
        for (let o = 0; o < n.length; o++) {
          const c = el(n[o], r?.casing ?? this.casing), l = this.ctx.measureText(c);
          i + l.width > this.maxWidth && (e.push([]), i = 0), e[e.length - 1].push(
            new il({ chars: c, style: r, metrics: l, offset: i })
          ), i += l.width, o < n.length - 1 && (e.push([]), i = 0);
        }
      }
    }
    return e;
  }
  applyFont({ style: e, weight: i, size: s, family: r, color: a, fontSize: n, opacity: o }) {
    this.ctx.font = `${e ?? nl} ${i ?? ol} ${(n ?? s) * sl * this.resolution}px ${r}`.trim(), this.ctx.fillStyle = a ? $e(a, o) : "transparent", this.ctx.textBaseline = this.baseline;
  }
  applyShadow({ offsetX: e = 0, offsetY: i = 0, blur: s = Rs, color: r = "#000000", opacity: a = 80 }) {
    this.ctx.shadowOffsetX = e * this.resolution, this.ctx.shadowOffsetY = i * this.resolution, this.ctx.shadowBlur = s * this.resolution, this.ctx.shadowColor = $e(r, a);
  }
  applyStroke({ width: e = Bs, color: i = "#000000", lineCap: s = "butt", lineJoin: r = "miter", miterLimit: a = 2, opacity: n = 100 }) {
    this.ctx.lineWidth = e * this.resolution * Fs, this.ctx.lineCap = s, this.ctx.lineJoin = r, this.ctx.miterLimit = a, this.ctx.strokeStyle = $e(i, n);
  }
  applyGlow({ radius: e = Os, opacity: i = 100 }) {
    this.ctx.filter = `blur(${e * this.resolution}px)`, this.ctx.globalAlpha = i / 100, this.ctx.globalCompositeOperation = "lighter";
  }
}
const ll = {
  center: 0.5,
  left: 0,
  right: 1
}, dl = {
  top: 0,
  middle: 0.5,
  alphabetic: 0.75,
  bottom: 1
};
var hl = Object.defineProperty, ul = Object.getOwnPropertyDescriptor, ie = (t, e, i, s) => {
  for (var r = s > 1 ? void 0 : s ? ul(e, i) : e, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = (s ? n(e, i, r) : n(r)) || r);
  return s && r && hl(e, i, r), r;
};
const fl = 0.95;
class L extends et($) {
  node = new cl();
  type = "text";
  animations = [];
  background;
  maxWidth;
  constructor(e = {}) {
    super(), Object.assign(this, e);
  }
  get text() {
    return this.node.text;
  }
  set text(e) {
    this.node.text = e;
  }
  get color() {
    return this.node.color;
  }
  set color(e) {
    this.node.color = e;
  }
  get font() {
    return this.node.font;
  }
  set font(e) {
    this.node.font = e;
  }
  get casing() {
    return this.node.casing;
  }
  set casing(e) {
    this.node.casing = e;
  }
  get align() {
    return this.node.align;
  }
  set align(e) {
    this.anchor = {
      x: ll[e],
      y: this.anchor.y
    }, this.node.align = e;
  }
  get baseline() {
    return this.node.baseline;
  }
  set baseline(e) {
    this.anchor = {
      x: this.anchor.x,
      y: dl[e]
    }, this.node.baseline = e;
  }
  /**
   * An object describing the stroke to apply
   */
  get stroke() {
    return this.node.strokes[0];
  }
  set stroke(e) {
    this.node.strokes = e ? [e] : [];
  }
  get strokes() {
    return this.node.strokes;
  }
  set strokes(e) {
    this.node.strokes = e;
  }
  /**
   * Set a drop shadow for the text.
   */
  get shadow() {
    return this.node.shadows[0];
  }
  set shadow(e) {
    Array.isArray(e) ? this.node.shadows = e : e ? this.node.shadows = [e] : this.node.shadows = [];
  }
  get shadows() {
    return this.node.shadows;
  }
  set shadows(e) {
    this.node.shadows = e;
  }
  get leading() {
    return this.node.leading;
  }
  set leading(e) {
    this.node.leading = e;
  }
  get glow() {
    return this.node.glow;
  }
  set glow(e) {
    this.node.glow = e;
  }
  get styles() {
    return this.node.styles;
  }
  set styles(e) {
    this.node.styles = e;
  }
  render(e) {
    this.node.resolution = e.resolution, this.node.maxWidth = _(this.maxWidth ?? Number.POSITIVE_INFINITY, e.width * e.resolution), this.node.render();
    const i = this.node.height - this.node.padding.y * 2, s = this.node.width - this.node.padding.x * 2;
    if (e.save().blendMode(this.blendMode).clip(this.mask).filter(this.filter).opacity(this.opacity).transform({ translate: this }).transform({
      translate: this.translate,
      rotate: this.rotation,
      scale: this.scale
    }), this.background) {
      const r = this.background.padding ?? { x: 30, y: 20 };
      e.rect({
        x: -(this.anchor.x * s) - r.x,
        y: -(this.anchor.y * i) - r.y,
        width: s + r.x * 2,
        height: i * fl + r.y * 2,
        radius: this.background.borderRadius ?? 20
      }).fill({
        color: this.background.fill ?? "#000000",
        opacity: this.background.opacity ?? 100
      }, !0);
    }
    e.transform({
      translate: {
        x: -(this.anchor.x * s) - this.node.padding.x,
        y: -(this.anchor.y * i) - this.node.padding.y
      }
    }).image(this.node.canvas, { width: this.node.width, height: this.node.height }).restore();
  }
  get width() {
    return this.node.width;
  }
  set width(e) {
    console.error("This method is not supported for text clips");
  }
  get height() {
    return this.node.height;
  }
  set height(e) {
    console.error("This method is not supported for text clips");
  }
  /**
   * Proxy for font.size
   */
  get fontSize() {
    return this.font.size;
  }
  set fontSize(e) {
    this.font.size = e;
  }
  get name() {
    return this.text;
  }
  get position() {
    return super.position;
  }
  set position(e) {
    super.position = e, e === "center" && (this.align = "center", this.baseline = "middle");
  }
  get lines() {
    return this.node.lines;
  }
}
ie([
  v()
], L.prototype, "background", 2);
ie([
  v()
], L.prototype, "maxWidth", 2);
ie([
  v()
], L.prototype, "text", 1);
ie([
  v()
], L.prototype, "color", 1);
ie([
  v()
], L.prototype, "font", 1);
ie([
  v()
], L.prototype, "casing", 1);
ie([
  v()
], L.prototype, "align", 1);
ie([
  v()
], L.prototype, "baseline", 1);
ie([
  v()
], L.prototype, "strokes", 1);
ie([
  v()
], L.prototype, "shadows", 1);
ie([
  v()
], L.prototype, "leading", 1);
ie([
  v()
], L.prototype, "glow", 1);
ie([
  v()
], L.prototype, "styles", 1);
class Sd extends L {
}
class ml {
  static fromJSON(e) {
    return B(Array.isArray(e)), B(typeof e[0] == "number" && typeof e[1] == "number"), [new E(e[0]), new E(e[1])];
  }
}
let pl = class {
  source;
  options;
  context;
  generator;
  constructor(e, i) {
    this.source = e, this.options = i;
  }
  /**
   * Initializes the audio decoder with the given audio context.
   * Sets up audio nodes and prepares the audio generator.
   */
  async init(e) {
    const s = await (await this.source.demuxer)?.getPrimaryAudioTrack();
    if (this.context = e, !s) return this;
    const { start: r, stop: a } = this.options;
    try {
      if (await s.canDecode()) {
        const n = new ic(s), o = await gl(n, r, a);
        this.generator = yl(o);
      } else {
        const n = await this.source.arrayBuffer(), o = await e.decodeAudioData(n);
        this.generator = wl(o, r, a);
      }
    } catch {
      console.error("Failed to decode audio using WebAudio API. Audio might not be available.");
    }
    return this;
  }
  async render(e) {
    if (!this.generator || !this.context) return;
    const { value: i } = await this.generator?.next() ?? {};
    if (!i?.buffer) return;
    const s = this.context.createGain();
    s.gain.value = this.options.volume, s.connect(this.context.destination);
    const r = this.context.createBufferSource();
    r.buffer = i.buffer, r.connect(s), r.start(Math.max(0, e + i.timestamp));
  }
};
async function gl(t, e, i) {
  const s = t.buffers(e, i), r = [];
  for await (const a of s)
    r.push(a);
  return r;
}
async function* wl(t, e, i) {
  const s = Math.floor(e * t.sampleRate), r = Math.floor(i * t.sampleRate), a = new AudioBuffer({
    length: r - s,
    numberOfChannels: t.numberOfChannels,
    sampleRate: t.sampleRate
  });
  for (let n = 0; n < t.numberOfChannels; n++) {
    const o = t.getChannelData(n), c = new Float32Array(o.slice(s, r));
    a.copyToChannel(c, n);
  }
  yield {
    buffer: a,
    timestamp: e,
    duration: i - e
  };
}
async function* yl(t) {
  if (!t.length) return;
  const e = t.reduce((a, n) => a + n.buffer.length, 0), i = t[0].buffer, s = new AudioBuffer({
    length: e,
    numberOfChannels: i.numberOfChannels,
    sampleRate: i.sampleRate
  });
  let r = 0;
  for (const { buffer: a } of t) {
    for (let n = 0; n < a.numberOfChannels; n++) {
      const o = a.getChannelData(n);
      s.copyToChannel(o, n, r);
    }
    r += a.length;
  }
  yield {
    buffer: s,
    timestamp: t[0].timestamp,
    duration: t[0].timestamp + s.duration
  };
}
async function bl(t) {
  const e = t.cloneNode();
  return e.controls = !1, e.style.display = "hidden", e.crossOrigin = "anonymous", e.preload = "auto", e instanceof HTMLVideoElement && (e.playsInline = !0), await new Promise((i, s) => {
    e.readyState >= 3 && i(null), e?.addEventListener("canplay", () => {
      i(null);
    }, { once: !0 }), e?.addEventListener("error", () => s(
      new Bt({
        message: `Failed to decode file: ${e?.error}`,
        code: "seek_error"
      })
    ), { once: !0 }), e?.load();
  }), e;
}
function As(t, e) {
  return t.start.millis >= e[0].millis && t.start.millis <= e[1].millis || t.stop.millis <= e[1].millis && t.stop.millis >= e[0].millis;
}
var kl = Object.defineProperty, Sl = Object.getOwnPropertyDescriptor, Wt = (t, e, i, s) => {
  for (var r = s > 1 ? void 0 : s ? Sl(e, i) : e, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = (s ? n(e, i, r) : n(r)) || r);
  return s && r && kl(e, i, r), r;
};
class pe extends $ {
  _muted = !1;
  _volume = 1;
  _transcript;
  gainNode;
  type = "audio";
  playing = !1;
  range = [new E(), new E()];
  /**
   * Defines the playback element
   */
  element;
  constructor(e, i = {}) {
    super(), e instanceof Ze ? this.source = e : this.input = e, Object.assign(this, i);
  }
  async init(e) {
    if (this.input && !this.source && (this.source = await Oe.from(this.input)), !this.source?.duration)
      throw new ee({
        code: "missing_duration",
        message: "No duration found for source"
      });
    if ((this.range[1].millis > this.source.duration.millis || this.range[1].millis === 0) && (this.range[1] = this.source.duration), e.context instanceof AudioContext && this.source?.element) {
      this.element = await bl(this.source.element), this.element.muted = this._muted;
      const i = e.context.createMediaElementSource(this.element);
      this.gainNode = e.context.createGain(), i.connect(this.gainNode), this.gainNode.connect(e.context.destination), this.gainNode.gain.value = Math.max(0, Math.min(1, this._volume)), this.element.addEventListener("play", () => {
        this.playing = !0;
      }), this.element.addEventListener("pause", () => {
        this.playing = !1;
      });
    }
  }
  async play(e) {
    !this.element || e.playbackTime > this.stop.seconds || (this.element.currentTime = Math.max(this.range[0].seconds, e.playbackTime - this.delay.seconds), e.playbackTime > this.start.seconds && await new Promise((i, s) => {
      this.element.onseeked = () => i(null), this.element.onerror = () => s(
        new Bt({
          message: `Failed to decode file: ${this.element?.error}`,
          code: "seek_error"
        })
      );
    }));
  }
  async pause() {
    this.playing && (this.element?.pause(), this.playing = !1);
  }
  async enter(e) {
    if (await super.enter(e), e.context instanceof OfflineAudioContext && this.source && !this.muted) {
      const [i, s] = this.getBufferRange(e), r = new pl(this.source, {
        volume: this.volume,
        start: i,
        stop: s
      });
      await r.init(e.context), await r.render(this.delay.seconds);
    }
  }
  async update(e) {
    const i = e.playbackTimestamp.millis - this.start.millis + this.range[0].millis;
    e.playing && !this.playing && i >= 0 && (this.element && !this.element.ended && await this.element?.play(), this.playing = !0);
  }
  async exit() {
    await this.pause();
  }
  get transcript() {
    const e = this.source?.transcript ?? this._transcript;
    if (e)
      return this.duration.millis ? Object.assign(
        new se(
          e.groups.map((i) => {
            const s = new Le(
              i.words.filter((n) => n.stop.millis > this.range[0].millis && n.start.millis < this.range[1].millis).map((n) => new Tt(n.text, n.start.millis, n.stop.millis))
            ), r = s.words[0], a = s.words[s.words.length - 1];
            return r && r.start.millis < this.range[0].millis && (r.start.millis = this.range[0].millis), a && a.stop.millis > this.range[1].millis && (a.stop.millis = this.range[1].millis), s;
          }).filter((i) => i.words.length > 0)
        ),
        { id: e.id }
      ) : e;
  }
  set transcript(e) {
    this._transcript = e, this.source && (this.source.transcript = e);
  }
  cleanup() {
    super.cleanup(), this.playing && this.element?.pause();
  }
  get start() {
    return this.range[0].add(this._delay);
  }
  get stop() {
    return this.range[1].add(this._delay);
  }
  get duration() {
    return this.range[1].subtract(this.range[0]);
  }
  set duration(e) {
    this.range[1] = this.range[0].add(O(e)), this.range[1].millis <= this.range[0].millis && (this.range[1] = this.range[0].copy().addMillis(1)), this.source?.duration && this.range[1].millis > this.source.duration.millis && (this.range[1] = this.source.duration), this.emit("frame", this.stop.frames);
  }
  get volume() {
    return this._volume;
  }
  set volume(e) {
    this._volume = Math.max(0, Math.min(1, e)), this.gainNode && (this.gainNode.gain.value = this._volume);
  }
  get muted() {
    return this.element?.muted ?? this._muted;
  }
  set muted(e) {
    this._muted = e, this.element && (this.element.muted = e);
  }
  trim(e = this.start, i = this.stop) {
    return e = O(e), i = O(i), this.subclip(e.subtract(this.delay), i.subtract(this.delay));
  }
  /**
   * Returns a slice of a media clip with trimmed start and stop
   */
  subclip(e = this.range[0], i = this.range[1]) {
    if (e = O(e), i = O(i), e.millis >= i.millis)
      throw new ee({
        code: "invalidKeyframe",
        message: "Start can't lower than or equal the stop"
      });
    return e.millis < 0 && (this.range[0].millis = 0, e = this.range[0]), this.source?.duration && i.millis > this.source.duration.millis && this.source.duration.millis && (i = this.source.duration), this.range = [e, i], this.emit("frame", void 0), this;
  }
  async split(e) {
    if (e = O(e ?? this.layer?.composition?.audio.playbackFrame), !e || e.millis <= this.start.millis || e.millis >= this.stop.millis)
      throw new ee({
        code: "split_out_of_range",
        message: "Cannot split clip at the specified time"
      });
    if (!this.layer)
      throw new ee({
        code: "layer_not_attached",
        message: "Layer must be attached to a layer"
      });
    const i = e.subtract(this._delay), s = this.animate(e).copy();
    this.range[1] = i.copy(), s.range[0] = i.copy().addMillis(1), s.animations = [];
    const r = this.layer.clips.findIndex((a) => a.id == this.id);
    return await this.layer.add(s, r + 1), s;
  }
  /**
   * Remove silences from the clip will return an array of clips with the silences removed.
   * If the clip has been added to a layer it will remove the silences within the layer.
   * @param options - Options for silence detection
   */
  async removeSilences(e = {}) {
    if (!this.source) return [this];
    const i = (await this.source.silences(e)).filter((a) => As(a, this.range)).sort((a, n) => a.start.millis - n.start.millis);
    if (i.length == 0) return [this];
    const s = e.padding ?? 500, r = [this];
    for (const a of i) {
      const n = r.at(-1);
      if (!n) break;
      if (!As(a, n.range)) continue;
      const o = new E(
        Math.min(a.start.millis + s, a.stop.millis)
      );
      if (a.start.millis > n.range[0].millis && a.stop.millis < n.range[1].millis) {
        const c = n.copy();
        n.range[1] = o, c.range[0] = a.stop, r.push(c);
      } else a.start.millis <= n.range[0].millis ? n.range[0] = a.stop : a.stop.millis >= n.range[1].millis && (n.range[1] = o);
    }
    if (this.layer) {
      let a = !1;
      this.layer.strategy.mode == "SEQUENTIAL" && (a = !0, this.layer.sequential(!1)), this.detach(), await Promise.all(r.map((n) => this.layer?.add(n))), a && this.layer.sequential();
    }
    return r;
  }
  /**
   * Get the range of the clip in **seconds**
   */
  getBufferRange(e) {
    const i = this.layer?.composition?.markers.stop;
    let s;
    e.playbackTime > this.start.seconds ? s = e.playbackTime - this.delay.seconds : this.start.millis < 0 ? s = Math.abs(this.delay.seconds) : s = this.range[0].seconds;
    let r;
    return i && this.stop.millis > i.millis ? r = i.subtract(this.delay).seconds : r = this.range[1].seconds, [s, r];
  }
}
Wt([
  v(ml)
], pe.prototype, "range", 2);
Wt([
  v(se)
], pe.prototype, "transcript", 1);
Wt([
  v()
], pe.prototype, "volume", 1);
Wt([
  v()
], pe.prototype, "muted", 1);
let vl = class {
  current;
  rotation;
  next;
  source;
  options;
  generator;
  constructor(e, i) {
    this.source = e, this.options = i;
  }
  async init() {
    const i = await (await this.source.demuxer)?.getPrimaryVideoTrack();
    if (!i || !await i.canDecode())
      throw new Bt({
        message: "Video track cannot be decoded",
        code: "video_track_cannot_be_decoded"
      });
    const s = new Wr(i);
    return this.rotation = i?.rotation, this.generator = s.samples(this.options.start, this.options.stop), this.current = (await this.generator?.next())?.value, this.next = (await this.generator?.next())?.value, this;
  }
  async forward(e, i) {
    for (; (this.next?.timestamp ?? Number.POSITIVE_INFINITY) <= e - i; )
      this.current?.close(), this.current = this.next, this.next = (await this.generator?.next())?.value;
  }
  close() {
    this.current?.close(), this.next?.close(), this.current = void 0, this.next = void 0;
  }
};
class zs extends et(pe) {
  _keepAspectRatio = !0;
  type = "video";
  videoDecoder;
  animations = [];
  constructor(e, i = {}) {
    super(), e instanceof qi ? this.source = e : this.input = e, Object.assign(this, i);
  }
  async init(e) {
    await super.init(e), this.element && (this.element.currentTime = Math.max(1e-3, e.playbackTime - this.delay.seconds), await new Promise((i, s) => {
      this.element.onseeked = () => i(null), this.element.onerror = () => s(
        new Bt({
          message: `Failed to decode file: ${this.element?.error}`,
          code: "seek_error"
        })
      );
    }));
  }
  async seek(e) {
    !this.element || e.playbackTime < this.start.seconds || e.playbackTime > this.stop.seconds || (this.element.currentTime = Math.max(0, e.playbackTime - this.delay.seconds));
  }
  async update(e) {
    if (await super.update(e), !(!(e.context instanceof OfflineAudioContext) || !this.source)) {
      if (!this.videoDecoder) {
        const [i, s] = this.getBufferRange(e);
        this.videoDecoder = await new vl(this.source, {
          start: i,
          stop: s
        }).init();
      }
      await this.videoDecoder.forward(e.playbackTime, this.delay.seconds);
    }
  }
  render(e) {
    if (!(this.element instanceof HTMLVideoElement)) return;
    const { width: i, height: s } = this.size, r = this.videoDecoder?.current?.toVideoFrame() ?? this.element;
    e.save().blendMode(this.blendMode).clip(this.mask).filter(this.filter).opacity(this.opacity).transform({ translate: this }).transform({
      translate: this.translate,
      rotate: this.rotation,
      scale: this.scale
    }).transform({
      translate: {
        x: -this.anchor.x * i,
        y: -this.anchor.y * s
      }
    }).image(r, {
      width: i,
      height: s,
      rotation: this.videoDecoder?.rotation
    }).restore(), r instanceof VideoFrame && r.close();
  }
  async exit() {
    await super.exit(), this.videoDecoder?.close(), this.videoDecoder = void 0;
  }
}
class Us extends et($) {
  _keepAspectRatio = !0;
  type = "html";
  animations = [];
  /**
   * Access to the html document that
   * will be rendered to the canvas
   */
  element = new Image();
  constructor(e, i = {}) {
    super(), e instanceof Xi ? this.source = e : this.input = e, Object.assign(this, i);
  }
  async init() {
    this.input && !this.source && (this.source = await Oe.from(this.input)), this.element.setAttribute("src", this.source.imageUrl), await new Promise((e, i) => {
      this.element.onload = () => e(), this.element.onerror = () => {
        i(
          new Ee({
            code: "sourceNotProcessable",
            message: "An error occurred while processing the input medium."
          })
        );
      };
    });
  }
  render(e) {
    const { width: i, height: s } = this.size;
    e.save().blendMode(this.blendMode).clip(this.mask).filter(this.filter).opacity(this.opacity).transform({ translate: this }).transform({
      translate: this.translate,
      rotate: this.rotation,
      scale: this.scale
    }).transform({
      translate: {
        x: -this.anchor.x * i,
        y: -this.anchor.y * s
      }
    }).image(this.element, { width: i, height: s }).restore();
  }
  refresh() {
    return this.element.setAttribute("src", this.source.imageUrl), this;
  }
}
var Tl = Object.defineProperty, ea = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && Tl(e, i, r), r;
};
class Ht extends et($) {
  type = "shape";
  fill = "#FFFFFF";
  strokes = [];
  get stroke() {
    return this.strokes[0];
  }
  set stroke(e) {
    this.strokes = e ? [e] : [];
  }
  constructor(e = {}) {
    super(), Object.assign(this, e);
  }
}
ea([
  v()
], Ht.prototype, "fill");
ea([
  v()
], Ht.prototype, "strokes");
class xl extends Ht {
  _keepAspectRatio = !0;
  type = "circle";
  animations = [];
  constructor(e = {}) {
    super(), Object.assign(this, { radius: 300, ...e });
  }
  get radius() {
    const e = this.size;
    return Math.min(e.width, e.height) * 0.5;
  }
  set radius(e) {
    this.keepAspectRatio = !0, this.aspectRatio = 1, typeof e == "number" ? this.height = e * 2 : this.height = `${Number(e.replace("%", "")) * 2}%`;
  }
  get name() {
    return "Circle";
  }
  /**
   * Access to the html document that
   * will be rendered to the canvas
   */
  render(e) {
    const { width: i, height: s } = this.size;
    e.save().blendMode(this.blendMode).clip(this.mask).filter(this.filter).opacity(this.opacity).transform({ translate: this }).transform({
      translate: this.translate,
      rotate: this.rotation,
      scale: this.scale
    }).transform({
      translate: {
        x: -this.anchor.x * i,
        y: -this.anchor.y * s
      }
    }).circle({
      cx: i * 0.5,
      cy: s * 0.5,
      width: i * 0.5,
      height: s * 0.5
    }).fill(this.fill ? { color: this.fill } : void 0, !0).stroke(this.stroke, !0).restore();
  }
}
var Cl = Object.defineProperty, _l = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && Cl(e, i, r), r;
};
class ta extends Ht {
  type = "rectangle";
  animations = [];
  radius;
  constructor(e = {}) {
    super(), Object.assign(this, e);
  }
  get name() {
    return "Rectangle";
  }
  /**
   * Access to the html document that
   * will be rendered to the canvas
   */
  render(e) {
    const { width: i, height: s } = this.size;
    e.save().blendMode(this.blendMode).clip(this.mask).filter(this.filter).opacity(this.opacity).transform({ translate: this }).transform({
      translate: this.translate,
      rotate: this.rotation,
      scale: this.scale
    }).transform({
      translate: {
        x: -this.anchor.x * i,
        y: -this.anchor.y * s
      }
    }).rect({
      width: i,
      height: s,
      radius: this.radius
    }).fill(this.fill ? { color: this.fill } : void 0, !0).stroke(this.stroke, !0).restore();
  }
}
_l([
  v()
], ta.prototype, "radius");
class Il extends et(pe) {
  type = "waveform";
  animations = [];
  sampleRate = 8e3;
  fill = { color: "#FFFFFF" };
  bar = { width: 28, gap: 26, radius: 14 };
  smoothing = 0.4;
  magnitude = 1;
  barHeights = [];
  channelData;
  time = new E(0, 0);
  constructor(e, i = {}) {
    super(), e instanceof Ze ? this.source = e : this.input = e, this.height = 700, this.width = "90%", Object.assign(this, i);
  }
  async init() {
    if (this.input && !this.source && (this.source = await Oe.from(this.input)), !this.source?.duration)
      throw new ee({
        message: "Could not find audio source",
        code: "no_audio_source"
      });
    (this.range[1].millis > this.source.duration.millis || this.range[1].millis === 0) && (this.range[1] = this.source.duration);
    const e = await this.source.decode(1, this.sampleRate, !0);
    this.channelData = e.getChannelData(0);
  }
  async enter() {
  }
  async update(e) {
    this.time = new E(0, e.playbackTime).subtract(this.delay);
  }
  async render(e) {
    if (!this.channelData) return;
    this.updateBarHeights(e);
    const i = _(this.height, e.height), s = _(this.width, e.width), r = Math.floor(s / (this.bar.width + this.bar.gap));
    e.save().blendMode(this.blendMode).clip(this.mask).filter(this.filter).opacity(this.opacity).transform({ translate: this }).transform({
      translate: this.translate,
      rotate: this.rotation,
      scale: this.scale
    }).transform({
      translate: {
        x: -this.anchor.x * s,
        y: -this.anchor.y * i
      }
    });
    const a = [];
    for (let n = 0; n < r; n++) {
      const o = this.time.millis - n * (1e3 / this.sampleRate), c = Math.floor(o * this.sampleRate / 1e3), l = c >= 0 && c < this.channelData.length ? this.channelData[c] : 0, d = Math.abs(l) * i, h = (this.barHeights[n] * this.smoothing + d * (1 - this.smoothing)) * this.magnitude;
      a.push(h);
      const u = n * (this.bar.width + this.bar.gap);
      e.rect({
        x: u,
        y: (i - h) * this.anchor.y,
        width: this.bar.width,
        height: h,
        radius: this.bar.radius
      }).fill(this.fill, !0);
    }
    this.barHeights = a, e.restore();
  }
  updateBarHeights(e) {
    const i = _(this.width, e.width), s = Math.floor(i / (this.bar.width + this.bar.gap));
    this.barHeights.length != s && (this.barHeights = new Array(s).fill(0));
  }
}
const St = {
  "The Bold Font": {
    weights: ["500"],
    url: "https://diffusion-studio-public.s3.eu-central-1.amazonaws.com/fonts/the-bold-font.ttf"
  },
  "Komika Axis": {
    weights: ["400"],
    url: "https://diffusion-studio-public.s3.eu-central-1.amazonaws.com/fonts/komika-axis.ttf"
  },
  Geologica: {
    weights: ["100", "200", "300", "400", "500", "600", "700", "800", "900"],
    url: "https://fonts.gstatic.com/s/geologica/v1/oY1l8evIr7j9P3TN9YwNAdyjzUyDKkKdAGOJh1UlCDUIhAIdhCZOn1fLsig7jfvCCPHZckUWE1lELWNN-w.woff2"
  },
  Nunito: {
    weights: ["200", "300", "400", "500", "600", "700", "800", "900"],
    url: "https://fonts.gstatic.com/s/nunito/v26/XRXV3I6Li01BKofINeaBTMnFcQ.woff2"
  },
  Figtree: {
    weights: ["300", "400", "500", "600", "700", "800", "900"],
    url: "https://fonts.gstatic.com/s/figtree/v5/_Xms-HUzqDCFdgfMm4S9DaRvzig.woff2"
  },
  Urbanist: {
    weights: ["100", "200", "300", "400", "500", "600", "700", "800", "900"],
    url: "https://fonts.gstatic.com/s/urbanist/v15/L0x-DF02iFML4hGCyMqlbS1miXK2.woff2"
  },
  Montserrat: {
    weights: ["100", "200", "300", "400", "500", "600", "700", "800", "900"],
    url: "https://fonts.gstatic.com/s/montserrat/v26/JTUSjIg1_i6t8kCHKm459WlhyyTh89Y.woff2"
  },
  Bangers: {
    weights: ["400"],
    url: "https://fonts.gstatic.com/s/bangers/v20/FeVQS0BTqb0h60ACH55Q2J5hm24.woff2"
  },
  Chewy: {
    weights: ["400"],
    url: "https://fonts.gstatic.com/s/chewy/v18/uK_94ruUb-k-wn52KjI9OPec.woff2"
  },
  "Source Code Pro": {
    weights: ["200", "300", "400", "500", "600", "700", "800", "900"],
    url: "https://fonts.gstatic.com/s/sourcecodepro/v22/HI_SiYsKILxRpg3hIP6sJ7fM7PqlPevWnsUnxg.woff2"
  }
}, xd = [
  "Helvetica",
  "Arial",
  "Arial Black",
  "Verdana",
  "Tahoma",
  "Trebuchet MS",
  "Impact",
  "Gill Sans",
  "Times New Roman",
  "Georgia",
  "Palatino",
  "Baskerville",
  "Andal Mono",
  "Courier",
  "Lucida",
  "Monaco",
  "Bradley Hand",
  "Brush Script MT",
  "Luminari",
  "Comic Sans MS"
], Cd = {
  100: "Thin",
  200: "Extra Light",
  300: "Light",
  400: "Normal",
  500: "Medium",
  600: "Semi Bold",
  700: "Bold",
  800: "Extra Bold",
  900: "Black"
};
function El(t) {
  return `${t}`.match(/oblique/i) ? "oblique" : `${t}`.match(/italic/i) ? "italic" : "normal";
}
function Pl(t) {
  return `${t}`.match(/black|heavy/i) ? "900" : `${t}`.match(/extrabold|ultrabold/i) ? "800" : `${t}`.match(/bold|strong/i) ? "700" : `${t}`.match(/semibold|demibold/i) ? "600" : `${t}`.match(/medium/i) ? "500" : `${t}`.match(/normal|regular|book/i) ? "400" : `${t}`.match(/light/i) ? "300" : `${t}`.match(/extralight|ultralight/i) ? "200" : `${t}`.match(/thin|hairline/i) ? "100" : "400";
}
var Fl = Object.defineProperty, Rl = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && Fl(e, i, r), r;
};
const ia = class xi extends Te {
  loadedFonts = [];
  /**
   * Load the font that has been initiated via the constructor
   */
  async load(e) {
    let i = "", s = 16, r = "normal";
    const a = e.weight ?? "400", n = e.family;
    "source" in e ? e.source.startsWith("https://") ? i = `url(${e.source})` : i = e.source : i = `url(${St[e.family].url})`, "size" in e && e.size !== void 0 && (s = e.size), "style" in e && e.style !== void 0 && (r = e.style);
    const o = new FontFace(n, i, { weight: a, style: r });
    await new Promise((l, d) => {
      o.load().then((h) => {
        document.fonts.add(h), l(null);
      }).catch((h) => {
        d(h);
      });
    });
    const c = {
      source: i,
      family: n,
      weight: a,
      style: r,
      size: s
    };
    return this.loadedFonts.some((l) => l.source === c.source) || this.loadedFonts.push(c), c;
  }
  /**
   * Reload all fonts
   * @deprecated Use restoreCheckpoint instead
   */
  async reload() {
    for (const e of this.loadedFonts)
      await this.load(e);
  }
  /**
   * Get all available local fonts, requires the
   * **Local Font Access API**
   */
  static async localFonts() {
    const e = {};
    "queryLocalFonts" in window || Object.assign(window, { queryLocalFonts: () => [] });
    for (const i of await window.queryLocalFonts()) {
      if (i.family in e) {
        e[i.family].push(i);
        continue;
      }
      e[i.family] = [i];
    }
    return Object.keys(e).map((i) => ({
      family: i,
      variants: e[i].map((s) => ({
        family: i,
        style: El(s.style),
        weight: Pl(s.fullName),
        source: `local('${s.fullName}'), local('${s.postscriptName}')`
      }))
    }));
  }
  /**
   * Get common web fonts
   */
  static webFonts() {
    return Object.keys(St).map((e) => ({
      family: e,
      variants: St[e].weights.map((i) => ({
        family: e,
        source: `url(${St[e].url})`,
        weight: i
      }))
    }));
  }
  static async load(e) {
    return await new xi().load(e);
  }
  copy() {
    return xi.fromJSON(this.toJSON());
  }
  /**
   * Create a checkpoint of the current font manager state
   */
  async createCheckpoint() {
    return this.toJSON();
  }
  /**
   * Restore the font manager state from a checkpoint
   * @param checkpoint - The checkpoint to restore from
   */
  async restoreCheckpoint(e) {
    B(typeof e == "object"), B(e != null), B("loadedFonts" in e), B(Array.isArray(e.loadedFonts)), this.fromJSON(e);
    for (const i of this.loadedFonts)
      await this.load(i);
  }
};
Rl([
  v()
], ia.prototype, "loadedFonts");
let ge = ia;
class Ce {
  /**
   * This function returns the position of the captions
   */
  position;
  constructor(e = {}) {
    this.position = {
      x: e.position?.x ?? "50%",
      y: e.position?.y ?? "50%"
    };
  }
  /**
   * This function syncs the timestamp of the captions to the audio clip
   * @param layer - The layer to sync
   * @param clip - The audio clip to sync to
   */
  sync(e, i) {
    return i.on("offset", (s) => {
      e.offset(s.detail);
    }), this;
  }
  /**
   * This function creates the captions
   * @param layer - The layer to apply the settings to
   * @param transcript - The transcript to apply the settings to
   * @param offset - The offset of the captions
   */
  async apply(e, i, s) {
  }
}
class Bl extends Ce {
  generatorOptions;
  constructor(e = {}) {
    super(e), this.generatorOptions = e.generatorOptions ?? { duration: [0.2] };
  }
  async apply(e, i, s) {
    const r = await ge.load({
      family: "Figtree",
      weight: "700",
      size: 27
    });
    for (const a of i.iter(this.generatorOptions))
      await e.add(
        new L({
          text: a.words.map((n) => n.text).join(" "),
          align: "center",
          baseline: "middle",
          color: "#FFFFFF",
          font: r,
          stroke: {
            color: "#000000",
            width: 6,
            lineJoin: "round",
            lineCap: "round"
          },
          shadow: {
            color: "#000000",
            blur: 0,
            offsetX: 2.4,
            offsetY: 3,
            opacity: 100
          },
          position: this.position,
          delay: a.start.add(s),
          duration: a.duration,
          animations: [{
            key: "scale",
            easing: "ease-out",
            frames: [{
              value: 0.94,
              time: 0
            }, {
              value: 1,
              time: 8
            }]
          }, {
            key: "opacity",
            easing: "ease-out",
            frames: [{
              value: 0,
              time: 0
            }, {
              value: 100,
              time: 4
            }]
          }]
        })
      );
  }
}
class _d extends Ce {
  generatorOptions;
  color;
  constructor(e = {}) {
    super(e), this.generatorOptions = e.generatorOptions ?? { duration: [0.2] }, this.color = e.color ?? "#00FF4C";
  }
  async apply(e, i, s) {
    const r = await ge.load({
      family: "The Bold Font",
      weight: "500",
      size: 28
    });
    for (const a of i.iter(this.generatorOptions))
      for (let n = 0; n < a.words.length; n++) {
        const o = a.words.map((c) => c.text);
        await e.add(
          new L({
            text: o.join(" "),
            align: "center",
            baseline: "middle",
            color: "#FFFFFF",
            maxWidth: "80%",
            font: r,
            stroke: {
              width: 4,
              color: "#000000"
            },
            shadow: [
              {
                color: "#000000",
                blur: 9,
                opacity: 100,
                offsetX: 0,
                offsetY: 0
              },
              {
                color: "#000000",
                blur: 13,
                opacity: 100,
                offsetX: 3,
                offsetY: 3
              },
              {
                color: "#000000",
                blur: 18,
                opacity: 100,
                offsetX: 3,
                offsetY: 3
              },
              {
                color: "#000000",
                blur: 25,
                opacity: 100,
                offsetX: 3,
                offsetY: 3
              }
            ],
            glow: {
              radius: 20,
              intensity: 2,
              opacity: 50
            },
            position: this.position,
            styles: a.words.length > 1 ? [{
              style: { color: this.color },
              start: o.slice(0, n).join(" ").length,
              stop: o.slice(0, n + 1).join(" ").length
            }] : void 0,
            duration: a.words[n].duration,
            delay: a.words[n].start.add(s)
          })
        );
      }
  }
}
class Id extends Ce {
  colors;
  constructor(e = {}) {
    super(e), this.colors = e.colors ?? ["#1BD724", "#FFEE0C", "#FF2E17"];
  }
  async apply(e, i, s) {
    const r = await ge.load({
      family: "The Bold Font",
      weight: "500",
      size: 26
    });
    for (const a of i.iter({ length: [18] })) {
      const { segments: n, words: o } = this.splitSequence(a);
      for (let c = 0; c < a.words.length; c++) {
        const l = o[c]?.at(0)?.start, d = o[c]?.at(-1)?.stop;
        !l || !d || await e.add(
          new L({
            text: n.join(`
`),
            align: "center",
            baseline: "middle",
            color: "#FFFFFF",
            shadow: Array.from({ length: 4 }).map(() => ({
              color: "#000000",
              blur: 40,
              opacity: 80
            })),
            stroke: {
              width: 6,
              color: "#000000"
            },
            glow: {
              radius: 30,
              opacity: 60
            },
            maxWidth: "90%",
            font: r,
            casing: "upper",
            position: this.position,
            duration: d.subtract(l),
            delay: l.add(s),
            styles: [{
              start: n.slice(0, c).join(" ").length,
              stop: n.slice(0, c + 1).join(" ").length,
              style: {
                fontSize: 30,
                color: this.colors[vt(0, 2)]
              }
            }]
          })
        );
      }
    }
  }
  splitSequence(e) {
    const i = e.text, s = Math.ceil(i.length / 2);
    let r = i.length;
    for (let o = s, c = s; o > 0 && c < e.text.length - 1; o--, c++) {
      if (i[o].match(/ /)) {
        r = o;
        break;
      }
      if (i[c].match(/ /)) {
        r = c;
        break;
      }
    }
    const a = [...Ct(i, r).map((o) => o.trim())], n = Ct(e.words, a[0].split(/ /).length);
    return { segments: a, words: n };
  }
}
class Ed extends Ce {
  constructor(e = {}) {
    super(e);
  }
  async apply(e, i, s) {
    const r = await ge.load({
      family: "Montserrat",
      weight: "300",
      size: 18
    }), a = await ge.load({
      family: "Montserrat",
      weight: "600",
      size: 18
    });
    for (const n of i.iter({ length: [18] })) {
      const { segments: o, words: c } = this.splitSequence(n);
      for (let l = 0; l < n.words.length; l++) {
        const d = c[l]?.at(0)?.start, h = c[l]?.at(-1)?.stop;
        !d || !h || await e.add(
          new L({
            text: o.join(`
`),
            align: "center",
            baseline: "middle",
            color: "#FFFFFF",
            shadow: {
              color: "#000000",
              blur: 7,
              opacity: 90,
              offsetX: 2,
              offsetY: 2
            },
            maxWidth: "90%",
            font: r,
            leading: 0.9,
            position: this.position,
            duration: h.subtract(d),
            delay: d.add(s),
            styles: [{
              start: o.slice(0, l).join(" ").length,
              stop: o.slice(0, l + 1).join(" ").length,
              style: {
                font: a
              }
            }]
          })
        );
      }
    }
  }
  splitSequence(e) {
    const i = e.text, s = Math.ceil(i.length / 2);
    let r = i.length;
    for (let o = s, c = s; o > 0 && c < e.text.length - 1; o--, c++) {
      if (i[o].match(/ /)) {
        r = o;
        break;
      }
      if (i[c].match(/ /)) {
        r = c;
        break;
      }
    }
    const a = [...Ct(i, r).map((o) => o.trim())], n = Ct(e.words, a[0].split(/ /).length);
    return { segments: a, words: n };
  }
}
class Pd extends Ce {
  generatorOptions;
  constructor(e = {}) {
    super(e), this.generatorOptions = e.generatorOptions ?? { duration: [1.4] }, this.position = e.position ?? { x: "12%", y: "44%" };
  }
  async apply(e, i, s) {
    const r = await ge.load({
      family: "Geologica",
      weight: "400",
      size: 20
    });
    for (const a of i.iter(this.generatorOptions))
      for (let n = 0; n < a.words.length; n++) {
        const o = () => a.words.length == 1 ? a.text : a.words.map((l) => l.text).slice(0, n + 1).join(" ");
        await e.add(
          new L({
            text: o(),
            position: this.position,
            align: "left",
            baseline: "top",
            color: "#FFFFFF",
            font: r,
            maxWidth: "70%",
            leading: 1.4,
            stroke: {
              color: "#000000",
              width: 4,
              lineJoin: "round"
            },
            shadow: {
              color: "#000000",
              blur: 3,
              opacity: 80,
              offsetX: 1.25,
              offsetY: 1.25
            },
            delay: a.words[n].start.add(s),
            duration: a.words[n].duration
          })
        );
      }
  }
}
class Fd extends Ce {
  generatorOptions;
  constructor(e = {}) {
    super(e), this.generatorOptions = e.generatorOptions ?? { duration: [0.2] };
  }
  async apply(e, i, s) {
    const r = await ge.load({
      family: "Urbanist",
      weight: "800",
      size: 24
    });
    for (const a of i.iter(this.generatorOptions))
      await e.add(
        new L({
          text: a.words.map((n) => n.text).join(" "),
          align: "center",
          baseline: "middle",
          color: "#fffe41",
          font: r,
          casing: "upper",
          shadow: [
            {
              color: "#ab7a00",
              blur: 0,
              offsetX: 6,
              offsetY: 3,
              opacity: 100
            },
            {
              color: "#ab7a00",
              blur: 0,
              offsetX: 4,
              offsetY: 2,
              opacity: 100
            },
            {
              color: "#ab7a00",
              blur: 0,
              offsetX: 2,
              offsetY: 1,
              opacity: 100
            }
          ],
          stroke: {
            color: "#ab7a00",
            width: 1
          },
          glow: {
            radius: 40,
            intensity: 2,
            opacity: 80
          },
          position: this.position,
          duration: a.duration,
          delay: a.start.add(s),
          animations: [{
            key: "scale",
            easing: "ease-out",
            frames: [{
              value: 0.94,
              time: 0
            }, {
              value: 1,
              time: 8
            }]
          }, {
            key: "opacity",
            easing: "ease-out",
            frames: [{
              value: 0,
              time: 0
            }, {
              value: 100,
              time: 4
            }]
          }]
        })
      );
  }
}
class Rd extends Ce {
  generatorOptions;
  color;
  constructor(e = {}) {
    super(e), this.generatorOptions = e.generatorOptions ?? { length: [20] }, this.color = e.color ?? "#c4c4c4";
  }
  async apply(e, i, s) {
    const r = await ge.load({
      family: "Montserrat",
      weight: "400",
      size: 16
    });
    for (const a of i.iter(this.generatorOptions))
      for (let n = 0; n < a.words.length; n++) {
        const o = a.words.map((c) => c.text);
        await e.add(
          new L({
            text: o.join(" "),
            align: "center",
            baseline: "middle",
            color: "#FFFFFF",
            background: {
              opacity: 50,
              padding: {
                x: 30,
                y: 20
              }
            },
            maxWidth: "80%",
            leading: 1.4,
            font: r,
            position: this.position,
            styles: o.length > 1 ? [{
              style: { color: this.color },
              start: o.slice(0, n + 1).join(" ").length
            }] : void 0,
            duration: a.words[n].duration,
            delay: a.words[n].start.add(s),
            animations: n == 0 ? [{
              key: "opacity",
              easing: "ease-in-out",
              frames: [
                {
                  time: 0,
                  value: 0
                },
                {
                  time: "0.05s",
                  value: 100
                }
              ]
            }, {
              key: "translateY",
              easing: "ease-in-out",
              frames: [
                {
                  time: 0,
                  value: 30
                },
                {
                  time: "0.05s",
                  value: 0
                }
              ]
            }] : []
          })
        );
      }
  }
}
class Bd extends Ce {
  generatorOptions;
  color;
  constructor(e = {}) {
    super(e), this.generatorOptions = e.generatorOptions ?? { count: [5] }, this.color = e.color ?? "#42f5ad";
  }
  async apply(e, i, s) {
    const r = await ge.load({
      family: "Montserrat",
      weight: "700",
      size: 19
    });
    for (const a of i.iter(this.generatorOptions))
      for (let n = 0; n < a.words.length; n++) {
        const o = a.words.map((c) => c.text);
        await e.add(
          new L({
            text: o.join(" "),
            align: "center",
            baseline: "alphabetic",
            color: "#FFFFFF",
            casing: "upper",
            shadow: {
              color: "#000000",
              blur: 8,
              opacity: 90,
              offsetX: 4,
              offsetY: 4
            },
            stroke: {
              width: 5,
              color: "#000000"
            },
            maxWidth: "90%",
            glow: {
              radius: 10,
              intensity: 1,
              opacity: 60
            },
            font: r,
            styles: [{
              style: {
                color: this.color,
                fontSize: 24
              },
              start: o.slice(0, n).join(" ").length,
              stop: o.slice(0, n + 1).join(" ").length
            }],
            position: this.position,
            duration: a.words[n].duration,
            delay: a.words[n].start.add(s)
          })
        );
      }
  }
}
const sa = ["DEFAULT", "SEQUENTIAL"];
class Ci {
  mode = sa[0];
  pauseEvents = !1;
  add(e, i) {
    if (!this.pauseEvents)
      try {
        this.pauseEvents = !0;
        let s = !0;
        for (let r = 0; r < i.clips.length && (s = Vs(e, i.clips[r]), !!s); r++)
          Ns(e, i.clips[r]);
        s && (i.clips.push(e), i.clips.sort(Ds));
      } finally {
        this.pauseEvents = !1;
      }
  }
  update(e, i) {
    if (!this.pauseEvents)
      try {
        this.pauseEvents = !0, i.clips.sort(Ds);
        for (let s = 0; s < i.clips.length; s++)
          if (e.id != i.clips[s].id) {
            if (!Vs(e, i.clips[s]))
              break;
            Ns(e, i.clips[s]);
          }
      } finally {
        this.pauseEvents = !1;
      }
  }
  offset(e, i) {
    if (!this.pauseEvents)
      try {
        this.pauseEvents = !0;
        for (const s of i.clips)
          s.offset(e);
      } finally {
        this.pauseEvents = !1;
      }
  }
  toJSON() {
    return this.mode;
  }
}
class ra {
  mode = sa[1];
  add(e, i, s = void 0) {
    let r = -1;
    (s != null && s > 0 || s == null) && (r = i.clips.at((s ?? 0) - 1)?.stop.millis ?? -1), e.offset(new E(r - e.start.millis + 1)), s == null ? i.clips.push(e) : (i.clips.splice(s, 0, e), i.clips.slice(s + 1).forEach((a) => {
      a.offset(e.stop.subtract(e.start));
    }));
  }
  update(e, i) {
    let s = 0;
    for (const r of i.clips) {
      if (r.start.millis != s) {
        const a = s - r.start.millis;
        r.offset(new E(a));
      }
      s = r.stop.millis + 1;
    }
  }
  offset() {
  }
  toJSON() {
    return this.mode;
  }
}
function Ds(t, e) {
  return t.start.millis - e.start.millis;
}
function Ns(t, e) {
  t.start.millis >= e.start.millis && t.start.millis <= e.stop.millis && t.trim(e.stop.copy().addMillis(1), t.stop.copy()), t.stop.millis >= e.start.millis && t.stop.millis <= e.stop.millis && t.trim(t.start.copy(), e.start.copy().addMillis(-1));
}
function Vs(t, e) {
  if (t.start.millis >= e.start.millis && t.stop.millis <= e.stop.millis) {
    const i = e.layer?.composition?.layers.find((s) => s.type == t.type && !s.clips.some((r) => t.id != r.id && t.start.millis >= r.start.millis && t.stop.millis <= r.stop.millis));
    return i ? i.add(t.detach()) : e.layer?.composition?.add(t.detach()), !1;
  }
  return !0;
}
function aa(t, e) {
  B(t.transition);
  const i = Math.max(
    Math.floor((t.stop.millis + e.start.millis) / 2),
    t.stop.millis
    // The midpoint cannot go before the end of the first clip
  );
  return new E(i);
}
function Ls(t, e) {
  const i = aa(t, e), s = O(t.transition?.duration ?? new E(1e3));
  return i.millis -= s.millis / 2, i;
}
function Ol(t, e) {
  const i = aa(t, e), s = O(t.transition?.duration ?? new E(1e3));
  return i.millis += s.millis / 2, i;
}
function Ml(t, e, i, s) {
  switch (B(t.transition), t.transition.type) {
    case "slide-from-right":
      t.render(s), s.save(), s.transform({
        translate: {
          x: (1 - i) ** 2 * s.width,
          y: 0
        }
      }), e.render(s), s.restore();
      break;
    case "slide-from-left":
      t.render(s), s.save(), s.transform({
        translate: {
          x: (1 - i) ** 2 * s.width * -1,
          y: 0
        }
      }), e.render(s), s.restore();
      break;
    case "fade-to-black":
      i < 0.5 ? t.render(s) : e.render(s), s.save(), s.rect({
        x: 0,
        y: 0,
        width: s.width,
        height: s.height
      }), s.opacity(100 * (i < 0.5 ? 2 * i : 2 * (1 - i))), s.fill({
        color: "#000000"
      }, !0), s.restore();
      break;
    case "fade-to-white":
      i < 0.5 ? t.render(s) : e.render(s), s.save(), s.rect({
        x: 0,
        y: 0,
        width: s.width,
        height: s.height
      }), s.opacity(100 * (i < 0.5 ? 2 * i : 2 * (1 - i))), s.fill({
        color: "#FFFFFF"
      }, !0), s.restore();
      break;
    default:
      t.render(s), s.save(), s.opacity(100 * i), e.render(s), s.restore();
      break;
  }
}
const Al = {
  fromJSON: (t) => (B(typeof t == "string"), t == "SEQUENTIAL" ? new ra() : new Ci())
};
var zl = Object.defineProperty, Me = (t, e, i, s) => {
  for (var r = void 0, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = n(e, i, r) || r);
  return r && zl(e, i, r), r;
};
class _e extends dt(Te) {
  id = crypto.randomUUID();
  data = {};
  primary = !1;
  disabled = !1;
  clips = [];
  /**
   * Reference to the composition
   */
  composition;
  strategy = new Ci();
  createdAt = /* @__PURE__ */ new Date();
  /**
   * Id of the clips in the layer
   */
  get type() {
    return this.clips[0]?.type ?? "base";
  }
  /**
   * The index of the clip that should be rendered
   */
  currentMainClipIndex = -1;
  /**
   * Tracks the clips that are currently visible. In the case of transitions, this can be more than one clip at a time.
   */
  visibleClips = /* @__PURE__ */ new Set();
  /**
   * Connect the layer with the composition
   */
  connect(e) {
    this.composition = e, this.bubble(e);
    for (const i of this.clips)
      if (!i.initialized)
        return i.init(e.audio).then(() => (i.initialized = !0, this));
    return this;
  }
  /**
   * Applies the stack property
   */
  sequential(e = !0) {
    return e ? (this.strategy = new ra(), this.strategy.update(new $(), this)) : this.strategy = new Ci(), this.emit("update", void 0), this;
  }
  /**
   * Change the index of the layer
   */
  index(e) {
    const i = this.composition?.layers ?? [], s = i.findIndex((n) => n.id == this.id), r = i.length - 1;
    if (s == -1) return this;
    let a = 0;
    return e == "bottom" ? a = r : e == "top" || e < 0 ? a = 0 : e > r ? a = r : a = e, la(i, s, a), this.emit("update", void 0), this;
  }
  /**
   * Seek the provided time if the layer contains
   * audio or video clips. Will await the promise 
   * otherwise will resolve immediately
   */
  async seek(e) {
    for (const i of this.clips)
      await i.seek(e);
  }
  /**
   * Seek the provided time if the layer contains
   * audio or video clips. Will await the promise 
   * otherwise will resolve immediately
   */
  async play(e) {
    for (const i of this.clips)
      await i.play(e);
  }
  /**
   * Seek the provided time if the layer contains
   * audio or video clips. Will await the promise 
   * otherwise will resolve immediately
   */
  async pause(e) {
    for (const i of this.clips)
      await i.pause(e);
  }
  /**
   * Move all clips of the layer at once along the timeline
   */
  offset(e) {
    return this.strategy.offset(O(e), this), this;
  }
  /**
   * Triggered when the layer is redrawn, manages
   * the clip's lifecycle
   */
  async update(e) {
    const i = e.playbackTimestamp;
    if (this.disabled) {
      for (const o of this.visibleClips)
        await o.exit(e);
      this.visibleClips.clear();
      return;
    }
    let s, r, a = -1;
    for (let o = 0; o < this.clips.length; o++) {
      const c = this.clips[o], l = this.clips[o + 1];
      if (c.disabled)
        continue;
      const d = c.start;
      let h = c.transition && l ? Ol(c, l) : c.stop;
      if (d.millis <= i.millis && i.millis <= h.millis) {
        s = c, a = o, r = l;
        break;
      }
    }
    const n = /* @__PURE__ */ new Set();
    s && (n.add(s), s.transition && r && i.millis >= Ls(s, r).millis && n.add(r));
    for (const o of this.visibleClips)
      n.has(o) || await o.exit(e);
    for (const o of n)
      this.visibleClips.has(o) || await o.enter(e);
    this.currentMainClipIndex = a, this.visibleClips = n;
    for (const o of n)
      o.animate(i), await o.update(e);
  }
  render(e, i) {
    if (this.currentMainClipIndex === -1 || this.disabled)
      return;
    const s = e.playbackTimestamp, r = this.clips[this.currentMainClipIndex], a = this.clips[this.currentMainClipIndex + 1];
    if (r.transition && a) {
      const n = Ls(r, a).millis, o = s.millis >= n, c = O(r.transition?.duration ?? new E(1e3)), l = da((s.millis - n) / c.millis, 0, 1);
      if (o) {
        Ml(r, a, l, i);
        return;
      }
    }
    r.render(i);
  }
  /**
   * Adds a new clip to the layer. Calls update after adding the clip.
   * @param clip The clip to add
   * @param index The index to insert the clip at, will be ignored if layer is not stacked
   * @throws Error if the clip can't be added
   */
  async add(e, i) {
    if (this.clips[0] && this.clips[0].type != e.type)
      throw new ee({
        code: "layer_type_mismatch",
        message: "Layer can only contain clips of the same type"
      });
    return !e.initialized && this.composition?.audio && (await e.init(this.composition.audio), e.initialized = !0), await e.connect(this), this.strategy.add(e, this, i), e.on("frame", () => {
      this.strategy.update(e, this);
    }), e.bubble(this), this.emit("attach", void 0), this.composition?.update(), e;
  }
  /**
   * Remove a given clip from the layer. Calls update after removing the clip.
   * @returns `Layer` when it has been successfully removed `undefined` otherwise
   */
  remove(e) {
    const i = this.clips.findIndex((s) => s.id == e.id);
    if (i != null && i >= 0)
      return this.clips.splice(i, 1), this.strategy.update(e, this), this.emit("detach", void 0), e.cleanup(), this.composition?.update(), e;
  }
  /**
   * Get the first visible frame of the clip
   */
  get stop() {
    return this.clips.at(-1)?.stop ?? new E();
  }
  /**
   * Get the last visible frame of the clip
   */
  get start() {
    return this.clips.at(0)?.start ?? new E();
  }
  /**
   * Remove the layer from the composition
   */
  detach() {
    return this.composition?.removeLayer(this), this;
  }
  /**
   * Remove all clips from the layer
   */
  clear() {
    for (const e of this.clips)
      e.off("*");
    this.clips = [];
  }
  _resetInternalState() {
    this.visibleClips.clear(), this.currentMainClipIndex = -1;
  }
  /**
   * Create a checkpoint of the layer. May include Blobs.
   * @returns A serialized representation of the layer
   */
  async createCheckpoint() {
    return this.toJSON();
  }
  /**
   * Restore a checkpoint of the layer.
   * @param checkpoint The checkpoint to restore
   * @param sources The sources to use for the restoration
   * @returns The restored layer
   */
  async restoreCheckpoint(e, i) {
    B(typeof e == "object"), B(e != null), B("clips" in e), B(Array.isArray(e.clips));
    const { clips: s, ...r } = e;
    this.fromJSON(r);
    for (const [a, n] of s.entries()) {
      if (B(typeof n == "object"), B(n != null), B("id" in n), n.id == this.clips.at(a)?.id) {
        await this.clips[a].restoreCheckpoint(n, i);
        continue;
      }
      this.clips.at(a) && this.remove(this.clips[a]);
      try {
        const o = Jc.fromType(n);
        await o.restoreCheckpoint(n, i), await this.add(o, a);
      } catch (o) {
        this.emit("error", new Error(`Failed to restore clip ${n.id}: ${o}`));
      }
    }
    if (this.clips.length > s.length)
      for (const a of this.clips.slice(s.length))
        this.remove(a);
    return this;
  }
}
Me([
  v()
], _e.prototype, "id");
Me([
  v()
], _e.prototype, "data");
Me([
  v()
], _e.prototype, "primary");
Me([
  v()
], _e.prototype, "disabled");
Me([
  v()
], _e.prototype, "clips");
Me([
  v(Al)
], _e.prototype, "strategy");
Me([
  v(Ot)
], _e.prototype, "createdAt");
var Ul = Object.defineProperty, Dl = Object.getOwnPropertyDescriptor, Ae = (t, e, i, s) => {
  for (var r = s > 1 ? void 0 : s ? Dl(e, i) : e, a = t.length - 1, n; a >= 0; a--)
    (n = t[a]) && (r = (s ? n(e, i, r) : n(r)) || r);
  return s && r && Ul(e, i, r), r;
};
class ze extends dt(Te) {
  id = crypto.randomUUID();
  /**
   * Access the video renderer
   */
  video = new Xe();
  /**
   * Access the audio renderer. Will drive the playback of the composition
   */
  audio = new Qs({ callback: this.update.bind(this) });
  layers = [];
  playbackEndBehavior = "stop";
  markers = {
    start: new E()
  };
  createdAt = /* @__PURE__ */ new Date();
  data = {};
  constructor({
    height: e = 1080,
    width: i = 1920,
    background: s = "#000000",
    playbackEndBehavior: r = "stop"
  } = {}) {
    super(), this.video.resize(i, e), this.video.background = s, this.playbackEndBehavior = r;
  }
  get settings() {
    return {
      height: this.video.height,
      width: this.video.width,
      background: this.video.background,
      playbackEndBehavior: this.playbackEndBehavior
    };
  }
  set settings(e) {
    this.video.background = e.background, this.video.resize(e.width, e.height), this.playbackEndBehavior = e.playbackEndBehavior, this.emit("resize", void 0);
  }
  /**
   * Get the current playback state of the composition
   */
  get playing() {
    return this.audio.playing;
  }
  /**
   * Get the current width of the canvas
   */
  get width() {
    return this.video.width;
  }
  /**
   * Get the current height of the canvas
   */
  get height() {
    return this.video.height;
  }
  /**
   * This is where the playback stops playing
   */
  get duration() {
    return this.markers.stop ? this.markers.stop : Ea(this.layers);
  }
  /**
   * Limit the total duration of the composition
   */
  set duration(e) {
    e ? this.markers.stop = O(e) : delete this.markers.stop, this.emit("frame", this.markers.stop?.frames ?? 0);
  }
  /**
   * Get the currently rendered time of the playback 
   */
  get playhead() {
    return this.audio.playbackTimestamp;
  }
  /**
   * Set the currently rendered time of the playback 
   */
  set playhead(e) {
    this.audio.playbackOffset = O(e).seconds;
  }
  /**
   * Get all clips in the composition
   */
  get clips() {
    return this.layers.flatMap((e) => e.clips);
  }
  /**
   * Resize the renderer
   */
  resize(e, i) {
    this.video.resize(e, i), this.update(), this.emit("resize", void 0);
  }
  /**
   * Add the renderer to the dom.
   * This will start the ticker
   */
  mount(e) {
    this.video.mount(e), this.audio.start(), this.emit("mount", void 0);
  }
  /**
   * Remove the renderer from the dom.
   * This will stop the ticker
   */
  unmount() {
    this.video.unmount(), this.audio.stop(), this.emit("unmount", void 0);
  }
  /**
   * Insert a new layer at the specified index (defaults to 0)
   * @param Layer The layer to insert
   * @param index The index to insert at (0 = top layer, default: 0)
   */
  async insertLayer(e, i = 0) {
    return this.layers.splice(
      Math.max(0, Math.min(i, this.layers.length)),
      0,
      await e.connect(this)
    ), this.emit("attach", void 0), this.update(), e;
  }
  /**
   * Create a layer with the given type
   * @param type the desired type of the layer
   * @returns A new layer
   */
  createLayer(e = 0) {
    const i = new _e();
    return i.connect(this), this.layers.splice(
      Math.max(0, Math.min(e, this.layers.length)),
      0,
      i
    ), this.emit("attach", void 0), i;
  }
  /**
   * Create captions for the composition
   * @param source The source to create captions for
   * @param strategy The strategy to use for creating captions
   * @returns A new layer with captions
   */
  async createCaptions(e, i) {
    let s = new Bl();
    typeof i == "object" ? s = i : i && (s = new i());
    const r = this.createLayer();
    if (e instanceof pe) {
      if (!e.transcript) throw new ee({
        code: "missing_transcript",
        message: "The audio clip does not contain a transcript"
      });
      await s.sync(r, e).apply(r, e.transcript, e.delay);
    } else
      await s.apply(r, e, new E());
    return r;
  }
  /**
   * Convenience function for appending a layer
   * aswell as the clip to the composition
   */
  async add(e) {
    const i = this.createLayer();
    if (!Array.isArray(e))
      return await i.add(e), e;
    for (const s of e)
      await i.add(s);
    return e;
  }
  /**
   * Remove a given clip from the composition
   * @returns `Clip` when it has been successfully removed `undefined` otherwise
   */
  remove(e) {
    for (const i of this.layers)
      if (i.clips.find((s) => s.id == e.id))
        return i.remove(e);
    this.update();
  }
  /**
   * Compute the currently active frame
   */
  async update() {
    for (let e = this.layers.length - 1; e >= 0; e--)
      await this.layers[e].update(this.audio);
    this.video.clear();
    for (let e = this.layers.length - 1; e >= 0; e--)
      this.layers[e].render(this.audio, this.video);
    this.emit("currentframe", this.audio.playbackFrame), this.audio.playbackFrame >= this.duration.frames && this.audio.playing && this.handlePlaybackEnd();
  }
  /**
   * Convenience function to take a screenshot of the current frame
   */
  screenshot(e = "png", i = 1) {
    return this.video.canvas.toDataURL(`image/${e}`, i);
  }
  /**
   * Set the playback position to a specific time
   * @param time new playback time (defaults to 0)
   */
  async seek(e = 0) {
    if (!this.audio.stopped) {
      this.audio.playing && await this.pause(), this.audio.playbackOffset = Math.max(0, O(e).seconds);
      for (const i of this.layers)
        await i.seek(this.audio);
      await this.update();
    }
  }
  /**
   * Play the composition
   * @param time The time to start playing from
   */
  async play(e) {
    (e || this.audio.playbackFrame >= this.duration.frames) && await this.seek(e), await this.audio.play();
    for (const i of this.layers)
      await i.play(this.audio);
    this.emit("play", this.audio.playbackFrame);
  }
  /**
   * Pause the composition
   */
  async pause() {
    await this.audio.pause();
    for (const e of this.layers)
      await e.pause(this.audio);
    this.emit("pause", this.audio.playbackFrame);
  }
  /**
   * Remove all layers and clips from the composition
   */
  clear() {
    for (const e of this.layers)
      e.off("*");
    this.layers = [];
  }
  /**
   * Get the current playback time and composition
   * duration formatted as `00:00 / 00:00` by default.
   * if **hours** is set the format is `HH:mm:ss` whereas
   * **milliseconds** will return `mm:ss.SSS`
   */
  time(e) {
    const i = e?.hours ? 11 : 14, s = e?.milliseconds ? 23 : 19;
    return new Date(this.audio.playbackTime * 1e3).toISOString().slice(i, s) + " / " + new Date(this.duration.millis).toISOString().slice(i, s);
  }
  /**
   * Remove a given layer from the composition
   * @returns `Layer` when it has been successfully removed `undefined` otherwise
   */
  removeLayer(e) {
    const i = this.layers.findIndex((s) => s.id == e.id);
    if (i != null && i >= 0)
      return this.layers.splice(i, 1), this.emit("detach", void 0), e.off("*"), e;
  }
  /**
   * Remove multiple layers from the composition
   * @returns `Layer[]` all removed layers
   */
  removeLayers(...e) {
    return e.map((i) => this.removeLayer(i)).filter(Boolean);
  }
  /**
   * Handle the playback end behavior
   */
  async handlePlaybackEnd() {
    this.playbackEndBehavior == "loop" ? (await this.seek(0), await this.play()) : this.playbackEndBehavior == "reset" ? await this.seek(0) : await this.pause();
  }
  /**
   * Create a checkpoint of the composition. May include Blobs.
   * @returns A serialized representation of the composition
   */
  async createCheckpoint() {
    return this.toJSON();
  }
  /**
   * Restore a checkpoint of the composition.
   * @param checkpoint The checkpoint to restore
   * @param sources The sources to use for the restoration
   * @returns The restored composition
   */
  async restoreCheckpoint(e, i = []) {
    B(typeof e == "object"), B(e != null), B("layers" in e), B(Array.isArray(e.layers));
    const { layers: s, ...r } = e;
    this.fromJSON(r);
    for (const [a, n] of s.entries()) {
      if (B(typeof n == "object"), B(n != null), B("id" in n), n.id == this.layers.at(a)?.id) {
        await this.layers[a].restoreCheckpoint(n, i);
        continue;
      }
      this.layers.at(a) && this.removeLayer(this.layers[a]), await this.createLayer(a).restoreCheckpoint(n, i);
    }
    return this.layers.length > s.length && this.removeLayers(...this.layers.slice(s.length)), this;
  }
}
Ae([
  v()
], ze.prototype, "id", 2);
Ae([
  v()
], ze.prototype, "layers", 2);
Ae([
  v()
], ze.prototype, "playbackEndBehavior", 2);
Ae([
  v(Ia)
], ze.prototype, "markers", 2);
Ae([
  v(Ot)
], ze.prototype, "createdAt", 2);
Ae([
  v()
], ze.prototype, "data", 2);
Ae([
  v()
], ze.prototype, "settings", 1);
const _i = [8e3, 12e3, 16e3, 24e3, 48e3], Nl = "https://cdn.jsdelivr.net/npm/@diffusionstudio/libopus-wasm@1.0.0/dist/opus.wasm", Vl = "https://cdn.jsdelivr.net/npm/@diffusionstudio/libopus-wasm@1.0.0/dist/opus.js";
function Ll(t, e, i) {
  const r = ((/* @__PURE__ */ new Date()).getTime() - i) / Wl(t) * (e - t);
  return { remaining: new Date(r), progress: t, total: e };
}
function Wl(t) {
  return t < 1 ? 1 : t;
}
function Hl(t) {
  const e = _i;
  let i = 48e3;
  for (const s of e)
    Math.abs(t - s) < Math.abs(t - i) && (i = s);
  return i;
}
class He {
  target;
  fastStart;
  handle;
  constructor(e, i, s) {
    this.target = e, this.fastStart = i, this.handle = s;
  }
  static async create(e) {
    if (typeof e == "string" || !e) {
      const s = new fi();
      return new He(s, "in-memory", e);
    }
    if (typeof e == "function") {
      let s = 0;
      const r = new WritableStream({
        write(a) {
          e(a, s), s += a.length;
        }
      });
      return new He(new ei(r), !1, e);
    }
    if (e instanceof WritableStream)
      return new He(new ei(e), !1, e);
    const i = await e.createWritable();
    return new He(new ei(i, { chunked: !0 }), !1, e);
  }
  async close(e = "mp4") {
    if (this.fastStart === "in-memory" && this.target instanceof fi) {
      if (!this.target.buffer) return;
      const i = new Blob([this.target.buffer], { type: $l[e] });
      if (typeof this.handle == "string") {
        await $s(i, this.handle);
        return;
      }
      return i;
    }
  }
}
const $l = {
  mp4: "video/mp4",
  webm: "video/webm",
  ogg: "audio/ogg"
};
function jl(t, e) {
  const i = new Uint8Array(19);
  return i[0] = 79, i[1] = 112, i[2] = 117, i[3] = 115, i[4] = 72, i[5] = 101, i[6] = 97, i[7] = 100, i[8] = 1, i[9] = e, i[10] = 0, i[11] = 0, i[12] = t & 255, i[13] = t >> 8 & 255, i[14] = t >> 16 & 255, i[15] = t >> 24 & 255, i[16] = 0, i[17] = 0, i[18] = 0, i;
}
class ql {
  output;
  error;
  config;
  encoder;
  opus;
  meta;
  /**
   * Create a new OpusEncoder for encoding pcm to opus
   * @param init encoder callbacks
   */
  constructor(e) {
    this.output = e.output, this.error = e.error;
  }
  /**
   * Configure the encoder. **Note** these values must match the samples to encode
   * @param config The sample rate and channel count to use
   */
  async configure(e) {
    const i = await import(
      /* @vite-ignore */
      /* webpackIgnore: true */
      Vl
    ), { numberOfChannels: s, sampleRate: r } = this.config = e;
    if (!_i.includes(r))
      throw new at({
        code: "sampleRateNotSupported",
        message: `Unsupported sample rate, supported: ${_i.join()}`
      });
    this.opus = await i.default({
      locateFile(a, n) {
        return a.endsWith(".wasm") ? Nl : n + a;
      }
    }), this.encoder = this.opus._opus_encoder_create(r, s, 2048), this.meta = {
      decoderConfig: {
        codec: "opus",
        // Extract or create the OpusHead
        description: jl(r, s).buffer,
        numberOfChannels: s,
        sampleRate: r
      }
    };
  }
  /**
   * Encode the samples synchronously (this is a blocking event)
   * @param samples The data to encode
   */
  encode({ data: e, numberOfFrames: i, timestamp: s = 0 }) {
    if (!this.encoder || !this.opus || !this.config || !this.meta)
      throw new at({
        code: "unconfiguredEncoder",
        message: "Cannot encode samples using an unconfigured encoder"
      });
    const { sampleRate: r, numberOfChannels: a } = this.config, n = Math.floor(r / 1e3 * 20);
    let o = 0;
    const c = n / r * 1e6;
    for (; o < i; ) {
      const l = e.subarray(o * a, (o + n) * a), d = this.opus._malloc(l.length * 2);
      this.opus.HEAP16.set(l, d >> 1);
      const h = 4e3, u = this.opus._malloc(h), m = this.opus._opus_encode(
        this.encoder,
        d,
        n,
        u,
        h
      );
      if (m > 0) {
        const p = new Uint8Array(
          this.opus.HEAPU8.subarray(u, u + m)
        );
        this.output(
          {
            data: p,
            timestamp: s,
            type: "key",
            duration: c
          },
          this.meta
        );
      } else
        this.error(new DOMException("PCM chunk could not be encoded"));
      this.opus._free(d), this.opus._free(u), o += n, s += c;
    }
  }
}
class Ql extends sr {
  _configured = !1;
  _encoder;
  async encode(e) {
    this._configured || (await this._encoder?.configure(this.config), this._configured = !0);
    const i = new Int16Array(e.numberOfFrames * e.numberOfChannels);
    e.copyTo(i, { planeIndex: 0, format: "s16" }), this._encoder?.encode({
      data: i,
      // 16-bit signed integer array of interleaved audio samples
      numberOfFrames: e.numberOfFrames
    }), e.close();
  }
  flush() {
    this._configured = !1;
  }
  static supports(e) {
    return e === "opus";
  }
  init() {
    this.codec = "opus", this.config = {
      ...this.config,
      sampleRate: Hl(this.config.sampleRate)
    }, this._encoder = new ql({
      output: (e, i) => {
        this.onPacket(
          new G(
            e.data,
            e.type,
            e.timestamp / 1e6,
            e.duration / 1e6
          ),
          i
        );
      },
      error: console.error
    }), this._configured = !1;
  }
  close() {
    this._configured = !1;
  }
}
const Xl = {
  enabled: !0,
  sampleRate: 48e3,
  numberOfChannels: 2,
  bitrate: 128e3,
  codec: "aac"
}, Kl = {
  enabled: !0,
  codec: "avc",
  bitrate: 1e7,
  fps: 30,
  resolution: 1,
  fullCodecString: ""
};
class Gl {
  static create(e, i) {
    return i == "webm" ? new qr() : i == "ogg" ? new ac() : new $r({ fastStart: e.fastStart });
  }
}
class Od extends js() {
  composition;
  config;
  /**
   * Create a new audio and video encoder and multiplex the result 
   * using a mp4 container
   * @param composition The composition to render
   * @param config Configure the output
   * @example 
   * ```
   * const blob = await new Encoder(composition).render();
   * ```
   */
  constructor(e, i = {}) {
    super(), this.composition = e, this.config = {
      ...i,
      video: { ...Kl, ...i?.video },
      audio: { ...Xl, ...i?.audio }
    }, this.config.format == "ogg" && (this.config.audio.enabled = !0, this.config.video.enabled = !1);
  }
  /**
   * Export the specified composition
   * @throws DOMException if the export has been aborted
   */
  async render(e, i) {
    try {
      const { duration: s, layers: r } = this.composition;
      this.log("Preparing export"), i?.addEventListener("abort", () => {
        throw new at({
          message: "Export aborted",
          code: "export_aborted"
        });
      }), await as(this.config.audio.codec) || (console.error("Unsupported audio codec or api not available. Falling back to opus."), this.config.audio.codec = "opus"), await as(this.config.audio.codec) || (console.info("Opus encoder not available. Registering wasm encoder..."), Wa(Ql)), await this.composition.seek(0), this.composition.audio.stop(), await Ws(this.composition);
      const a = new OfflineAudioContext({
        numberOfChannels: this.config.audio.numberOfChannels,
        sampleRate: this.config.audio.sampleRate,
        length: Math.ceil(s.seconds * this.config.audio.sampleRate)
      }), n = new Xe({
        ...this.composition.settings,
        resolution: this.config.video.resolution
      }), o = new Qs({
        context: a,
        fps: this.config.video.fps
      });
      let c, l = this.config.video.fullCodecString.length ? this.config.video.fullCodecString : void 0;
      const d = new cc(n.canvas, {
        codec: this.config.video.codec,
        bitrate: this.config.video.bitrate,
        latencyMode: "quality",
        fullCodecString: l,
        onEncoderConfig: (S) => {
          c = S;
        },
        onEncoderError: (S) => {
          throw console.error("Encoder error", S, c), new at({
            message: `EncoderError: ${S.message}`,
            code: "encoder_error"
          });
        }
      }), h = new hc({
        codec: this.config.audio.codec,
        bitrate: this.config.audio.bitrate
      }), u = await He.create(e), m = Gl.create(u, this.config.format), p = new mc({ format: m, target: u.target });
      this.config.audio.enabled && p.addAudioTrack(h), this.config.video.enabled && p.addVideoTrack(d), this.log("Configured dependencies"), await p.start();
      const g = performance.now(), w = (/* @__PURE__ */ new Date()).getTime(), b = Math.floor(s.seconds * this.config.video.fps);
      let x = -1 / 0;
      for (let S = 0; S < b; S++) {
        if (i?.aborted)
          throw new at({
            message: "Export aborted",
            code: "export_aborted"
          });
        const k = S / this.config.video.fps;
        o.playbackOffset = k;
        for (let T = r.length - 1; T >= 0; T--)
          await r[T].update(o);
        if (this.config.video.enabled) {
          n.clear();
          for (let T = r.length - 1; T >= 0; T--)
            r[T].render(o, n);
          n.watermark(this.config.watermark), await d.add(k, 1 / this.config.video.fps);
        }
        const y = performance.now();
        (y - x > 100 || S === b - 1) && (this.emit("render", Ll(S, b, w)), x = y);
      }
      if (this.log(`Encoded frames at ${(b * 1e3 / (performance.now() - g)).toFixed(2)}FPS`), this.log("Encoding audio"), this.config.audio.enabled) {
        const S = await a.startRendering();
        await h.add(S);
      }
      return this.log("Finalizing file"), await p.finalize(), this.log("Export complete"), await u.close(this.config.format);
    } catch (s) {
      throw s;
    } finally {
      await Ws(this.composition), this.composition.audio.start(), await this.composition.seek(0);
    }
  }
  async audioCodecs() {
    return await en(void 0, {
      numberOfChannels: this.config.audio.numberOfChannels,
      sampleRate: this.config.audio.sampleRate,
      bitrate: this.config.audio.bitrate
    });
  }
  async videoCodecs() {
    return await Za(void 0, {
      height: this.composition.height,
      width: this.composition.width,
      bitrate: this.config.video.bitrate
    });
  }
  log(e) {
    this.config.debug && console.log(e);
  }
}
async function Ws(t) {
  for (const e of t.clips)
    await e.exit(t.audio);
  for (const e of t.layers)
    e._resetInternalState();
}
export {
  Yl as AUDIO_LOOK_AHEAD,
  pe as AudioClip,
  Qs as AudioRenderer,
  Ze as AudioSource,
  lt as BaseError,
  le as BaseSource,
  Ce as CaptionPreset,
  Pd as CascadeCaptionPreset,
  xl as CircleClip,
  At as CircleMask,
  Bl as ClassicCaptionPreset,
  $ as Clip,
  Jc as ClipDeserializer,
  ze as Composition,
  Ot as DateDeserializer,
  Bt as DecoderError,
  Od as Encoder,
  at as EncoderError,
  js as EventEmitter,
  dt as EventEmitterMixin,
  Cd as FONT_WEIGHTS,
  Rt as FPS_DEFAULT,
  oa as FPS_INACTIVE,
  ge as FontManager,
  Id as GuineaCaptionPreset,
  Us as HtmlClip,
  Xi as HtmlSource,
  Ee as IOError,
  Es as ImageClip,
  Qi as ImageSource,
  ai as Language,
  _e as Layer,
  we as Mask,
  Ta as MaskDeserializer,
  ql as OpusEncoder,
  Ed as PaperCaptionPreset,
  ta as RectangleClip,
  ut as RectangleMask,
  Jl as ReferenceError,
  Sd as RichTextClip,
  xd as SAFE_BROSER_FONTS,
  Te as Serializer,
  Ht as ShapeClip,
  Fd as SolarCaptionPreset,
  Oe as Source,
  _d as SpotlightCaptionPreset,
  qt as StorageItem,
  pd as Store,
  L as TextClip,
  E as Timestamp,
  se as Transcript,
  ee as ValidationError,
  Bd as VerdantCaptionPreset,
  zs as VideoClip,
  Xe as VideoRenderer,
  qi as VideoSource,
  et as VisualMixin,
  Pi as VisualSourceMixin,
  Il as WaveformClip,
  St as WebFonts,
  Rd as WhisperCaptionPreset,
  Tt as Word,
  Le as WordGroup,
  _ as abs,
  la as arraymove,
  B as assert,
  nd as assertNever,
  od as audioBufferToWav,
  dd as blobToMonoBuffer,
  cd as bufferToF32Planar,
  ld as bufferToI16Interleaved,
  ad as capitalize,
  da as clamp,
  sd as debounce,
  Fa as detectContentType,
  $s as downloadObject,
  ua as floatTo16BitPCM,
  jt as framesToMillis,
  Zl as framesToSeconds,
  fd as getBestSupportedAvcProfile,
  td as groupBy,
  $e as hexWithOpacity,
  ha as interleave,
  O as parseTime,
  vt as randInt,
  hd as resampleBuffer,
  ca as secondsToFrames,
  v as serializable,
  ud as showFileDialog,
  id as sleep,
  Ct as splitAt,
  ed as toHex,
  gd as transformText,
  rd as uid
};
